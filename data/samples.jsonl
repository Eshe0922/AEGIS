{"id": "NVlabs/Sana/110", "title": "Question: Flow-DPM-Solver", "body": "Is there already a generic implementation of your Flow-DPM-Solver, such as for Comfy or Swarm?\r\nWould you expect it to work equally well for other flow matching models such as Flux?\r\n\r\nthank you\r\n\r\nedit: I might have misunderstood your paper. on second read, it sounds like you have adapted DPM to flow matching and compared it to Euler - not proposed a new inference method.", "labels": ["Answered"], "repository": "NVlabs/Sana", "url": "https://github.com/NVlabs/Sana/issues/110", "created_at": "2024-12-22T19:10:58Z", "updated_at": "2024-12-23T13:58:40Z", "issue_number": 110, "state": "open", "comments": 6}
{"id": "simonw/llm-claude-3/31", "title": "Rename this plugin to llm-anthropic", "body": "So it can work with Claude 4 when that happens.", "labels": ["enhancement"], "repository": "simonw/llm-claude-3", "url": "https://github.com/simonw/llm-claude-3/issues/31", "created_at": "2024-12-17T20:09:09Z", "updated_at": "2024-12-17T20:47:39Z", "issue_number": 31, "state": "open", "comments": 3}
{"id": "cfahlgren1/observers/54", "title": "bug: async clients are not supported at the moment", "body": "LLM providers also have async clients which  are not supported at the moment", "labels": ["enhancement", "good first issue", "P0"], "repository": "cfahlgren1/observers", "url": "https://github.com/cfahlgren1/observers/issues/54", "created_at": "2024-12-16T10:30:21Z", "updated_at": "2024-12-18T12:03:46Z", "issue_number": 54, "state": "open", "comments": 1}
{"id": "cfahlgren1/observers/43", "title": "[FEAT] - Add option for HF Dataset Sync to publish on request interval", "body": "Currently, we publish via `every` parameter which corresponds to minutes and a schedule. We would like to be able to publish on X amount of requests", "labels": ["enhancement", "observer", "low-priority"], "repository": "cfahlgren1/observers", "url": "https://github.com/cfahlgren1/observers/issues/43", "created_at": "2024-12-01T22:21:54Z", "updated_at": "2024-12-16T10:44:09Z", "issue_number": 43, "state": "open", "comments": 0}
{"id": "cfahlgren1/observers/41", "title": "[FEAT] - Capture temperature and top_k etc in traces", "body": null, "labels": ["enhancement", "observer", "models", "stores"], "repository": "cfahlgren1/observers", "url": "https://github.com/cfahlgren1/observers/issues/41", "created_at": "2024-12-01T22:19:37Z", "updated_at": "2024-12-01T22:19:38Z", "issue_number": 41, "state": "open", "comments": 0}
{"id": "AnswerDotAI/rerankers/50", "title": "Support Cohere Rerank-v3.5", "body": "See https://docs.cohere.com/docs/rerank-2 -- looks like an API  v1 to v2 upgrade as well as the model itself", "labels": [], "repository": "AnswerDotAI/rerankers", "url": "https://github.com/AnswerDotAI/rerankers/issues/50", "created_at": "2024-12-06T02:26:12Z", "updated_at": "2024-12-06T02:26:12Z", "issue_number": 50, "state": "open", "comments": 0}
{"id": "basf/MolPipeline/111", "title": "Reduce model bloat: Chemprop trainer stores training data ", "body": null, "labels": ["type: bug", "status: doing"], "repository": "basf/MolPipeline", "url": "https://github.com/basf/MolPipeline/issues/111", "created_at": "2024-12-04T16:06:42Z", "updated_at": "2024-12-04T16:06:42Z", "issue_number": 111, "state": "open", "comments": 0}
{"id": "gusye1234/nano-graphrag/111", "title": "customizing chunking function", "body": "the `get_chunks` function is not exposed, it calls tiktoken and pass only tokens into the customized chunking function\r\n\r\nI think the origin text should be passed into the chunking function instead, so we could use custom tokenizer base on the embedding model ", "labels": [], "repository": "gusye1234/nano-graphrag", "url": "https://github.com/gusye1234/nano-graphrag/issues/111", "created_at": "2024-12-20T09:27:41Z", "updated_at": "2024-12-22T14:44:52Z", "issue_number": 111, "state": "open", "comments": 2}
{"id": "gusye1234/nano-graphrag/109", "title": "Writing graph with 0 nodes, 0 edges", "body": "Hi, every time I ran insert() in using_llm_api_as_llm+ollama_embedding.py for a few hours, I received the sentence `INFO:nano-graphrag:Writing graph with 0 nodes, 0 edges`, then received an error `openai.APIConnectionError: Connection error`. Could you share suggestions for a possible fix? Thank you very much.\r\n\r\n\r\n```\r\n\u2834 Processed 1015 chunks, 41892 entities(duplicated), 15247 relations(duplicated)\r\nINFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\r\nINFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\r\n\u2826 Processed 1016 chunks, 41923 entities(duplicated), 15256 relations(duplicated)\r\nINFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\r\nINFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\r\nINFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\r\nINFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\r\nINFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\r\nINFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\r\nINFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\r\nINFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\r\n\u2827 Processed 1017 chunks, 41943 entities(duplicated), 15265 relations(duplicated)\r\nINFO:nano-graphrag:Writing graph with 0 nodes, 0 edges\r\nTraceback (most recent call last):\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpx/_transports/default.py\", line 72, in map_httpcore_exceptions\r\n    yield\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpx/_transports/default.py\", line 257, in __aiter__\r\n    async for part in self._httpcore_stream:\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpcore/_async/connection_pool.py\", line 367, in __aiter__\r\n    raise exc from None\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpcore/_async/connection_pool.py\", line 363, in __aiter__\r\n    async for part in self._stream:\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpcore/_async/http11.py\", line 349, in __aiter__\r\n    raise exc\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpcore/_async/http11.py\", line 341, in __aiter__\r\n    async for chunk in self._connection._receive_response_body(**kwargs):\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpcore/_async/http11.py\", line 210, in _receive_response_body\r\n    event = await self._receive_event(timeout=timeout)\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpcore/_async/http11.py\", line 220, in _receive_event\r\n    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\r\n    raise to_exc(exc) from exc\r\nhttpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/openai/_base_client.py\", line 1565, in _request\r\n    response = await self._client.send(\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpx/_client.py\", line 1688, in send\r\n    raise exc\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpx/_client.py\", line 1682, in send\r\n    await response.aread()\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpx/_models.py\", line 913, in aread\r\n    self._content = b\"\".join([part async for part in self.aiter_bytes()])\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpx/_models.py\", line 913, in <listcomp>\r\n    self._content = b\"\".join([part async for part in self.aiter_bytes()])\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpx/_models.py\", line 931, in aiter_bytes\r\n    async for raw_bytes in self.aiter_raw():\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpx/_models.py\", line 989, in aiter_raw\r\n    async for raw_stream_bytes in self.stream:\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpx/_client.py\", line 150, in __aiter__\r\n    async for chunk in self._stream:\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpx/_transports/default.py\", line 256, in __aiter__\r\n    with map_httpcore_exceptions():\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/httpx/_transports/default.py\", line 89, in map_httpcore_exceptions\r\n    raise mapped_exc(message) from exc\r\nhttpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/research/d2/msc/khsew24/nano-graphrag/examples/using_llm_api_as_llm+ollama_embedding.py\", line 168, in <module>\r\n    insert(args.documents_path)\r\n  File \"/research/d2/msc/khsew24/nano-graphrag/examples/using_llm_api_as_llm+ollama_embedding.py\", line 127, in insert\r\n    rag.insert(file_contents_list)\r\n  File \"/research/d2/msc/khsew24/nano-graphrag/nano_graphrag/graphrag.py\", line 207, in insert\r\n    return loop.run_until_complete(self.ainsert(string_or_strings))\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"/research/d2/msc/khsew24/nano-graphrag/nano_graphrag/graphrag.py\", line 296, in ainsert\r\n    maybe_new_kg = await self.entity_extraction_func(\r\n  File \"/research/d2/msc/khsew24/nano-graphrag/nano_graphrag/_op.py\", line 383, in extract_entities\r\n    results = await asyncio.gather(\r\n  File \"/research/d2/msc/khsew24/nano-graphrag/nano_graphrag/_op.py\", line 326, in _process_single_content\r\n    glean_result = await use_llm_func(continue_prompt, history_messages=history)\r\n  File \"/research/d2/msc/khsew24/nano-graphrag/nano_graphrag/_utils.py\", line 177, in wait_func\r\n    result = await func(*args, **kwargs)\r\n  File \"/research/d2/msc/khsew24/nano-graphrag/examples/using_llm_api_as_llm+ollama_embedding.py\", line 60, in llm_model_if_cache\r\n    response = await openai_async_client.chat.completions.create(\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1490, in create\r\n    return await self._post(\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/openai/_base_client.py\", line 1832, in post\r\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/openai/_base_client.py\", line 1526, in request\r\n    return await self._request(\r\n  File \"/research/d2/msc/khsew24/conda_envs/nano-graphrag/lib/python3.10/site-packages/openai/_base_client.py\", line 1599, in _request\r\n    raise APIConnectionError(request=request) from err\r\nopenai.APIConnectionError: Connection error.\r\n```", "labels": [], "repository": "gusye1234/nano-graphrag", "url": "https://github.com/gusye1234/nano-graphrag/issues/109", "created_at": "2024-12-14T17:30:15Z", "updated_at": "2024-12-14T17:30:15Z", "issue_number": 109, "state": "open", "comments": 0}
{"id": "gusye1234/nano-graphrag/105", "title": "Customization of Azure OpenAI", "body": "please add support for Embedding and LLM Model Name for Azure OpenAI in .env\r\n\r\ne.g. \r\n\r\n```bash\r\nAZURE_OPENAI_EMBEDDING_MODEL=text-embedding-extra-large-3\r\nAZURE_OPENAI_LLM_MODLE=o1-preview\r\nAZURE_OPENAI_LLM_MODLE_LOW_COST=gpt-4o-mini-2\r\n```\r\n\r\nCurrently need to modify the source code in _llm.py for `gpt-4o` , `gpt-4o-mini` , `text-embedding-3-small` with custom names ", "labels": [], "repository": "gusye1234/nano-graphrag", "url": "https://github.com/gusye1234/nano-graphrag/issues/105", "created_at": "2024-12-02T09:55:04Z", "updated_at": "2024-12-02T10:02:34Z", "issue_number": 105, "state": "open", "comments": 0}
{"id": "ServiceNow/BrowserGym/295", "title": "How to connect to openended website through http proxy?", "body": null, "labels": [], "repository": "ServiceNow/BrowserGym", "url": "https://github.com/ServiceNow/BrowserGym/issues/295", "created_at": "2024-12-22T09:01:45Z", "updated_at": "2024-12-22T09:01:45Z", "issue_number": 295, "state": "open", "comments": 0}
{"id": "explosion/spacy-layout/23", "title": "Options for omitting tables or zooming in on table data", "body": "Apologies if this is answered elsewhere, but I could not find it.\r\n\r\nI was wondering if it possible to obtain more granular bounding box information of the contents of a table. That is, currently only the bounding box information of the table entity as a whole is given, but not of the individual text cells or rows. Is this possible?\r\n\r\nOr otherwise, is it possible to treat table information as non-table information? It seems that by turning of table structures, the data is empty and disregarded.", "labels": [], "repository": "explosion/spacy-layout", "url": "https://github.com/explosion/spacy-layout/issues/23", "created_at": "2024-12-24T12:12:31Z", "updated_at": "2024-12-24T12:12:31Z", "issue_number": 23, "state": "open", "comments": 0}
{"id": "Genentech/gReLU/84", "title": "Questions about early stopping", "body": "Hi, thanks for your great work. I wonder if it is possible to use early stopping for model training. Thanks a lot.", "labels": ["enhancement"], "repository": "Genentech/gReLU", "url": "https://github.com/Genentech/gReLU/issues/84", "created_at": "2024-11-25T15:34:40Z", "updated_at": "2024-12-16T16:48:09Z", "issue_number": 84, "state": "open", "comments": 1}
{"id": "siliconflow/BizyAir/279", "title": "[FEATURE REQUEST] Support Custom Remote ComfyUI API", "body": " Support Custom Remote ComfyUI API\r\n", "labels": ["enhancement"], "repository": "siliconflow/BizyAir", "url": "https://github.com/siliconflow/BizyAir/issues/279", "created_at": "2024-12-17T01:27:22Z", "updated_at": "2024-12-17T01:27:22Z", "issue_number": 279, "state": "open", "comments": 0}
{"id": "Marker-Inc-Korea/AutoRAG/1065", "title": "[BUG] When `file_type` is set to `all_files`, only `pdfminer` is used", "body": "**Describe the bug**\nHello,\n\nIt seems that when the parser module is configured with `file_type: all_files`, only pdfminer is applied. I have tried using `langchain_parser/upstagedocumentparse` and `llamaparser`, and both appear to use pdfminer exclusively. Even when I set the `output_format` to `html`, it seems like pdfminer is still being used. Am I mistaken about something?\n\nBelow is the YAML file I configured:\n```yaml\n- module_type: langchain_parse\n  parse_method: upstagedocumentparse\n  split: page\n  file_type: all_files\n  output_format: html\n```\nor\n```yaml\n- module_type: llamaparse\n  result_type: markdown\n  file_type: all_files\n  language: ko\n```\n\nAnd here is the result:\n![Image](https://github.com/user-attachments/assets/6f2a893b-74ba-484b-b77d-40790c8adf98)\n\nI would appreciate your help. Thank you.", "labels": ["bug"], "repository": "Marker-Inc-Korea/AutoRAG", "url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1065", "created_at": "2024-12-20T03:22:11Z", "updated_at": "2024-12-21T01:39:44Z", "issue_number": 1065, "state": "open", "comments": 2}
{"id": "Marker-Inc-Korea/AutoRAG/1060", "title": "[Feature Request] Proposal to Allow Dynamic Use of Additional Embedding Models", "body": "**Is your feature request related to a problem? Please describe.**\nHello!\n\nHow about improving the system to allow the use of other embedding models dynamically, in addition to the predefined models?\n\nhttps://github.com/Marker-Inc-Korea/AutoRAG/blob/499f54a2a012ee398850bd94ea10127a1860caf6/autorag/vectordb/base.py#L19\n\nhttps://github.com/Marker-Inc-Korea/AutoRAG/blob/499f54a2a012ee398850bd94ea10127a1860caf6/autorag/__init__.py#L65-L103", "labels": ["enhancement"], "repository": "Marker-Inc-Korea/AutoRAG", "url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1060", "created_at": "2024-12-18T01:46:30Z", "updated_at": "2024-12-19T08:36:41Z", "issue_number": 1060, "state": "open", "comments": 2}
{"id": "Marker-Inc-Korea/AutoRAG/1036", "title": "[API] Add QA view api endpoint", "body": "/projects/[project_id]/qa/[qa_name]/length\n/projects/[project_id]/qa/[qa_name]/row?index=[index_num]", "labels": [], "repository": "Marker-Inc-Korea/AutoRAG", "url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1036", "created_at": "2024-12-07T07:39:37Z", "updated_at": "2024-12-07T07:39:37Z", "issue_number": 1036, "state": "open", "comments": 0}
{"id": "Marker-Inc-Korea/AutoRAG/1024", "title": "[Feature Request] A feature to view comprehensive statistics and optimal settings in AutoRAG", "body": "**Is your feature request related to a problem? Please describe.**\nI have been using AutoRAG and performing the parse, chunk, and evaluate steps separately, and then reviewing the data stored in the benchmark one by one. However, it is time-consuming to click through each step and difficult to compare the results. Is there a feature that would allow me to view the optimal settings and detailed statistics considering all the configuration values collectively?\n\n```python\n    for parsed_raw in parsed_raw_files:\n        for chunk in chunked_file_list:\n            parsed_raw_index = parsed_raw.split(\"/\")[-1].split(\".\")[0]\n            chunk_index = str(chunk).split(\"/\")[-1].split(\".\")[0]\n            \n            if not parsed_raw_index == chunk.parent.name:\n                continue\n            initial_raw = Raw(pd.read_parquet(parsed_raw, engine=\"pyarrow\"))\n            initial_corpus = Corpus(pd.read_parquet(chunk, engine=\"pyarrow\"), initial_raw)\n            qa = initial_corpus.sample(random_single_hop, n=len(initial_corpus.data), random_state=random.randint(1,100)).map(\n                    lambda df: df.reset_index(drop=True),\n                ).make_retrieval_gt_contents().batch_apply(\n                    multiple_queries_gen,  # query generation\n                    llm=llm,\n                    lang=\"ko\",\n                    n=10,\n                ).batch_apply(\n                    make_basic_gen_gt,  # answer generation (basic)\n                    llm=llm,\n                    lang=\"ko\",\n                ).batch_apply(\n                    make_concise_gen_gt,  # answer generation (concise)\n                    llm=llm,\n                    lang=\"ko\",\n                ).filter(\n                    dontknow_filter_rule_based,  # filter unanswerable questions\n                    lang=\"ko\",\n                )\n                \n            qa_dir_name = \"qa\"\n            \n            if not os.path.exists(os.path.join(current_dir, qa_dir_name)):\n                os.makedirs(os.path.join(current_dir, qa_dir_name))\n\n            output_path = os.path.join(current_dir, qa_dir_name, f\"parsed_{parsed_raw_index}_chunk_{chunk_index}_qa.parquet\")\n            corpus_output_path = os.path.join(current_dir, qa_dir_name, f\"parsed_{parsed_raw_index}_chunk_{chunk_index}_corpus.parquet\")\n            qa.to_parquet(output_path, corpus_output_path)\n```\n\n```python\n    for i in range(10):\n        for j in range(4):\n            opt[\"config\"] = os.path.join(current_dir, \"config\", \"evaluate_config.yaml\")\n            opt[\"qa_data_path\"] = os.path.join(current_dir, \"qa\",\"parsed_{}_chunk_{}_qa.parquet\".format(i,j))\n            opt[\"corpus_data_path\"] = os.path.join(current_dir, \"qa\",\"parsed_{}_chunk_{}_corpus.parquet\".format(i,j))\n            opt[\"project_dir\"] = os.path.join(current_dir, \"benchmark\")\n            evaluate(**opt)\n```\n\n![Image](https://github.com/user-attachments/assets/2e1779e8-265f-496a-bf58-275b81ef7c33)\n", "labels": ["enhancement"], "repository": "Marker-Inc-Korea/AutoRAG", "url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1024", "created_at": "2024-12-02T07:55:20Z", "updated_at": "2024-12-02T12:15:45Z", "issue_number": 1024, "state": "open", "comments": 1}
{"id": "Marker-Inc-Korea/AutoRAG/1019", "title": "[Feature Request] AutoRAG RaaS Clinet", "body": "**Is your feature request related to a problem? Please describe.**\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n\n**Describe the solution you'd like**\n\n\n# AutoRAGClient: RAG Pipeline Management Client\n\n## Purpose\nAutoRAGClient is a client library designed to simplify the creation and management of RAG (Retrieval-Augmented Generation) pipelines through a REST API interface.\n\n## Key Features\n1. **Project Management**\n   - Create new RAG projects\n   - Manage documents within projects\n   - Project-level configuration\n\n2. **Document Processing**\n   - Support for multiple file formats (PDF, TXT, CSV, MD)\n   - Batch file uploads with pattern matching\n   - File tracking and management\n\n3. **Embedding Management**\n   - Generate vector embeddings for documents\n   - Flexible vector storage options\n   - Automatic embedding model selection\n\n4. **RAG Pipeline Operations**\n   - Create and manage RAG pipelines\n   - Question-answering capabilities\n   - Context retrieval and results management\n\n## Technical Details\n- Asynchronous HTTP client (using aiohttp)\n- Context manager support (`async with` syntax)\n- Environment-based configuration\n- Integrated logging system\n- Error handling and API error abstractions\n\n## Usage Example\n\n\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n", "labels": ["enhancement", "Client"], "repository": "Marker-Inc-Korea/AutoRAG", "url": "https://github.com/Marker-Inc-Korea/AutoRAG/issues/1019", "created_at": "2024-11-30T20:03:25Z", "updated_at": "2024-12-03T06:28:53Z", "issue_number": 1019, "state": "open", "comments": 0}
{"id": "narwhals-dev/narwhals/1647", "title": "ci: use environment variables instead of contexts in `publish_to_pypi.yml`", "body": "We should apply the same diff from https://github.com/pypa/packaging.python.org/pull/1765/files to our [publish_to_pypi.yml](https://github.com/narwhals-dev/narwhals/blob/main/.github/workflows/publish_to_pypi.yml) file", "labels": ["good first issue", "ci", "high priority"], "repository": "narwhals-dev/narwhals", "url": "https://github.com/narwhals-dev/narwhals/issues/1647", "created_at": "2024-12-23T08:27:46Z", "updated_at": "2024-12-23T09:09:15Z", "issue_number": 1647, "state": "open", "comments": 0}
{"id": "narwhals-dev/narwhals/1610", "title": "[Enh]: a method similar to `has_operation` in Ibis", "body": "### We would like to learn about your use case. For example, if this feature is needed  to adopt Narwhals in an open source project, could you please enter the link to it below?\n\n\nhttps://github.com/e10v/tea-tasting\n\n### Please describe the purpose of the new feature or describe the problem to solve.\n\nA quote from #1607:\r\n\r\n> Not all dataframes have a covariance function. For example, pyarrow doesn't have it (though it's still possible to calculate it the same way as tea-tasting does). In this case it would be very useful to have a method similar to Ibis has_operation ([example usage](https://github.com/e10v/tea-tasting/blob/077626dfe44940f2cb2360a9d1d60d5949afdbbd/src/tea_tasting/aggr.py#L355-L358)). Could you consider adding it as well?\r\n\r\nProbably, it could be useful for other methods and downstream packages as well.\n\n### Suggest a solution if possible.\n\n_No response_\n\n### If you have tried alternatives, please describe them below.\n\n_No response_\n\n### Additional information that may help us understand your needs.\n\n_No response_", "labels": ["enhancement"], "repository": "narwhals-dev/narwhals", "url": "https://github.com/narwhals-dev/narwhals/issues/1610", "created_at": "2024-12-17T11:20:37Z", "updated_at": "2024-12-17T12:10:52Z", "issue_number": 1610, "state": "open", "comments": 0}
{"id": "narwhals-dev/narwhals/1609", "title": "enh: support `nw.col('a').shift(n).over('b')`", "body": "We now support cumulative operations in `over` for pandas and Polars\r\n\r\nWe should do the same for `shift`, as pandas also supports that. One challenge will be figuring out how to pass the argument down, but it should be doable\r\n\r\ncc @ClaudioSalvatoreArcidiacono in case this interests you", "labels": ["enhancement"], "repository": "narwhals-dev/narwhals", "url": "https://github.com/narwhals-dev/narwhals/issues/1609", "created_at": "2024-12-17T11:14:52Z", "updated_at": "2024-12-20T08:11:12Z", "issue_number": 1609, "state": "open", "comments": 1}
{"id": "narwhals-dev/narwhals/1597", "title": "[Doc]: change return type from `Any` to `str | None` in `Dataframe.write_csv()`", "body": "### What type of report is this?\n\nImprovement\n\n### Please describe the issue.\n\nCurrently the return type of the `write_csv` function in the `DataFrame` class is `Any` but it should be `str | None` (see [here](https://github.com/narwhals-dev/narwhals/blob/main/narwhals/dataframe.py#L584)). When this is changed, mypy gives an error because the return types in the callstack also need to be updated. This is something for someone who likes to go on a little quest. \ud83d\ude00\n\n### If you have a suggestion on how it should be, add it below.\n\n_No response_", "labels": ["good first issue", "typing"], "repository": "narwhals-dev/narwhals", "url": "https://github.com/narwhals-dev/narwhals/issues/1597", "created_at": "2024-12-15T14:52:34Z", "updated_at": "2024-12-15T14:55:48Z", "issue_number": 1597, "state": "open", "comments": 0}
{"id": "narwhals-dev/narwhals/1583", "title": "feat: is_nan", "body": "Looks like this would be useful to Tubular: https://github.com/lvgig/tubular/pull/349#discussion_r1884006320\r\n\r\nThere would need to be a caveat in the docstring about how pandas doesn't distinguish between nan and null but other libraries typically do\r\n\r\nThis requires some attention to check that we get such nan vs null details correct:\r\n\r\n- for Polars, there is already is_nan\r\n- for PyArrow, there is https://arrow.apache.org/docs/python/generated/pyarrow.compute.is_nan.html\r\n- for pandas, we need to be careful. We don't want `is_nan` to pick up `NaT` / `None` / anything else which is used for missing values, but instead, specifically, we only want `nan`. So, we should check\r\n  - that the dtype is `Float`\r\n  - that `s != s`", "labels": ["enhancement", "good first issue"], "repository": "narwhals-dev/narwhals", "url": "https://github.com/narwhals-dev/narwhals/issues/1583", "created_at": "2024-12-13T14:50:57Z", "updated_at": "2024-12-18T17:52:41Z", "issue_number": 1583, "state": "open", "comments": 3}
{"id": "narwhals-dev/narwhals/1546", "title": "test: remove filteredwarnings", "body": "I'm concerned about all these:\r\n\r\nhttps://github.com/narwhals-dev/narwhals/blob/a1a44568ab8d523fa35916c300e98157b9ff4595/pyproject.toml#L112-L131\r\n\r\nIf we need to tightly scope ignoring a warning to one particular test, then OK, so be it, but most of these shouldn't be globally ignored\r\n\r\nIt would be very useful to go through the list and see, one by one, which ones can be removed, and where they can be more tightly set (e.g. just using `pytest.mark.filterwarnings` in one particular test case)", "labels": ["help wanted"], "repository": "narwhals-dev/narwhals", "url": "https://github.com/narwhals-dev/narwhals/issues/1546", "created_at": "2024-12-09T14:05:39Z", "updated_at": "2024-12-09T14:47:36Z", "issue_number": 1546, "state": "open", "comments": 0}
{"id": "narwhals-dev/narwhals/1541", "title": "enh: let `Enum` take arguments, allow it in construction", "body": "We should be able to do\r\n```\r\ns = nw.new_series('foo', ['a', 'b', 'c'], dtype=nw.Enum(['a', 'b', 'c', ''d]), native_namespace=pd)\r\n```\r\nand have it create a Series with dtype Categorical and categories 'a', 'b', 'c', 'd'", "labels": ["enhancement"], "repository": "narwhals-dev/narwhals", "url": "https://github.com/narwhals-dev/narwhals/issues/1541", "created_at": "2024-12-08T19:33:07Z", "updated_at": "2024-12-18T18:25:06Z", "issue_number": 1541, "state": "open", "comments": 5}
{"id": "narwhals-dev/narwhals/1512", "title": "[Doc]: Add the Narwhals calendar to the Scientific Python site for visibility", "body": "### What type of report is this?\n\nCorrection\n\n### Please describe the issue.\n\nAdd the Narwhals calendar to the Scientific Python site for visibility. \r\nExamples are here: \r\nhttps://scientific-python.org/calendars/\n\n### If you have a suggestion on how it should be, add it below.\n\n_No response_", "labels": [], "repository": "narwhals-dev/narwhals", "url": "https://github.com/narwhals-dev/narwhals/issues/1512", "created_at": "2024-12-05T18:00:32Z", "updated_at": "2024-12-05T18:00:32Z", "issue_number": 1512, "state": "open", "comments": 0}
{"id": "narwhals-dev/narwhals/1500", "title": "docs: Make docstring examples function names more descriptive, use type hints, use from_native/to_native instaed of narwhalify", "body": "Use descriptive names in docstring examples\r\n\r\nMany docstrings have examples such as `def func`, which are non-descriptive\r\n\r\nFor example:\r\n\r\n```python\r\n>>> @nw.narwhalify\r\n... def func(df):\r\n...     return df.select(threshold=nw.col(\"foo\") * 2)\r\n```\r\n\r\nWe should do two things here:\r\n- use a descriptive function name which starts with `agnostic_`. For example, this one\r\n  could be called `agnostic_select_threshold`\r\n- rewrite it to use `from_native` / `to_native` instead of `narwhalify`\r\n- add type hints: instead of just `(df):`, we should write `(df_native: IntoFrameT) -> IntoFrameT`\r\n\r\nSo, the docstring example above should become:\r\n```python\r\n>>> def agnostic_select_threshold(df_native: IntoFrameT) -> IntoFrameT:\r\n...     df = nw.from_native(df_native)\r\n...     return df.select(threshold=nw.col(\"foo\") * 2).to_native()\r\n```\r\n\r\nSome notes on  the type hints:\r\n- If you're using `IntoFrameT` make sure to include `from narwhals.typing import IntoFrameT`\r\n- If a function is only supported for eager execution (i.e. it's only defined on `narwhals.DataFrame`, not `narwhals.LazyFrame`), then use `IntoDataFrameT` instead of `IntoFrameT`\r\n- make sure the return value of the function is correct. For example, if it returns a Series, use `IntoSeriesT`\r\n\r\n---\r\n\r\nBefore submitting a pull request, please make sure that:\r\n1. linting / formatting checks all pass (`pre-commit run --all-files`)\r\n2. doctests pass (`pytest narwhals --doctest-modules`)\r\n\r\nPlease choose 1-3 docstrings to work on at a time, and comment here which ones you're working on", "labels": ["documentation", "good first issue"], "repository": "narwhals-dev/narwhals", "url": "https://github.com/narwhals-dev/narwhals/issues/1500", "created_at": "2024-12-05T07:41:06Z", "updated_at": "2024-12-16T19:52:56Z", "issue_number": 1500, "state": "open", "comments": 3}
{"id": "narwhals-dev/narwhals/1479", "title": "API: `collect` for lazy-only libraries", "body": "We currently have `LazyFrame.collect`:\r\n- for Polars.LazyFrame, this goes to Polars.DataFrames\r\n- for dask.DataFrames, it goes to pandas.DataFrame\r\n- when we get to PySpark / DuckDB / etc ... where will it go?\r\n\r\nI think one solution could be to add extra keyword arguments to `LazyFrame.collect` to specify such ambiguous cases. Adding extra (non-required) keyword arguments would still be backwards-compatible\r\n\r\nThe signature could look like this:\r\n```python\r\ndef collect(self, duckdb_eager_frame: Literal['pandas', 'polars', 'pyarrow'] = 'pyarrow', pyspark_eager_frame: Literal['pandas'] = 'pandas') -> DataFrame[Any]:\r\n```\r\n\r\nThis still leave us with the question about what to do for \"extension\" dataframes, i.e. if someone implements `__narwhals_dataframe__` / `__narwhals_lazyframe__` and extends Narwhals themself. We could require them to also implement a dunder method specifying which eager frame they want their lazy frame collected to, or which ones they support collecting into, and then have an extra kwarg in `collect` for that\r\n\r\n@EdAbati any thoughts?", "labels": ["api design", "needs discussion"], "repository": "narwhals-dev/narwhals", "url": "https://github.com/narwhals-dev/narwhals/issues/1479", "created_at": "2024-12-01T11:57:39Z", "updated_at": "2024-12-03T13:07:38Z", "issue_number": 1479, "state": "open", "comments": 7}
{"id": "pymupdf/RAG/204", "title": "Slow bbox merging algorithm prevents conversion of some pdfs", "body": "This code in pymupdf_rag.py:795 has complexity O(n^2), which is ok if there are about 100 rectangles, but I have some pdfs in which >6000 rectangles are detected. In that case, this algorithm runs virtually forever.\r\n\r\n```py\r\n        # sort descending by image area size\r\n        img_info.sort(key=lambda i: abs(i[\"bbox\"]), reverse=True)\r\n        # run from back to front (= small to large)\r\n        for i in range(len(img_info) - 1, 0, -1):\r\n            r = img_info[i][\"bbox\"]\r\n            if r.is_empty:\r\n                del img_info[i]\r\n                continue\r\n            for j in range(i):  # image areas larger than r\r\n                if r in img_info[j][\"bbox\"]:\r\n                    del img_info[i]  # contained in some larger image\r\n                    break\r\n```\r\n\r\nTwo solutions come to mind:\r\n- Small images are dropped by default. They should be dropped before trying to merge them into larger images.\r\n- It is possible to turn this into a O(n log n) with a swipe line algorithm. ", "labels": ["enhancement"], "repository": "pymupdf/RAG", "url": "https://github.com/pymupdf/RAG/issues/204", "created_at": "2024-12-03T11:30:01Z", "updated_at": "2024-12-03T20:38:34Z", "issue_number": 204, "state": "open", "comments": 5}
{"id": "frdel/agent-zero/272", "title": "Prevent agent-zero from overriding sampling parameters - or provide more options", "body": "At present Agent-Zero only has the basic '`temperature`' sampling parameter and as it cannot be disabled - may mess up the more advanced sampling options configured in the model.\r\n\r\nFor context - `temperature` is a very basic sampling option which essentially injects noise (randomness) into part of the prediction algorithm and is very rudimentary and can be problematic for code generation (should pretty much always be set to 0 unless you're using min_p). Initially `top_p` replaced `temperature` as the most effective simple way of controlling \"creativity\" aka randomness, then `min_p` came out which is a good upgrade from `top_p`. In other words - we're two evolutions past setting '`temperature`' alone.\r\n\r\nIt would be great if Agent Zero additionally let you set:\r\n\r\n- `min_p`\r\n- `top_p`\r\n- `repetition penalty`\r\n\r\nand let you unset `temperature` (as in - don't send it with API requests to Ollama).\r\n\r\nFurther reading on why `min_p` is important https://github.com/huggingface/transformers/issues/27670\r\n\r\nExample of a sensible default configuration for a coding model (Qwen 2.5 Coder 32b in this case) that you don't want to be too ridged but want to retain high quality tokens:\r\n\r\n```\r\n### min_p sampling ##\r\n\r\n# 1.0 disables top_p, so we can use min_p\r\nPARAMETER top_p 1.0\r\n\r\n# min_p works best with a small amount of temperature\r\nPARAMETER temperature 0.2\r\nPARAMETER min_p 0.9\r\n\r\nPARAMETER repeat_penalty 1.05\r\n```", "labels": [], "repository": "frdel/agent-zero", "url": "https://github.com/frdel/agent-zero/issues/272", "created_at": "2024-12-20T21:47:56Z", "updated_at": "2024-12-20T21:51:03Z", "issue_number": 272, "state": "open", "comments": 0}
{"id": "apify/crawlee-python/833", "title": "Update \"Request storage\" & \"Result storage\" guides to reflect the v0.5 state", "body": "Request loaders, request manager, request manager tandem and other things from https://github.com/apify/crawlee-python/pull/777.", "labels": ["documentation", "t-tooling"], "repository": "apify/crawlee-python", "url": "https://github.com/apify/crawlee-python/issues/833", "created_at": "2024-12-19T15:55:17Z", "updated_at": "2024-12-19T15:55:29Z", "issue_number": 833, "state": "open", "comments": 0}
{"id": "apify/crawlee-python/824", "title": "Make sleep time based tests more robust and faster", "body": "Quite a lot tests in our code base have pattern like this:\r\n\r\n```\r\naction\r\n\r\nsleep(sleep_time)\r\n\r\nassert expectation\r\n```\r\n\r\nSuch test can be flaky when sleep time is not large enough or will take a lot of time, when sleep time is too big.\r\nIt can be advantageous to create a helper function that makes such test more robust in worst case scenario and faster in best case scenario.\r\n\r\n\r\n```\r\naction\r\n\r\nassert expectation_met_within_deadline(expectation, check_period, check_deadline)\r\n```\r\nwhere check_period is very low number and check_deadline is larger number.\r\n\r\nNon abstracted version of this pattern was used here for example:\r\nhttps://github.com/apify/crawlee-python/pull/810/files\r\n\r\n\r\nCreate such abstracted helper function and apply it across the codebase tests.", "labels": ["t-tooling", "debt"], "repository": "apify/crawlee-python", "url": "https://github.com/apify/crawlee-python/issues/824", "created_at": "2024-12-16T15:26:20Z", "updated_at": "2024-12-16T15:29:37Z", "issue_number": 824, "state": "open", "comments": 0}
{"id": "apify/crawlee-python/809", "title": "Extract state persistence into a mixin-style base class", "body": "As of now, only the `Statistics` class saves state on `persistState` events. We should make it possible to implement this behavior in other classes by simply extending some mixin and passing it a pydantic model that would validate the serialized state from the key-value store.\r\n\r\nEDIT: `SessionPool` also supports persistence", "labels": ["enhancement", "t-tooling"], "repository": "apify/crawlee-python", "url": "https://github.com/apify/crawlee-python/issues/809", "created_at": "2024-12-11T16:03:51Z", "updated_at": "2024-12-16T16:55:50Z", "issue_number": 809, "state": "open", "comments": 1}
{"id": "apify/crawlee-python/776", "title": "Document state persistence", "body": "- Document how we handle the state persistence in Crawlee.\r\n- Based on the https://github.com/apify/crawlee-python/pull/772#discussion_r1867279239.", "labels": ["t-tooling"], "repository": "apify/crawlee-python", "url": "https://github.com/apify/crawlee-python/issues/776", "created_at": "2024-12-03T12:31:49Z", "updated_at": "2024-12-09T10:41:04Z", "issue_number": 776, "state": "open", "comments": 0}
{"id": "apify/crawlee-python/770", "title": "Add progress logging for scraping", "body": "A feedback from @honzajavorek:\r\n\r\n> I do think that Crawlee should log some \"progress\" information about requests made or - especially - items scraped. It's so weird to run the program and then just look at the program as if it hanged, waiting if something happens or not. E.g. Scrapy logs how many items per minute I scraped, which I personally find super useful.", "labels": ["t-tooling"], "repository": "apify/crawlee-python", "url": "https://github.com/apify/crawlee-python/issues/770", "created_at": "2024-12-02T13:01:16Z", "updated_at": "2024-12-09T10:37:16Z", "issue_number": 770, "state": "open", "comments": 1}
{"id": "apify/crawlee-python/767", "title": "Add code coverage badge to README", "body": "- [covecov](https://about.codecov.io/)\r\n- Apify service account?\r\n- Same for the SDK & client", "labels": ["t-tooling"], "repository": "apify/crawlee-python", "url": "https://github.com/apify/crawlee-python/issues/767", "created_at": "2024-11-29T14:46:23Z", "updated_at": "2024-11-29T14:46:40Z", "issue_number": 767, "state": "open", "comments": 0}
{"id": "apify/crawlee-python/766", "title": "Document the crawler & context iheritance", "body": "- Maybe \"Concepts\" section?\r\n\r\n### Generated simplified diagrams\r\n\r\n![simplified_crawling_contexts_diagram](https://github.com/user-attachments/assets/1edd8f26-76ae-4222-b460-6c0261a5bccb)\r\n\r\n![simplified_crawlers_diagram](https://github.com/user-attachments/assets/48bd7589-d10a-425e-a088-1b6cd504b857)\r\n\r\n### Generated diagrams\r\n\r\n![inheritance_diagram](https://github.com/user-attachments/assets/62ec3057-83ae-4ede-9ccc-c6d8b0de1aeb)\r\n\r\n![crawling_contexts_inheritance_diagram](https://github.com/user-attachments/assets/e2908b5f-d5b2-41e6-b8d6-ab8349e4434a)\r\n", "labels": ["documentation", "t-tooling"], "repository": "apify/crawlee-python", "url": "https://github.com/apify/crawlee-python/issues/766", "created_at": "2024-11-29T14:43:38Z", "updated_at": "2024-12-05T10:23:35Z", "issue_number": 766, "state": "open", "comments": 0}
{"id": "apify/crawlee-python/740", "title": "Consider adapting a typestate-ish pattern for context manager-based components", "body": "It would probably be hard to do so without straying even further from how the JS implementation works, but it may be easier to just do something like\r\n\r\n```py\r\nclass EventManager:\r\n  async def __aenter__(self) -> EventManagerContext:\r\n    pass\r\n  # ... no methods that need to be called on an initialized event manager\r\n\r\nclass EventManagerContext:\r\n  def emit():\r\n    pass\r\n  # and whatever else needs an initialized context manager\r\n```\r\n\r\n... and so on for everything with an initialization step. Honestly, even `BasicCrawler` could benefit from this - the tracking of active state is kinda hairy there as well.\r\n\r\n_Originally posted by @janbuchar in https://github.com/apify/crawlee-python/pull/714#discussion_r1856199249_\r\n            ", "labels": ["t-tooling", "solutioning"], "repository": "apify/crawlee-python", "url": "https://github.com/apify/crawlee-python/issues/740", "created_at": "2024-11-25T12:28:42Z", "updated_at": "2024-11-25T12:29:11Z", "issue_number": 740, "state": "open", "comments": 0}
{"id": "microsoft/TinyTroupe/80", "title": "Fix typo in init file of tinytroupe folder", "body": "There is typo in tinytroupe\\__init__.py file as mentioned in below image.\n\n![Image](https://github.com/user-attachments/assets/81f0da16-2adc-42df-8ef8-c4770b9f248b)\n", "labels": [], "repository": "microsoft/TinyTroupe", "url": "https://github.com/microsoft/TinyTroupe/issues/80", "created_at": "2024-12-10T15:50:41Z", "updated_at": "2024-12-10T15:50:41Z", "issue_number": 80, "state": "open", "comments": 0}
{"id": "microsoft/TinyTroupe/72", "title": "[FEATURE-REQUEST] Generate UML UseCase (text/diagrams) as output option", "body": "Generate UML UseCase (text/diagrams) as output option\n\nThis will provide:\n- additional acceptance rate of this TinyTroupe framework / library as UML UseCases are a common artefact in this area\n- output should be completely optional, so if people do not need it, it should be no problem\n- output should be as text and optional as diagrams (maybe ship a default implementation)\n\nThe idea is to:\n\n- use TinyTroupe outputs especially personas and scenarios:\nPersonas: Represent user archetypes with goals, behaviors, and preferences.\nScenarios/Tropes: Describe typical interactions or tasks related to a persona's goals.\nThe framework may output structured descriptions (like JSON or YAML files) that can feed into a use case generator.\n\n\nPossible Steps to Generate a Use Case Descriptions\n\nStep 1: Define the Personas and Tropes\nCreate personas and associate them with tropes (e.g., tasks, behaviors, challenges). \nOptionally UML specific properties could be added during the conversation....\n\nStep 2: Export Structured Data\nUse TinyTroupe to export the data (e.g., as JSON or YAML) for the defined personas and their tropes.\n\nStep 3: Process Data into Use Case Templates\nUse a python script or extension point to transform the exported data into use case descriptions. \nWe could offer a default implementation which can be customized by the framework users for their specific format/conventions\n\nStep 4: Generate Use Case Documentation\nTools or libraries like Markdown processors, LaTeX templates, or document generation tools can format the output into readable documentation. We could use PlantUML , PyUML or Graphviz. We should strive for easy setup and easy usage.\n\n\nTypical UseCase properties are:\nPrimary actor: The persona performing the task.\nGoal: The objective they aim to achieve.\nTriggers: Events prompting the action.\nPreconditions: Necessary conditions for the scenario to start.\nPostconditions: Expected outcomes.\n", "labels": [], "repository": "microsoft/TinyTroupe", "url": "https://github.com/microsoft/TinyTroupe/issues/72", "created_at": "2024-12-03T09:07:41Z", "updated_at": "2024-12-03T09:10:38Z", "issue_number": 72, "state": "open", "comments": 0}
{"id": "airtai/fastagency/620", "title": "Create minimal python client library for OpenAI Realtime API", "body": null, "labels": [], "repository": "airtai/fastagency", "url": "https://github.com/airtai/fastagency/issues/620", "created_at": "2024-12-11T07:13:40Z", "updated_at": "2024-12-11T07:16:02Z", "issue_number": 620, "state": "open", "comments": 1}
{"id": "airtai/fastagency/606", "title": "Implement code injection for OpenApi class", "body": "We use Pydantic models for function parameters.\n\nTo inject some field in the pydantic object we will need following:\n- [ ] Create new pydantic class which won't have the fields which need to be injected\n- [ ] inject_params wrapper needs to get dict of params which need to be injected e.g.\n{\n  \"Pet\": {\n      \"id\": 123\n   }\n}\n- [ ] inject_params wrapper returns the function with new pydantic classes", "labels": [], "repository": "airtai/fastagency", "url": "https://github.com/airtai/fastagency/issues/606", "created_at": "2024-11-27T07:01:29Z", "updated_at": "2024-11-29T12:07:56Z", "issue_number": 606, "state": "open", "comments": 0}
{"id": "circlemind-ai/fast-graphrag/31", "title": "Support for SurrealDB", "body": "**Is your feature request related to a problem? Please describe.**\nAmazing project! I would love to see support for [SurrealDB](https://github.com/surrealdb/surrealdb), one of the hottest open-source multimodal, document, graph, and vector db.\n\n**Describe the solution you'd like**\nAllow to use SurrealDB as a storage layer. It's ideal because it supports both graph and vector, and would probably allow you to add PageRank at the query level, as they support such things. It would be the most natural fit.\n\n**Describe alternatives you've considered**\nI think there's no other db that would fit so well.\n\n**Additional context**\nMore on SurrealDB here https://surrealdb.com/features\n", "labels": [], "repository": "circlemind-ai/fast-graphrag", "url": "https://github.com/circlemind-ai/fast-graphrag/issues/31", "created_at": "2024-11-25T02:57:09Z", "updated_at": "2024-11-25T02:57:09Z", "issue_number": 31, "state": "open", "comments": 0}
{"id": "princeton-nlp/SWE-agent/850", "title": "^C in run-batch should stop existing agent runs", "body": "For this, add a hook or flag to the `Agent` class that is set by `run_batch` on `KeyboardInterrupt`. If `Agent` encounters this in the main `run` loop, stop execution", "labels": ["\u2728 enhancement"], "repository": "princeton-nlp/SWE-agent", "url": "https://github.com/SWE-agent/SWE-agent/issues/850", "created_at": "2024-12-08T00:19:54Z", "updated_at": "2024-12-11T23:32:09Z", "issue_number": 850, "state": "open", "comments": 0}
{"id": "ogkalu2/comic-translate/192", "title": "Please implement cli feature ", "body": "Please implement cli feature ", "labels": [], "repository": "ogkalu2/comic-translate", "url": "https://github.com/ogkalu2/comic-translate/issues/192", "created_at": "2024-12-23T16:02:20Z", "updated_at": "2024-12-23T16:02:20Z", "issue_number": 192, "state": "open", "comments": 0}
{"id": "ogkalu2/comic-translate/186", "title": "Ukrainian language", "body": "What files do I need to change to add the Ukrainian language or could you do it?", "labels": [], "repository": "ogkalu2/comic-translate", "url": "https://github.com/ogkalu2/comic-translate/issues/186", "created_at": "2024-12-09T21:17:27Z", "updated_at": "2024-12-17T12:40:50Z", "issue_number": 186, "state": "open", "comments": 1}
{"id": "ogkalu2/comic-translate/178", "title": "Add Arabic to target language ", "body": "Could you please add Arabic language to be a target language for translation.", "labels": [], "repository": "ogkalu2/comic-translate", "url": "https://github.com/ogkalu2/comic-translate/issues/178", "created_at": "2024-11-29T12:25:07Z", "updated_at": "2024-11-29T12:25:07Z", "issue_number": 178, "state": "open", "comments": 0}
{"id": "Azure-Samples/rag-postgres-openai-python/152", "title": "[Question] Using HNSW index requires using Euclidian Distance Operator?", "body": "### Description\r\n\r\nYou have this comment from the [pgvector playgroud repo](https://github.com/pamelafox/pgvector-playground/blob/c876828d6ce7a8da5e6e4e20839c39ec78e0a86c/examples/sqlalchemy_async.py#L24C1-L25C1)\r\n```python\r\n# Define HNSW index to support vector similarity search through the vector_l2_ops access method (Euclidean distance). The SQL operator for Euclidean distance is written as <->.\r\n```\r\nDoes it mean that the HNSW index will only work with `<->` operator?\r\n\r\nSince here in the repo you are using the cosine similarity operator not the `<->`\r\nhttps://github.com/Azure-Samples/rag-postgres-openai-python/blob/61bde71a40ae76baa1445129a709d972f126f205/src/backend/fastapi_app/postgres_searcher.py#L48\r\n\r\n", "labels": [], "repository": "Azure-Samples/rag-postgres-openai-python", "url": "https://github.com/Azure-Samples/rag-postgres-openai-python/issues/152", "created_at": "2024-12-18T09:33:05Z", "updated_at": "2024-12-18T09:33:05Z", "issue_number": 152, "state": "open", "comments": 0}
{"id": "dottxt-ai/outlines-core/124", "title": "Redefine `Error`s interface", "body": "We would like to keep the size of `Error` as small as possible (currently, it's 48 bytes already).\r\n\r\nSo it needs to be defined as a struct with `Box`ed inner error, in the same manner as it's done in `serde_json` or `reqwest`:\r\nhttps://github.com/serde-rs/json/blob/master/src/error.rs\r\nhttps://github.com/seanmonstar/reqwest/blob/master/src/error.rs", "labels": ["enhancement"], "repository": "dottxt-ai/outlines-core", "url": "https://github.com/dottxt-ai/outlines-core/issues/124", "created_at": "2024-12-12T19:53:14Z", "updated_at": "2024-12-21T10:41:34Z", "issue_number": 124, "state": "open", "comments": 5}
{"id": "dottxt-ai/outlines-core/110", "title": "Consider property based testing for `json_schema`", "body": "`json_schema` might be a good candidate for property based testing.", "labels": ["enhancement", "json schema"], "repository": "dottxt-ai/outlines-core", "url": "https://github.com/dottxt-ai/outlines-core/issues/110", "created_at": "2024-12-06T14:41:18Z", "updated_at": "2024-12-21T10:09:34Z", "issue_number": 110, "state": "open", "comments": 0}
{"id": "dottxt-ai/outlines-core/109", "title": "Refactoring of the parsing logic of `json_schema`", "body": "Parsing logic of `json_schema` is written in python style and creates many unnecessary allocations/deallocations, which could be re-written much more efficiently. ", "labels": ["enhancement", "json schema"], "repository": "dottxt-ai/outlines-core", "url": "https://github.com/dottxt-ai/outlines-core/issues/109", "created_at": "2024-12-06T14:39:35Z", "updated_at": "2024-12-21T10:09:40Z", "issue_number": 109, "state": "open", "comments": 0}
{"id": "dottxt-ai/outlines-core/108", "title": "Support the `patternProperties` keyword in Json Schema", "body": "https://json-schema.org/understanding-json-schema/reference/object#patternProperties", "labels": ["enhancement", "json schema"], "repository": "dottxt-ai/outlines-core", "url": "https://github.com/dottxt-ai/outlines-core/issues/108", "created_at": "2024-12-06T10:23:08Z", "updated_at": "2024-12-21T10:07:51Z", "issue_number": 108, "state": "open", "comments": 0}
{"id": "dottxt-ai/outlines-core/105", "title": "Consider using `maturin-action` to compile the wheels", "body": "I have spent more time than I should have trying to fix the workflows with `cibuildwheels`.", "labels": ["question", "CI"], "repository": "dottxt-ai/outlines-core", "url": "https://github.com/dottxt-ai/outlines-core/issues/105", "created_at": "2024-12-05T20:20:54Z", "updated_at": "2024-12-06T13:26:34Z", "issue_number": 105, "state": "open", "comments": 0}
{"id": "dottxt-ai/outlines-core/97", "title": "Build `index` from regex", "body": "Build `index` from regex in rust in order to replace `FSMInfo` intermediary.", "labels": ["documentation", "enhancement", "testing"], "repository": "dottxt-ai/outlines-core", "url": "https://github.com/dottxt-ai/outlines-core/issues/97", "created_at": "2024-11-25T18:27:11Z", "updated_at": "2024-11-25T18:27:15Z", "issue_number": 97, "state": "open", "comments": 0}
{"id": "thousandbrainsproject/tbp.monty/113", "title": "Ensure that local doc references inside documents also exist in the hierarchy.md", "body": "### Your proposal or request for the feature\r\n\r\nThe github_readme_sync tool does check the following broken links:\r\n\r\n- Ensures the `hierarchy.md` references exist on the file system. \r\n- Ensures that references inside a document exist on the file system.   \r\n\r\nBut it does not check (and should)\r\n- that references inside a document exist in the `hierarchy.md`\r\n\r\n### Alternatives\r\n\r\nn/a\r\n\r\n### Additional context\r\n\r\nThis was brought to light when this PR was merged https://github.com/thousandbrainsproject/tbp.monty/pull/87\r\n\r\n\r\nand those missing links were then added in this PR https://github.com/thousandbrainsproject/tbp.monty/pull/111\r\n", "labels": ["documentation", "enhancement", "triaged"], "repository": "thousandbrainsproject/tbp.monty", "url": "https://github.com/thousandbrainsproject/tbp.monty/issues/113", "created_at": "2024-12-13T16:38:30Z", "updated_at": "2024-12-13T18:07:10Z", "issue_number": 113, "state": "open", "comments": 0}
{"id": "thousandbrainsproject/tbp.monty/86", "title": "Test failure: worker crashes during setup", "body": "\r\nHello,\r\n\r\nI was following the installation guide on \r\nhttps://thousandbrainsproject.readme.io/docs/getting-started\r\n, and for some reason all the workers crashed. \r\nI'm running a very basic Debian 12 environment.\r\n\r\n```\r\nmaximum crashed workers reached: 8\r\n\r\n=================================================================================== FAILURES ====================================================================================\r\n________________________________________________________________________ tests/unit/base_config_test.py _________________________________________________________________________\r\n[gw1] linux -- Python 3.8.20 /home/defaultuser/miniconda3/envs/tbp.monty/bin/python\r\nworker 'gw1' crashed while running 'tests/unit/base_config_test.py::BaseConfigTest::test_can_run_eval_epoch'\r\n________________________________________________________________________ tests/unit/base_config_test.py _________________________________________________________________________\r\n[gw0] linux -- Python 3.8.20 /home/defaultuser/miniconda3/envs/tbp.monty/bin/python\r\nworker 'gw0' crashed while running 'tests/unit/base_config_test.py::BaseConfigTest::test_can_run_episode'\r\n________________________________________________________________________ tests/unit/base_config_test.py _________________________________________________________________________\r\n[gw3] linux -- Python 3.8.20 /home/defaultuser/miniconda3/envs/tbp.monty/bin/python\r\nworker 'gw3' crashed while running 'tests/unit/base_config_test.py::BaseConfigTest::test_can_save_and_load'\r\n________________________________________________________________________ tests/unit/evidence_lm_test.py _________________________________________________________________________\r\n[gw2] linux -- Python 3.8.20 /home/defaultuser/miniconda3/envs/tbp.monty/bin/python\r\nworker 'gw2' crashed while running 'tests/unit/evidence_lm_test.py::EvidenceLMTest::test_evidence_time_out'\r\n_______________________________________________________________________ tests/unit/graph_learning_test.py _______________________________________________________________________\r\n[gw4] linux -- Python 3.8.20 /home/defaultuser/miniconda3/envs/tbp.monty/bin/python\r\nworker 'gw4' crashed while running 'tests/unit/graph_learning_test.py::GraphLearningTest::test_reproduce_multiple_episodes'\r\n___________________________________________________________________________ tests/unit/policy_test.py ___________________________________________________________________________\r\n[gw5] linux -- Python 3.8.20 /home/defaultuser/miniconda3/envs/tbp.monty/bin/python\r\nworker 'gw5' crashed while running 'tests/unit/policy_test.py::PolicyTest::test_can_run_curv_informed_policy'\r\n________________________________________________________________________ tests/unit/base_config_test.py _________________________________________________________________________\r\n[gw6] linux -- Python 3.8.20 /home/defaultuser/miniconda3/envs/tbp.monty/bin/python\r\nworker 'gw6' crashed while running 'tests/unit/base_config_test.py::BaseConfigTest::test_logging_info_level'\r\n_______________________________________________________________________ tests/unit/graph_learning_test.py _______________________________________________________________________\r\n[gw7] linux -- Python 3.8.20 /home/defaultuser/miniconda3/envs/tbp.monty/bin/python\r\nworker 'gw7' crashed while running 'tests/unit/graph_learning_test.py::GraphLearningTest::test_can_run_eval_episode_with_surface_agent'\r\n___________________________________________________________________________ tests/unit/policy_test.py ___________________________________________________________________________\r\n[gw8] linux -- Python 3.8.20 /home/defaultuser/miniconda3/envs/tbp.monty/bin/python\r\nworker 'gw8' crashed while running 'tests/unit/policy_test.py::PolicyTest::test_surface_policy_moves_back_to_object'\r\n________________________________________________________________________ tests/unit/evidence_lm_test.py _________________________________________________________________________\r\n[gw9] linux -- Python 3.8.20 /home/defaultuser/miniconda3/envs/tbp.monty/bin/python\r\nworker 'gw9' crashed while running 'tests/unit/evidence_lm_test.py::EvidenceLMTest::test_moving_off_object_5lms'\r\n=================================================================== xdist: maximum crashed workers reached: 8 ===================================================================\r\n============================================================================ short test summary info ============================================================================\r\nFAILED tests/unit/base_config_test.py::BaseConfigTest::test_can_run_eval_epoch\r\nFAILED tests/unit/base_config_test.py::BaseConfigTest::test_can_run_episode\r\nFAILED tests/unit/base_config_test.py::BaseConfigTest::test_can_save_and_load\r\nFAILED tests/unit/evidence_lm_test.py::EvidenceLMTest::test_evidence_time_out\r\nFAILED tests/unit/graph_learning_test.py::GraphLearningTest::test_reproduce_multiple_episodes\r\nFAILED tests/unit/policy_test.py::PolicyTest::test_can_run_curv_informed_policy\r\nFAILED tests/unit/base_config_test.py::BaseConfigTest::test_logging_info_level\r\nFAILED tests/unit/graph_learning_test.py::GraphLearningTest::test_can_run_eval_episode_with_surface_agent\r\nFAILED tests/unit/policy_test.py::PolicyTest::test_surface_policy_moves_back_to_object\r\nFAILED tests/unit/evidence_lm_test.py::EvidenceLMTest::test_moving_off_object_5lms\r\n======================================================================== 10 failed, 148 passed in 19.93s ========================================================================\r\n```\r\n\r\n", "labels": ["bug", "triaged"], "repository": "thousandbrainsproject/tbp.monty", "url": "https://github.com/thousandbrainsproject/tbp.monty/issues/86", "created_at": "2024-11-30T14:28:26Z", "updated_at": "2024-12-01T03:25:20Z", "issue_number": 86, "state": "open", "comments": 3}
{"id": "zou-group/textgrad/148", "title": "Support a regression task?", "body": "In analyzing this project, I noticed that the current task description uses \"correct/incorrect\" to describe the model's output. However, this approach is more suited for classification tasks. I am currently using this project for a task, but my task is a regression task. How should I adjust the model architecture to support a regression task? For example, should the loss function be modified?", "labels": [], "repository": "zou-group/textgrad", "url": "https://github.com/zou-group/textgrad/issues/148", "created_at": "2024-12-20T09:05:05Z", "updated_at": "2024-12-20T09:05:05Z", "issue_number": 148, "state": "open", "comments": 0}
{"id": "NVIDIA/NeMo-Skills/280", "title": "Replace llm-math-judge and decontamination inference scripts with pre/post-process commands", "body": "The generate pipeline now accepts pre/post-process commands and it would be great for us to remove the other inference scripts we have that are very similar and instead move whatever separate logic they need into pre/post scripts, so that the core code to generate from an llm can be kept in a single place", "labels": [], "repository": "NVIDIA/NeMo-Skills", "url": "https://github.com/NVIDIA/NeMo-Skills/issues/280", "created_at": "2024-12-03T22:34:29Z", "updated_at": "2024-12-03T22:34:29Z", "issue_number": 280, "state": "open", "comments": 0}
{"id": "souzatharsis/podcastfy/208", "title": "total token count and price calculation", "body": "Is there any possibility that can output the total token, model and fee used ?\r\n\r\nThat would be very helpful", "labels": [], "repository": "souzatharsis/podcastfy", "url": "https://github.com/souzatharsis/podcastfy/issues/208", "created_at": "2024-12-08T01:48:20Z", "updated_at": "2024-12-08T01:48:20Z", "issue_number": 208, "state": "open", "comments": 0}
{"id": "souzatharsis/podcastfy/200", "title": "Support for multiple characters in a podcast", "body": "Many podcasts have quests or more than 2 people. Would be awesome to make the code support as many characters as supplied from the config files. ", "labels": [], "repository": "souzatharsis/podcastfy", "url": "https://github.com/souzatharsis/podcastfy/issues/200", "created_at": "2024-11-29T17:00:56Z", "updated_at": "2024-11-29T17:22:21Z", "issue_number": 200, "state": "open", "comments": 1}
{"id": "souzatharsis/podcastfy/199", "title": "How to get Elevenlabs to work?", "body": "Thanks for this excellent package. I use the downloaded package, not the pip install version. I want to try to use custom voices from Elevenlabs but somehow I can't get it to work. \r\nI have a custom.yaml with tts_model: \"elevenlabs\" but when I run this I get an error\r\n\r\n  File \"/Users/Documents/podcastfy/podcastfy/client.py\", line 342, in generate_podcast\r\n    tts_model = conversation_config.get(\"default_tts_model\", \"openai\")\r\n                ^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'str' object has no attribute 'get'\r\n\r\nI am not sure how to proceed from here... an example with custom yaml to use Elevenlabs in another language would be appreciated. \r\n", "labels": [], "repository": "souzatharsis/podcastfy", "url": "https://github.com/souzatharsis/podcastfy/issues/199", "created_at": "2024-11-29T15:28:53Z", "updated_at": "2024-11-30T11:16:19Z", "issue_number": 199, "state": "open", "comments": 4}
{"id": "souzatharsis/podcastfy/195", "title": "Feature Request for Azure OpenAI support", "body": "First of all thank you for sharing such a nice podcast tool with the community.\r\n\r\nSecondly, I want to use the Azure OpenAI model instead of the OpenAI model. Is it possible to use it in the existing version of the tool? \r\nIf no, is it possible to add this feature to the tool?\r\nLooking forward to hearing from you.\r\nthanks ", "labels": [], "repository": "souzatharsis/podcastfy", "url": "https://github.com/souzatharsis/podcastfy/issues/195", "created_at": "2024-11-27T11:16:06Z", "updated_at": "2024-11-27T11:16:06Z", "issue_number": 195, "state": "open", "comments": 0}
{"id": "Comfy-Org/comfy-cli/217", "title": "Add \"--fast-deps\" (using uv) flag to restore snapshot functionality", "body": "Now it's possible to run the \"**comfy install**\" command with the \"**--fast-deps**\" flag that significantly saves time on the installation process.\r\n\r\nBut restoring a snapshot process, especially with many nodes, still takes a lot of time, and adding this functionality can help to save years for users around the world.\r\n\r\nIt would be great to have the possibility to use it something like this:\r\n`comfy node restore-snapshot snapshot.json --fast-deps`\r\n\r\n", "labels": ["enhancement"], "repository": "Comfy-Org/comfy-cli", "url": "https://github.com/Comfy-Org/comfy-cli/issues/217", "created_at": "2024-12-04T16:22:05Z", "updated_at": "2024-12-04T16:22:05Z", "issue_number": 217, "state": "open", "comments": 0}
{"id": "lenML/Speech-AI-Forge/194", "title": "API Demo Examples", "body": "\u5e94\u8be5\u589e\u52a0\u4e00\u4e2a /examples \u5c55\u793a\u4e00\u4e9b\u5982\u4f55\u4f7f\u7528 api \u7684\u4ee3\u7801\u4f8b\u5b50\r\n\r\n### TODOs\r\n- [x] python speech-forge client\r\n- [x] js speech-forge client\r\n- [x] tts examples\r\n- [ ] stt examples\r\n- [ ] ssml examples\r\n- [ ] voice clone examples\r\n- [ ] script/speaker_builder", "labels": ["documentation", "Story"], "repository": "lenML/Speech-AI-Forge", "url": "https://github.com/lenML/Speech-AI-Forge/issues/194", "created_at": "2024-11-27T12:04:20Z", "updated_at": "2024-11-28T11:37:23Z", "issue_number": 194, "state": "open", "comments": 0}
{"id": "epic-open-source/seismometer/116", "title": "Move minimum python version (remove 3.10 support)", "body": "As we stay on the latest versions of our dependencies, we need to change versions of python accordingly.\r\n\r\nSee numpy [drop schedule](https://numpy.org/neps/nep-0029-deprecation_policy.html#drop-schedule)  \r\nWhile numpy v2 supports 3.9-3.12, v2.1 moves to 3.10-3.13 and may be establishing a pattern.\r\n\r\n", "labels": ["compatibility"], "repository": "epic-open-source/seismometer", "url": "https://github.com/epic-open-source/seismometer/issues/116", "created_at": "2024-12-04T12:48:05Z", "updated_at": "2024-12-04T12:54:15Z", "issue_number": 116, "state": "open", "comments": 0}
{"id": "epic-open-source/seismometer/115", "title": "Move base python version to 3.12", "body": "This issue is to document and track the expectation to move the base python version forward.\r\n\r\nCurrently, core testing is done in a py3.10 environment with test suite verifying stability in 3.11.  \r\nWe will be moving this forward, targeting 3.12 as a baseline and starting to test in 3.13.\r\n\r\n[python version table](https://devguide.python.org/versions/)\r\n\r\nThis is NOT removing support for either 3.10 or 3.11 (see issues: #116)", "labels": ["compatibility"], "repository": "epic-open-source/seismometer", "url": "https://github.com/epic-open-source/seismometer/issues/115", "created_at": "2024-12-04T12:38:55Z", "updated_at": "2024-12-04T15:28:26Z", "issue_number": 115, "state": "open", "comments": 1}
{"id": "DAGWorks-Inc/burr/484", "title": "Async persister interface", "body": "**Is your feature request related to a problem? Please describe.**\r\nThe current [persister interface](https://github.com/DAGWorks-Inc/burr/blob/50d33522b609774cd4c916a68131dc44b7844c5c/burr/core/persistence.py#L98) is synchronous. This can cause performance issues in asynchronous code.\r\n\r\n**Describe the solution you'd like**\r\nWe should have a separate stack for asynchronous persistence -- ideally with modes for either (1) fire-and-forget or (2) blocking/transactional (depends on how consistent the user wants it -- this part might be general persister configuration, rather than async-specific).\r\n\r\nTo dig in, we will need:\r\n- [ ] Persistence interfaces for this (`BaseStateLoader`, `BaseStateSaver`, `PersisterHook`) -- each converted to async\r\n- [ ] A few default implementations (SQLLite, redis? TBD -- go with the easy ones on this)\r\n- [ ] Integrations with `application`\r\n    - [ ] asynchronous `build()` method in application builder (`.abuild()`) - or maybe another builder? Guess is we want an asynchronous build method\r\n    - [ ] validation in `.run()` -- E.G. if you have async hooks they will not be called -- at least a warning message if not an error\r\n- [ ] Comprehensive testing for ^^^\r\n- [ ] Think through integration with parallel actions  --- will want an `acreate_app` [here](https://github.com/DAGWorks-Inc/burr/blob/50d33522b609774cd4c916a68131dc44b7844c5c/burr/core/parallelism.py#L113).\r\n\r\n**Describe alternatives you've considered**\r\n\r\nCould have synchronous fire/forget mode, although we very likely want something transactional.\r\n", "labels": ["enhancement"], "repository": "DAGWorks-Inc/burr", "url": "https://github.com/DAGWorks-Inc/burr/issues/484", "created_at": "2024-12-20T17:16:45Z", "updated_at": "2024-12-20T17:16:52Z", "issue_number": 484, "state": "open", "comments": 0}
{"id": "DAGWorks-Inc/burr/482", "title": "List all partition keys of an application", "body": "**Is your feature request related to a problem? Please describe.**\r\nRight now, burr does not provide an interface to list all `partition_key`s of an application.\r\n\r\nQuoting from the [documentation](https://burr.dagworks.io/concepts/state-persistence/#state-keys):\r\n\r\n> In the case of a chatbot, the app_id could be a uuid, and the partition_key could be the user\u2019s ID or email, etc. Note that partition_key can be None if this is not relevant. A UUID is always generated for the app_id if not provided.\r\n\r\nImagine we are implementing an admin page for the system managing the chatbot, and need to display all the users' chat sessions, that the interface to retrieve all the `partition_key`s from an application should become necessary.\r\n\r\n**Describe the solution you'd like**\r\nMaybe a method from the application to retrieve all the `partition_key`s?\r\n\r\n**Describe alternatives you've considered**\r\nDirect data retrieval from the storage where the states of the application are persisted.\r\n\r\n**Additional context**\r\nNone\r\n", "labels": [], "repository": "DAGWorks-Inc/burr", "url": "https://github.com/DAGWorks-Inc/burr/issues/482", "created_at": "2024-12-18T20:09:29Z", "updated_at": "2024-12-18T23:26:06Z", "issue_number": 482, "state": "open", "comments": 5}
{"id": "DAGWorks-Inc/burr/468", "title": "Tags for actions", "body": "**Is your feature request related to a problem? Please describe.**\r\nWe should be able to tag actions. In particular, this could be used to do halt_after and halt_before.\r\n\r\n**Describe the solution you'd like**\r\n\r\n```python\r\n@action(reads=[...], writes=[...], tags=[\"requires_human_input\"])\r\ndef act(state: State, human_input: ...) -> State:\r\n    ...\r\n\r\napp.run(halt_before=[\"requires_human_input\"])\r\n```\r\n\r\nWould have to do this at the class-level as well, should be straightforward. Also there's the possibility of name-clashes on halt. I'm OK with that due to the simplicity here.\r\n\r\nCould also do this at the action level -- in the builder `with_actions(some_action=action.with_tags([\"requires_human_input\"])`. Would have to think about how to do this with function-based actions (TBD).\r\n\r\n**Describe alternatives you've considered**\r\nSee above\r\n\r\n**Additional context**\r\nSomething I've been mulling over for a while. ", "labels": ["enhancement"], "repository": "DAGWorks-Inc/burr", "url": "https://github.com/DAGWorks-Inc/burr/issues/468", "created_at": "2024-12-11T22:56:49Z", "updated_at": "2024-12-12T20:11:52Z", "issue_number": 468, "state": "open", "comments": 1}
{"id": "DAGWorks-Inc/burr/467", "title": "Object/Blob store capabilities", "body": "**Is your feature request related to a problem? Please describe.**\r\nIt is common to hold a reference to some file/large object in state. This can be, for example:\r\n\r\n1. A PDF for RAG/ingestion\r\n2. An excel document that we're generating\r\n3. A large dataframe that was pulled from S3\r\n\r\nThe problem is that this can really crowd out the state, if we JSON serialize it. E>G> a large pdf could be mbs of base-64 data each time, and as state is currently saved in its entirety after every node execution, this can get even worse.\r\n\r\n**Describe the solution you'd like**\r\n\r\nA few ideas, still thinking through:\r\n\r\n#### Push to the serialization layer\r\nThis is the current approach  -- we actually do this for pandas dataframes. To do this effectively, the user would be a `CachingObject` data type and register serialization capabilities for it. Pydantic also could do this easily.\r\n\r\nThis would look something like this -- this is an immutable reference file (E.G. one that is passed in or generated externally). Note you'd have extra steps if you were generating a file.\r\n\r\n```python\r\n@dataclass\r\nclass ReferenceCachingFile:\r\n    path: str\r\n    contents_hash: str = None\r\n    contents: Optional[str] = None \r\n\r\n    def get() -> str:\r\n        if self.contents is not None:\r\n            return self.contents\r\n        self.contents  = self.load()\r\n        return self.contents\r\n\r\n    def load() -> str:\r\n         with open(path, \"r\") as f:\r\n            contents = f.read()\r\n        # TODO -- compute + verify hash, TBD how much verification we'd want\r\n        return contents\r\n\r\n        \r\n       \r\n@serde.serialize.register(ReferenceCachingFile)\r\ndef serialize_reference_caching_file(value: ReferenceCachingFile):\r\n    return {\"path\" value.path, \"contents_hash\": value.contents_hash, serde.KEY: \"reference_caching_file\"}\r\n\r\n@serde.deserializer.register(\"reference_caching_file\")\r\ndef deserialize_myclass(value: dict, myclass_kwargs: dict = None, **kwargs) -> cls:\r\n    out = ReferenceCachingFile(**value)\r\n    out.contents = out.load()\r\n```\r\n\r\nThis is just an illustration -- it's missing two things:\r\n1. Generic pluggability to load from anywhere\r\n2. Saving capability as well in the case of generating files\r\n\r\nSome options:\r\n1. Have mixins for saveable/writable with the right functions\r\n2. Keep these as part of the framework -- then just have people implement their own with a few in plugins/subclas\r\n\r\n#### Push to the persistence layer\r\n\r\nI think there's room for a `o9bject_store` abstraction here. Just like we have a persister, we could also have a blob store. E.G.\r\n\r\n```python\r\napp = Application()\r\n    .with_state_persister(...)\r\n    .with_object_store(FileSystemStore())\r\n    ...\r\n```\r\n\r\nThen the user could specify some sort of `CachingFile` (for lack of a better name) like we had above:\r\n\r\n```python\r\n\r\nT = TypeVar(\"ObjectType\")\r\n\r\nclass CachingFile(abc.ABC):\r\n    contents: T \r\n    contents_hash: str\r\n    location_data: dict # URI? TBD what extra data we need...\r\n    \r\n    @abc.abstractmethod\r\n    def to_bytes() -> bytes:\r\n        ...\r\n\r\n    @abc.abstractmethod\r\n    def from_bytes() -> T:\r\n        ...\r\n```\r\n\r\nThe blob store would then interact with this -- calling `to_bytes`/`from_bytes` and comparing the hash/saving at the hash location. Then the user can implement a `CachingFile` or use one of the plugins (pandas, pdf, etc...)... THe db would store the serialized/deserialized version, and then delegate to the blob store when rehydrating or saving state. \r\n\r\nSome other things to flesh out:\r\n1. How to ensure we don't save it twice (the hash -- will need to put this in the interface...)\r\n2. When it would be loaded. Dynamically? On startup? Configurable?\r\n\r\n**Describe alternatives you've considered**\r\nThe first one above is easiest to do now -- this is how it works, and we do something similar for pandas, a bit messy though.\r\n\r\n**Additional context**\r\nMilan J came to office hours today asking about this, thanks!\r\n", "labels": [], "repository": "DAGWorks-Inc/burr", "url": "https://github.com/DAGWorks-Inc/burr/issues/467", "created_at": "2024-12-11T21:52:15Z", "updated_at": "2024-12-11T21:52:15Z", "issue_number": 467, "state": "open", "comments": 0}
{"id": "DAGWorks-Inc/burr/461", "title": "Bundling Actions Together", "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\n@gamarin2 asks:\r\n\r\n```\r\nHey! While working with class-based actions, I ended up creating state transitions that must logically be split in two separate actions, but where the second one must always be preceded by the first one.\r\n\r\nIn my case, it's a FormatSummarizer action that formats messages before transitioning to a Summarizer action. These two actions form what I would call an \"action bundle\": the second one can't be separated from the first one. For now, I have documented that requirement in my class docstring, but it would be nice to have it more strictly enforced. \r\n\r\nI'm sure there are various ways we could achieve this, but perhaps the easiest would be to add a property to the action class where you could specify preceding/following action classes. The AppBuilder would then fail to build if you haven't added the proper transitions.\r\n```\r\n\r\n@mdrideout also suggests:\r\n```\r\nI would say that I have a very large number of use cases for this, and creating an entire sub-graph for it is painful.\r\n\r\nTo piggyback on this, I also have a very large number of use cases for running actions with high concurrency, and have been creating sub-graphs simply for the purpose of running a single action for ~10 items. \r\nbut I may be doing this as a workaround because I had a some issues trying to use MappedStates (?) with either pydantic or async.\r\n```\r\n\r\n**Describe the solution you'd like**\r\nA way to have an action represent a graph.\r\n\r\n```python\r\nclass MultiAction:\r\n    @abc.abstractmethod\r\n    def get_graph() -> Graph:\r\n        ...\r\n\r\nclass ActionChain(MultiAction):\r\n    @abc.abstractmethod\r\n    def get_actions() -> List[[Tuple[str, Union[Action, Callable]]]:\r\n        ...\r\n\r\n    def get_graph() -> Graph:\r\n        named_actions = self.get_actions()\r\n        action_names = [name for name, _ in named_actions]\r\n        return GraphBuilder().with_actions(**dict(actions)).with_transitions(zip(action_name, action_names[1:]))\r\n```\r\n\r\n**Describe alternatives you've considered**\r\nCould also have `with_subgraph` in builder, similar thing. This allows for packaging at the action level for reusability. Might also want the ability to remap inputs/outputs:\r\n\r\n```python\r\ndef state_maps() -> Tuple[dict, dict]:\r\n    ....\r\n```\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n", "labels": ["enhancement"], "repository": "DAGWorks-Inc/burr", "url": "https://github.com/DAGWorks-Inc/burr/issues/461", "created_at": "2024-12-05T20:33:29Z", "updated_at": "2024-12-05T20:33:35Z", "issue_number": 461, "state": "open", "comments": 0}
{"id": "DAGWorks-Inc/burr/437", "title": "Add visibility into conditions", "body": "**Is your feature request related to a problem? Please describe.**\r\nWe should be able to understand the condition.  E.G. what happened/why it transitioned from one node to the next. Want to see this in the UI.\r\n\r\n**Describe the solution you'd like**\r\nTracking + Viz. Want to see in the UI:\r\n\r\n- Condition inputs\r\n- Condition result\r\n- Condition name\r\n- Condition code\r\n\r\nWill have to provide something for the condition code -- E.G. a string representation that's overwriteable for a lambda of sorts.\r\n\r\nShould be part of data/in the graph as needed?\r\n\r\n**Describe alternatives you've considered**\r\nYou can kind of derive this but it's tricky\r\n\r\n**Additional context**\r\nRelated to #436 -- would build on each other.\r\n", "labels": ["enhancement"], "repository": "DAGWorks-Inc/burr", "url": "https://github.com/DAGWorks-Inc/burr/issues/437", "created_at": "2024-11-26T19:42:57Z", "updated_at": "2024-11-26T19:43:33Z", "issue_number": 437, "state": "open", "comments": 0}
{"id": "DAGWorks-Inc/burr/436", "title": "Add `condition` to `halt_after` / `halt_before`", "body": "**Is your feature request related to a problem? Please describe.**\r\nCurrently, it is not possible to halt the app after a given action iff a `condition` is met. Here is a motivating example of when it might be needed:\r\n\r\nSay you have a chatgpt-like app with various \"chats\", where each chat is a distinct burr app. When a user starts a new chat, you'd like to have a quick routing step that will detect, based on the first prompt, if the new chat is actually the continuation of a previous chat. If so, we want to halt the current app and restart the relevant app. If not, the app can continue its normal lifecycle.\r\n\r\n**Describe the solution you'd like**\r\nInstead of providing a list of actions to `halt_after` (or `halt_before`), provide a list of tuples `(action, condition)`. The application will halt after a given `action` if and only if `condition` is met. If no condition is provided, the app will always halt after `action` is processed (current behavior).\r\n\r\n**Describe alternatives you've considered**\r\nRight now, the alternative is to have a dummy action to halt on and a conditional transition to it. \r\n\r\n```py\r\nwith_transitions(\r\n    (\"routing_action\", \"continue_action\", when(continue=True)),\r\n    (\"routing_action, \"halt_action\", when(continue=False))\r\n)\r\n```\r\n\r\nAnd then when calling `app.run`:\r\n\r\n```py\r\nfinal_action, result, final_state = app.run(\r\n            halt_after=[\"halt_action\"],\r\n            inputs={\"prompt\" : user_input}\r\n        )\r\n```\r\n\r\nThis is less clean than the suggested API. \r\n\r\nAnother possible way to address this would be to enable halting the application from within an action, although it may be harder to implement. \r\n", "labels": [], "repository": "DAGWorks-Inc/burr", "url": "https://github.com/DAGWorks-Inc/burr/issues/436", "created_at": "2024-11-26T16:11:22Z", "updated_at": "2024-11-26T20:31:56Z", "issue_number": 436, "state": "open", "comments": 10}
{"id": "getzep/graphiti/221", "title": "Proposal: Maintaining a standard schema to convert text to knowledge graphs", "body": "Hi, I am not sure, if this sounds accurate, but just a though I wanted to share and understand the feasibility of it. So currently, the way LLMs are generating nodes / edges for KGs can be either be static or dynamic. For static, it does not have much degree of freedom and for dynamic, it can generate too much nodes (without maintaining a uniformity, does not help during searches). \r\n\r\nI then recently bumped into this website called [Schema.org](https://schema.org/) which is a public standardization for internet level schema. So how about exploring this and incorporating into the text to knowledge graph pipeline? \r\n\r\n\r\nExample:\r\n\r\n```txt\r\nExtract entities and relationships from the following text and represent them using the Schema.org vocabulary in JSON-LD format:\r\n\r\n\"John Doe, a software engineer at TechCorp, attended the AI conference in San Francisco on September 21, 2023.\"\r\n\r\nProvide the output in JSON-LD.\r\n```\r\n\r\nAnd getting this as output:\r\n\r\n```json\r\n{\r\n  \"@context\": \"https://schema.org\",\r\n  \"@type\": \"Person\",\r\n  \"name\": \"John Doe\",\r\n  \"jobTitle\": \"Software Engineer\",\r\n  \"worksFor\": {\r\n    \"@type\": \"Organization\",\r\n    \"name\": \"TechCorp\"\r\n  },\r\n  \"attendedEvent\": {\r\n    \"@type\": \"Event\",\r\n    \"name\": \"AI Conference\",\r\n    \"location\": {\r\n      \"@type\": \"Place\",\r\n      \"name\": \"San Francisco\"\r\n    },\r\n    \"startDate\": \"2023-09-21\"\r\n  }\r\n}\r\n```", "labels": [], "repository": "getzep/graphiti", "url": "https://github.com/getzep/graphiti/issues/221", "created_at": "2024-11-27T19:42:34Z", "updated_at": "2024-12-02T16:11:03Z", "issue_number": 221, "state": "open", "comments": 2}
{"id": "eakmanrq/sqlframe/231", "title": "please provide public base class for sqlframe dataframes", "body": "Currently, if I have an object, I can check if it's a sqlframe dataframe by doing `isinstance(df, sqlframe.base.dataframe._BaseDataFrame)`. This works, but it's a private class - any chance a public class (or a public function) could be provided to check if an object is of any sqlframe kind?\r\n\r\nuse case: sqlframe support in Narwhals", "labels": [], "repository": "eakmanrq/sqlframe", "url": "https://github.com/eakmanrq/sqlframe/issues/231", "created_at": "2024-12-24T11:21:58Z", "updated_at": "2024-12-24T11:21:58Z", "issue_number": 231, "state": "open", "comments": 0}
{"id": "eakmanrq/sqlframe/230", "title": "Please provide `sqlframe.__version__`", "body": "Use case - SQLFrame support in Narwhals\r\n\r\nOther tools provide e.g. `pandas.__version__`, `polars.__version__`, and we use it to keep track of the backend version in case of API changes", "labels": [], "repository": "eakmanrq/sqlframe", "url": "https://github.com/eakmanrq/sqlframe/issues/230", "created_at": "2024-12-24T11:17:35Z", "updated_at": "2024-12-24T11:17:35Z", "issue_number": 230, "state": "open", "comments": 0}
{"id": "eakmanrq/sqlframe/229", "title": "Please consider not setting upper-bounds on dependencies", "body": "Hey, well done with SQLFrame, and thanks for the quick fixes, I'm taking a look at how to support / build on it in [Narwhals](https://github.com/narwhals-dev/narwhals)\r\n\r\nMay I suggest you consider not setting upper bounds on dependencies?\r\nThis can cause issues in CI. For example [here](https://github.com/vega/altair/pull/3591) an upper bound on pyarrow in ibis was blocking upgrading to Python 3.13\r\n\r\nI'd suggest not setting upper bounds on dependencies, as suggested here https://iscinumpy.dev/post/bound-version-constraints/\r\n\r\n> Capping dependencies has long term negative effects, especially for libraries, and should never be taken lightly. A library is not installed in isolation; it has to live with other libraries in a shared environment. Only add a cap if a dependency is known to be incompatible or there is a high (>75%) chance of it being incompatible in its next release. Do not cap by default - capping dependencies makes your software incompatible with other libraries that also have strict lower limits on dependencies, and limits future fixes. ", "labels": [], "repository": "eakmanrq/sqlframe", "url": "https://github.com/eakmanrq/sqlframe/issues/229", "created_at": "2024-12-24T09:01:26Z", "updated_at": "2024-12-24T09:01:26Z", "issue_number": 229, "state": "open", "comments": 0}
{"id": "eakmanrq/sqlframe/209", "title": "Support more DML statements", "body": "Hi,\r\nRight now we could say that sqlframe only support the **INSERT** statement\r\n\r\nI know that spark by itself does not support this kind of statements but I think they would be really usefull. \r\nAlso I don't think that supporting that would be out of scope for a library like this because at the end the idea of sqlframe is to run using a SQL Database as the backend and these are usual operations done in a Database/Datawarehouse.\r\n\r\nLibraries like [delta-lake](https://docs.delta.io/latest/api/python/spark/index.html) and [Snowpark](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/1.25.0/snowpark/api/snowflake.snowpark.Table.update) support these kind of operations so they could be a good starting point to define a good API for this.\r\n\r\n\r\n", "labels": ["enhancement"], "repository": "eakmanrq/sqlframe", "url": "https://github.com/eakmanrq/sqlframe/issues/209", "created_at": "2024-11-27T21:45:44Z", "updated_at": "2024-12-13T16:11:23Z", "issue_number": 209, "state": "open", "comments": 10}
{"id": "chaidiscovery/chai-lab/228", "title": "guarantee result reproducibility with seed", "body": "Hi.\r\n\r\nDoing inference with command `chai fold {input_fasta} {ouptut_dir} --seed {input_seed}` doesn't guarantee fixed result for given seed.\r\nIs there any other things to consider for result reproduciblity?\r\n\r\nThx.", "labels": [], "repository": "chaidiscovery/chai-lab", "url": "https://github.com/chaidiscovery/chai-lab/issues/228", "created_at": "2024-12-09T07:26:26Z", "updated_at": "2024-12-09T19:17:22Z", "issue_number": 228, "state": "open", "comments": 1}
{"id": "showlab/computer_use_ootb/61", "title": "ValueError: not enough values to unpack (expected 2, got 1)", "body": "I am currently trying to run the demo and when I run it, it gives the following error\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\chirag_tubakad\\anaconda3\\envs\\ootb\\Lib\\site-packages\\gradio\\queueing.py\", line 714, in process_events\r\n    response = await route_utils.call_process_api(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\chirag_tubakad\\anaconda3\\envs\\ootb\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\chirag_tubakad\\anaconda3\\envs\\ootb\\Lib\\site-packages\\gradio\\blocks.py\", line 2047, in process_api\r\n    result = await self.call_function(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\chirag_tubakad\\anaconda3\\envs\\ootb\\Lib\\site-packages\\gradio\\blocks.py\", line 1606, in call_function\r\n    prediction = await utils.async_iteration(iterator)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\chirag_tubakad\\anaconda3\\envs\\ootb\\Lib\\site-packages\\gradio\\utils.py\", line 714, in async_iteration\r\n    return await anext(iterator)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\chirag_tubakad\\anaconda3\\envs\\ootb\\Lib\\site-packages\\gradio\\utils.py\", line 708, in __anext__\r\n    return await anyio.to_thread.run_sync(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\chirag_tubakad\\anaconda3\\envs\\ootb\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\chirag_tubakad\\anaconda3\\envs\\ootb\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\r\n    return await future\r\n           ^^^^^^^^^^^^\r\n  File \"C:\\Users\\chirag_tubakad\\anaconda3\\envs\\ootb\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\r\n    result = context.run(func, *args)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\chirag_tubakad\\anaconda3\\envs\\ootb\\Lib\\site-packages\\gradio\\utils.py\", line 691, in run_sync_iterator_async\r\n    return next(iterator)\r\n           ^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\chirag_tubakad\\anaconda3\\envs\\ootb\\Lib\\site-packages\\gradio\\utils.py\", line 852, in gen_wrapper\r\n    response = next(iterator)\r\n               ^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\chirag_tubakad\\Documents\\computer_use_ootb-main\\app.py\", line 273, in process_input\r\n    for loop_msg in sampling_loop_sync(\r\n  File \"C:\\Users\\chirag_tubakad\\Documents\\computer_use_ootb-main\\computer_use_demo\\loop.py\", line 175, in sampling_loop_sync\r\n    vlm_response = planner(messages=messages)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\chirag_tubakad\\Documents\\computer_use_ootb-main\\computer_use_demo\\gui_agent\\planner\\api_vlm_planner.py\", line 141, in __call__\r\n    vlm_response, token_usage = run_oai_interleaved(\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: not enough values to unpack (expected 2, got 1)\r\n\r\nAny help would be appreciated!", "labels": [], "repository": "showlab/computer_use_ootb", "url": "https://github.com/showlab/computer_use_ootb/issues/61", "created_at": "2024-12-24T06:45:51Z", "updated_at": "2024-12-24T12:43:00Z", "issue_number": 61, "state": "open", "comments": 0}
{"id": "showlab/computer_use_ootb/57", "title": "What annotation platform do you use to create the sft dataset? ", "body": "Hi there,\r\n\r\nI want to create my own UI agent sft dataset. After trying several open source data annotation tools, I find no one is capable of annotating such kind of data. Would you like to share some information about the annotation tools you use?", "labels": [], "repository": "showlab/computer_use_ootb", "url": "https://github.com/showlab/computer_use_ootb/issues/57", "created_at": "2024-12-16T09:03:22Z", "updated_at": "2024-12-16T09:03:22Z", "issue_number": 57, "state": "open", "comments": 0}
{"id": "showlab/computer_use_ootb/44", "title": "Add the option to use a custom OpenAI endpoint and model.", "body": null, "labels": [], "repository": "showlab/computer_use_ootb", "url": "https://github.com/showlab/computer_use_ootb/issues/44", "created_at": "2024-12-05T12:07:13Z", "updated_at": "2024-12-12T16:36:45Z", "issue_number": 44, "state": "open", "comments": 2}
{"id": "rio-labs/rio/196", "title": "Expose Additional Platform Information", "body": "Rio currently provides some information about the session's platform. More information can be gained by manually parsing the user agent. It would be nice to provide more, such as:\r\n\r\n- primary interaction type (touchscreen vs mouse)\r\n- browser (+ separate web engine?)\r\n- operating sytem\r\n- display size (rather than window size)\r\n- pixel density\r\n- pixels per rem?\r\n\r\nAlso, consider a high-level, well-known attribute that all of Rio, as well as user layouts use to switch between desktop & mobile views.", "labels": ["new feature"], "repository": "rio-labs/rio", "url": "https://github.com/rio-labs/rio/issues/196", "created_at": "2024-12-15T16:39:38Z", "updated_at": "2024-12-23T08:55:56Z", "issue_number": 196, "state": "open", "comments": 4}
{"id": "rio-labs/rio/192", "title": "Add More Blogposts to Multipage Website Example", "body": "### Description\r\nThe current Multipage Website includes one blogpost. To enhance its usability and showcase its versatility, we should add more blogposts with the following specifications:\r\n\r\n### 1. Support for Mobile and Desktop:\r\n\r\n-Ensure that all blogposts support both mobile and desktop screens.\r\n-Maintain a consistent design across devices while adapting layouts as necessary for smaller screens.\r\n\r\n### 2. Distinct Blogpost Designs:\r\n- **Each blogpost should have a unique layout to demonstrate various design possibilities.**\r\n- Examples of differences:\r\n-- Varying text-to-image ratios.\r\n-- Diverse image placement.\r\n-- Thematic variations (e.g., minimalist vs. media-rich).\r\n\r\n### 3. Content Requirements:\r\n\r\n- Use lorem ipsum text for placeholders, but feel free to use actual blog content if available.\r\n- Include representative images or graphics for visual diversity.\r\n\r\nAs a refernce have a look at `adventure_in_alps.py`. If you need any help or guidance while working on this issue, feel free to join our Discord server and contact me directly.", "labels": ["good first issue"], "repository": "rio-labs/rio", "url": "https://github.com/rio-labs/rio/issues/192", "created_at": "2024-12-07T13:22:47Z", "updated_at": "2024-12-07T14:47:46Z", "issue_number": 192, "state": "open", "comments": 2}
{"id": "rio-labs/rio/186", "title": "[Feature Request] Add `tabs` component", "body": "### Description\r\n\r\nI suggest adding a Tabs component to the `rio-ui` Python UI library. Tabs are a useful feature that helps users by organizing content into different sections that they can easily switch between. This makes the layout easier to use and looks more professional. Adding this feature will make `rio-ui` meet user needs better and keep up with similar libraries.\r\n\r\n\r\n### Suggested Solution\r\n\r\nReference:\r\n- [radix-ui tabs component](https://www.radix-ui.com/primitives/docs/components/tabs) \r\n- [material design tabs component](https://m3.material.io/components/tabs/overview)\r\n- [shadcn tabs component](https://ui.shadcn.com/docs/components/tabs)\r\n\r\n### Alternatives\r\n\r\n`rio-ui` have revealer component, but this way of showing information not convey connection between section\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### Related Issues/Pull Requests\r\n\r\n_No response_", "labels": ["new component"], "repository": "rio-labs/rio", "url": "https://github.com/rio-labs/rio/issues/186", "created_at": "2024-12-03T11:17:32Z", "updated_at": "2024-12-03T13:56:24Z", "issue_number": 186, "state": "open", "comments": 1}
{"id": "rio-labs/rio/183", "title": "Make all input components consistent (in terms of events, parameters, etc)", "body": "In terms of interface:\r\n- Focus events: `on_gain_focus`, `on_lose_focus`\r\n- Change event: `on_change`\r\n- Confirm event: `on_confirm`\r\n\r\nThere are also some design decisions we should think about:\r\n\r\n- Should each component get its own events? Do we really need a `TextInputFocusEvent` and a `MultiLineTextInputFocusEvent` and a `NumberInputFocusEvent` and ...?\r\n- Should there be a base class (in JS and/or in python) that implements this interface?\r\n\r\nIn terms of optimization, we should also avoid sending unnecessary messages between the backend and the frontend. Most `TextInput`s don't have a `on_gain_focus` or `on_lose_focus` event handler, and yet the backend notifies the frontend every time one of these events occurs.", "labels": ["enhancement"], "repository": "rio-labs/rio", "url": "https://github.com/rio-labs/rio/issues/183", "created_at": "2024-11-30T18:32:36Z", "updated_at": "2024-11-30T18:32:45Z", "issue_number": 183, "state": "open", "comments": 0}
{"id": "rio-labs/rio/182", "title": "Add Click Events to Tables", "body": "Tables are sometimes used to display available items. When an item is clicked, additional information is displayed.\r\n\r\nConsider adding click / doubleclick events for this.", "labels": ["enhancement"], "repository": "rio-labs/rio", "url": "https://github.com/rio-labs/rio/issues/182", "created_at": "2024-11-29T23:01:37Z", "updated_at": "2024-12-04T18:25:43Z", "issue_number": 182, "state": "open", "comments": 0}
{"id": "opendatalab/MinerU/1342", "title": "\u5f02\u6b65\u3001\u6279\u91cf\u89e3\u6790\u5bf9\u8c61\u5b58\u50a8\u6587\u4ef6", "body": "\u6839\u636e\u5bf9\u8c61\u5b58\u50a8\uff1ahttps://mineru.readthedocs.io/zh-cn/latest/user_guide/quick_start/to_markdown.html \r\n1\u3001\u6211\u7684\u9700\u6c42\u662f \u628a\u5bf9\u8c61\u5b58\u50a8\u6587\u4ef6\u90e8\u5206\u5c01\u88c5\u6210\u4e00\u4e2a\u63a5\u53e3A\uff0c\u7531\u4e8e\u6211\u53ea\u6709cpu\u8d44\u6e90\uff0c\u8fd9\u4e2a\u89e3\u6790\u6587\u4ef6\u5e76\u4e0a\u4f20\u5927\u90e8\u5206\u65f6\u95f4\u4f1a\u5f88\u6162\r\n\u6240\u4ee5\u6211\u60f3\u63d0\u4f9b\u4e00\u4e2a\u5f02\u6b65\u63a5\u53e3\uff0c\u5728\u8c03\u7528\u4e4b\u540e\u7acb\u9a6c \u8fd4\u56de\u4e00\u4e2a\u6587\u4ef6\u5bf9\u5e94\u7684uuid\uff0c\u7b49\u6587\u4ef6\u89e3\u6790\u5b8c\u6210\u540e\u3001\u8bb0\u5f55\u89e3\u6790\u7684\u7ed3\u679c\r\n2\u3001\u518d\u63d0\u4f9b\u4e00\u4e2a\u63a5\u53e3B \u6839\u636euuid\u67e5\u8be2\u89e3\u6790\u7ed3\u679c\r\n3\u3001\u7b2c\u4e00\u6b21\u8c03\u7528\u63a5\u53e3A\u5f88\u5feb\u8fd4\u56deuuid\uff0c\u540e\u53f0\u5728\u6267\u884c\u6587\u4ef6\u89e3\u6790\r\n4\u3001\u518d\u6b21\u8c03\u7528\u63a5\u53e3A\u6216\u8005\u4e0e\u6587\u4ef6\u89e3\u6790\u65e0\u5173\u63a5\u53e3B \u5c31\u4f1a\u88ab\u963b\u585e\r\n5\u3001\u5f53\u524d\u4f7f\u7528BackgroundTasks\uff0c\u5f53\u6211\u5c1d\u8bd5\u4f7f\u7528celery[redis]\uff0c\u5b83\u4e00\u76f4\u62a5\u9519TypeError('Object of type coroutine is not JSON serializable')\uff0c\u5373\u4fbf\u5b83\u53ef\u80fd\u8ddfminerU\u6ca1\u6709\u5173\u7cfb\uff0c\u6211\u4f9d\u7136\u6ca1\u6709\u627e\u5230\u539f\u56e0\r\n6\u3001\u6211\u60f3\u77e5\u9053\u6b63\u786e\u89e3\u51b3\u6279\u91cf\u89e3\u6790pdf\u6587\u4ef6\u7684\u65b9\u5f0f\u662f\u4ec0\u4e48\r\n", "labels": ["enhancement"], "repository": "opendatalab/MinerU", "url": "https://github.com/opendatalab/MinerU/issues/1342", "created_at": "2024-12-20T17:06:24Z", "updated_at": "2024-12-20T17:06:24Z", "issue_number": 1342, "state": "open", "comments": 0}
{"id": "opendatalab/MinerU/1341", "title": "Could you provide word boxes' boundaries in the output json files?", "body": "Does MinerU provide the box boundaries for each word in the PDF in the output JSON files? If not, could you provide it? This information should be already available in the OCR engine used by MinerU.", "labels": ["enhancement"], "repository": "opendatalab/MinerU", "url": "https://github.com/opendatalab/MinerU/issues/1341", "created_at": "2024-12-20T14:24:30Z", "updated_at": "2024-12-20T14:24:30Z", "issue_number": 1341, "state": "open", "comments": 0}
{"id": "opendatalab/MinerU/1319", "title": "\u65e0\u6cd5\u8bc6\u522b\u56fe\u7247\u578bPDF", "body": "\u6211\u53d1\u73b0\u76ee\u524d\u7684\u6a21\u578b\u8bc6\u522b\u666e\u901aPDF\u8fd8\u662f\u4e0d\u9519\u7684\uff0c\u4f46\u662f\u5982\u679c\u6211\u628aPDF\u8f6c\u6362\u4e3a\u56fe\u7247\u578bPDF\uff0c\u5c31\u4e00\u70b9\u90fd\u8bc6\u522b\u4e0d\u51fa\u6765\u4e86", "labels": ["enhancement"], "repository": "opendatalab/MinerU", "url": "https://github.com/opendatalab/MinerU/issues/1319", "created_at": "2024-12-18T06:41:15Z", "updated_at": "2024-12-19T06:37:42Z", "issue_number": 1319, "state": "open", "comments": 4}
{"id": "opendatalab/MinerU/1279", "title": "\u89e3\u6790\u540e\u7684 `head` \u6ca1\u6709\u5c42\u7ea7\u7ed3\u6784\uff0c\u53ea\u6709\u4e00\u7ea7 `head`", "body": "**Is your feature request related to a problem? Please describe.**\r\n**\u60a8\u7684\u7279\u6027\u8bf7\u6c42\u662f\u5426\u4e0e\u67d0\u4e2a\u95ee\u9898\u76f8\u5173\uff1f\u8bf7\u63cf\u8ff0\u3002**\r\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\r\n\u5bf9\u5b58\u5728\u7684\u95ee\u9898\u8fdb\u884c\u6e05\u6670\u4e14\u7b80\u6d01\u7684\u63cf\u8ff0\u3002\u4f8b\u5982\uff1a\u6211\u4e00\u76f4\u5f88\u56f0\u6270\u7684\u662f [...]\r\n\r\n**Describe the solution you'd like**\r\n**\u63cf\u8ff0\u60a8\u671f\u671b\u7684\u89e3\u51b3\u65b9\u6848**\r\nA clear and concise description of what you want to happen.\r\n\u6e05\u6670\u4e14\u7b80\u6d01\u5730\u63cf\u8ff0\u60a8\u5e0c\u671b\u5b9e\u73b0\u7684\u5185\u5bb9\u3002\r\n\r\n**Describe alternatives you've considered**\r\n**\u63cf\u8ff0\u60a8\u5df2\u8003\u8651\u7684\u66ff\u4ee3\u65b9\u6848**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\u6e05\u6670\u4e14\u7b80\u6d01\u5730\u63cf\u8ff0\u60a8\u5df2\u7ecf\u8003\u8651\u8fc7\u7684\u4efb\u4f55\u66ff\u4ee3\u89e3\u51b3\u65b9\u6848\u3002\r\n\r\n**Additional context**\r\n**\u63d0\u4f9b\u66f4\u591a\u7ec6\u8282**\r\nAdd any other context or screenshots about the feature request here.\r\n\u8bf7\u9644\u4e0a\u4efb\u4f55\u76f8\u5173\u622a\u56fe\u3001\u94fe\u63a5\u6216\u6587\u4ef6\uff0c\u4ee5\u5e2e\u52a9\u6211\u4eec\u66f4\u597d\u5730\u7406\u89e3\u60a8\u7684\u8bf7\u6c42\u3002\r\n![image](https://github.com/user-attachments/assets/3c0b07ca-8f83-421a-9494-f0fb70f728b3)\r\n", "labels": ["enhancement"], "repository": "opendatalab/MinerU", "url": "https://github.com/opendatalab/MinerU/issues/1279", "created_at": "2024-12-12T08:13:23Z", "updated_at": "2024-12-19T04:49:44Z", "issue_number": 1279, "state": "open", "comments": 2}
{"id": "andrewyng/aisuite/153", "title": "running the sample code raises errors with anthropic with \"pip install aisuite[all]", "body": "a/Desktop/echo_hive_all/openai swarm/test.py\"\r\nWhy did the pirate bring a ladder to the bar? Because he heard the drinks were on the house! Arrr!\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\memoa\\Desktop\\echo_hive_all\\openai swarm\\test.py\", line 12, in <module>\r\n    response = client.chat.completions.create(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\memoa\\miniconda3\\envs\\basic3\\Lib\\site-packages\\aisuite\\client.py\", line 108, in create\r\n    self.client.providers[provider_key] = ProviderFactory.create_provider(\r\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\memoa\\miniconda3\\envs\\basic3\\Lib\\site-packages\\aisuite\\provider.py\", line 46, in create_provider\r\n    return provider_class(**config)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\memoa\\miniconda3\\envs\\basic3\\Lib\\site-packages\\aisuite\\providers\\anthropic_provider.py\", line 16, in __init__\r\n    self.client = anthropic.Anthropic(**config)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\memoa\\miniconda3\\envs\\basic3\\Lib\\site-packages\\anthropic\\_client.py\", line 121, in __init__\r\n    super().__init__(\r\n  File \"C:\\Users\\memoa\\miniconda3\\envs\\basic3\\Lib\\site-packages\\anthropic\\_base_client.py\", line 835, in __init__\r\n    self._client = http_client or SyncHttpxClientWrapper(\r\n                                  ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\memoa\\miniconda3\\envs\\basic3\\Lib\\site-packages\\anthropic\\_base_client.py\", line 733, in __init__\r\n    super().__init__(**kwargs)\r\nTypeError: Client.__init__() got an unexpected keyword argument 'proxies'\r\n\r\nC:\\Users\\memoa\\Desktop\\echo_hive_all\\openai swarm>\r\n\r\n\r\n--------------\r\n---------------\r\n-----------\r\nCODE I RAN:\r\n\r\n`import aisuite as ai\r\nclient = ai.Client()\r\n\r\nmodels = [\"openai:gpt-4o\", \"anthropic:claude-3-5-sonnet-latest\"]\r\n\r\nmessages = [\r\n    {\"role\": \"system\", \"content\": \"Respond in Pirate English.\"},\r\n    {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\r\n]\r\n\r\nfor model in models:\r\n    response = client.chat.completions.create(\r\n        model=model,\r\n        messages=messages,\r\n        temperature=0.75\r\n    )\r\n    print(response.choices[0].message.content)\r\n`", "labels": [], "repository": "andrewyng/aisuite", "url": "https://github.com/andrewyng/aisuite/issues/153", "created_at": "2024-12-15T19:05:28Z", "updated_at": "2024-12-17T02:26:20Z", "issue_number": 153, "state": "open", "comments": 1}
{"id": "andrewyng/aisuite/137", "title": "Usage with Instructor", "body": "How would this be used with [instructor?](https://python.useinstructor.com/)\r\nWould it only require `.from_openai?`", "labels": [], "repository": "andrewyng/aisuite", "url": "https://github.com/andrewyng/aisuite/issues/137", "created_at": "2024-12-06T15:39:01Z", "updated_at": "2024-12-07T20:05:57Z", "issue_number": 137, "state": "open", "comments": 0}
{"id": "andrewyng/aisuite/136", "title": "Add support for custom headers", "body": "I don't see a way to be able to add a custom header (or headers) to a request.  It would be great if the client code was change such that the headers could be added to.\r\n\r\nWe are required in our use cases to provide a user header but this library won't allow clients to pass this (unless I am mistaken).\r\n\r\nhttps://github.com/andrewyng/aisuite/blob/main/aisuite/providers/azure_provider.py#L31\r\n\r\n", "labels": [], "repository": "andrewyng/aisuite", "url": "https://github.com/andrewyng/aisuite/issues/136", "created_at": "2024-12-06T15:15:27Z", "updated_at": "2024-12-06T15:16:49Z", "issue_number": 136, "state": "open", "comments": 0}
{"id": "andrewyng/aisuite/135", "title": "Software development suggestions", "body": "I think aisuite has a lot of potential. However, I didn't find any logic from an integration and generalization point of view for teams of developers within an IT.\r\n\r\nThe aisuite code seems to be selfishly centered from a \"developer who acts alone in his corner\" point of view...\r\n\r\nFor example, forcing the Python version to at least 3.10 is really a shame because in the real business world there are many Python projects (not too old) that use versions 3.7.x, 3.8.x, 3.9.x and more.\r\nAnd some of these projects could evolve very quickly by integrating aisuite, but the fact that aisuite requires at least 3.10 does not allow it.\r\n\r\nIt is also difficult to find a way around the organization of aisuite's system files, it lacks logic. For example, there is no configuration file centralizing the URLs for each provider. If I integrate an Ollama service into my IT, I don't know which configuration file to use to configure the URL of my Ollama!", "labels": [], "repository": "andrewyng/aisuite", "url": "https://github.com/andrewyng/aisuite/issues/135", "created_at": "2024-12-06T09:20:03Z", "updated_at": "2024-12-06T15:54:01Z", "issue_number": 135, "state": "open", "comments": 2}
{"id": "andrewyng/aisuite/127", "title": "Support keyless auth for Azure OpenAI (token provider)", "body": "Many of our Azure developers do not use API keys, they use \"keyless auth\" which passes in a short-lived OAuth token, which can then be refreshed. The OpenAI package already supports that keyless auth, by accepting a token_provider callback, calling that at the appropriate times, and passing in a Bearer header with the resulting token. Would you consider adding bearer token auth for this wrapper? Or is this wrapper meant for hobbyist scenarios only?", "labels": ["provider support"], "repository": "andrewyng/aisuite", "url": "https://github.com/andrewyng/aisuite/issues/127", "created_at": "2024-12-02T22:27:56Z", "updated_at": "2024-12-03T03:30:33Z", "issue_number": 127, "state": "open", "comments": 2}
{"id": "andrewyng/aisuite/125", "title": "Using locally downloaded Hugging Face models", "body": "Can I use Aisuit with Hugging Face models I've downloaded locally? For instance, can I specify models like models=['openai:gpt-4', 'huggingface:/models/llm/llama/llama3-7b-instruct']?\r\nthanks", "labels": ["provider support"], "repository": "andrewyng/aisuite", "url": "https://github.com/andrewyng/aisuite/issues/125", "created_at": "2024-12-02T03:29:54Z", "updated_at": "2024-12-05T07:52:20Z", "issue_number": 125, "state": "open", "comments": 2}
{"id": "andrewyng/aisuite/99", "title": "Detailed docs like Litellm Required", "body": "I would like to know what makes this project different from litellm and detailed docs for its usage like litellm project is needed for others to understand clearly how to integrate different llms with other open source projects like openwebui", "labels": ["documentation"], "repository": "andrewyng/aisuite", "url": "https://github.com/andrewyng/aisuite/issues/99", "created_at": "2024-11-28T15:09:40Z", "updated_at": "2024-12-11T01:53:39Z", "issue_number": 99, "state": "open", "comments": 3}
{"id": "andrewyng/aisuite/96", "title": "Consider normalizing equivalent inference parameters", "body": "I would find it very helpful if common inference parameters, such as temperature or max. tokens, had normalized argument names. These could then be automatically mapped to the required configuration names for each provider.  \r\n\r\nFor example:  \r\n- AWS provider uses `topP` and `maxTokens`  \r\n- OpenAI provider uses `top_p` and `max_tokens`  \r\n\r\nThis discrepancy necessitates the use of if-else statements to handle different providers, which seems counter to the framework's goal of streamlining integration.\r\n \r\nWhile I am not a professional developer, here\u2019s an idea for implementation:\r\n\r\n```python\r\n# aisuite/providers/__init__.py\r\nNORM_KWARGS_AWS = {\r\n    \"max_tokens\": \"maxTokens\",\r\n    \"temperature\": \"temperature\",\r\n    \"top_p\": \"topP\",\r\n    \"stop_sequences\": \"stopSequences\",\r\n}\r\n```\r\n\r\n```python\r\n# aisuite/providers/aws_provider.py\r\nfrom aisuite.providers import NORM_KWARGS_AWS\r\n\r\nfor norm_key, value in kwargs.items():\r\n    key = NORM_KWARGS_AWS.get(norm_key, norm_key)\r\n    ...\r\n```\r\n\r\nSee: https://github.com/viictorjimenezzz/aisuite\r\n\r\nThank you for considering this request!  \r\n\r\n", "labels": ["enhancement"], "repository": "andrewyng/aisuite", "url": "https://github.com/andrewyng/aisuite/issues/96", "created_at": "2024-11-28T11:44:57Z", "updated_at": "2024-12-01T20:35:33Z", "issue_number": 96, "state": "open", "comments": 2}
{"id": "andrewyng/aisuite/91", "title": "Add observability and security part of the providers", "body": "It would be great to see basic observability and security features integrated directly into BaseProvider class.\r\n\r\nObservability:\r\n\r\n1. Logging: Track API calls, errors, and performance metrics for better debugging.\r\n2. Tracing: Integrate with OpenTelemetry for distributed tracing across providers.\r\n3. Metrics: Expose latency, throughput, and error rates to monitor provider performance.\r\n\r\nSecurity:\r\n\r\n1. Input Validation: Mitigate risks like prompt injection and ensure PII masking.\r\n2. Response Filtering: Enable moderation to filter sensitive or unsafe outputs.\r\n\r\nThese features would enhance reliability, transparency, and safety.", "labels": [], "repository": "andrewyng/aisuite", "url": "https://github.com/andrewyng/aisuite/issues/91", "created_at": "2024-11-28T01:37:35Z", "updated_at": "2024-11-29T19:50:53Z", "issue_number": 91, "state": "open", "comments": 2}
{"id": "instructlab/instructlab/2834", "title": "Issue while creating Synthetic Data Creation ", "body": "**Describe the bug**\r\nGetting multiple errors while generating synthetic data \"ilab generate\" command\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nSynthetic data generation should go fine.\r\n\r\n**Screenshots**\r\n\r\nGenerating synthetic data using 'models/merlinite-7b-lab-Q4_K_M.gguf' model, taxonomy:'taxonomy' against http://*********:10261/v1 server\r\nCannot find prompt.txt. Using default prompt depending on model-family.\r\nTraceback (most recent call last):\r\n  File \"/Users/tanvimaira/Documents/demo/venv/bin/ilab\", line 8, in <module>\r\n    sys.exit(ilab())\r\n             ^^^^^^\r\n  File \"/Users/tanvimaira/Documents/demo/venv/lib/python3.12/site-packages/click/core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/tanvimaira/Documents/demo/venv/lib/python3.12/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n         ^^^^^^^^^^^^^^^^\r\n  File \"/Users/tanvimaira/Documents/demo/venv/lib/python3.12/site-packages/click/core.py\", line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/tanvimaira/Documents/demo/venv/lib/python3.12/site-packages/click/core.py\", line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/tanvimaira/Documents/demo/venv/lib/python3.12/site-packages/click/core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/tanvimaira/Documents/demo/venv/lib/python3.12/site-packages/click/decorators.py\", line 33, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/tanvimaira/Documents/demo/venv/lib/python3.12/site-packages/instructlab/data/generate.py\", line 198, in generate\r\n    generate_data(\r\n  File \"/Users/tanvimaira/Documents/demo/venv/lib/python3.12/site-packages/instructlab/data/generator/generate_data.py\", line 407, in generate_data\r\n    len(seed_example[\"instruction\"])\r\nTypeError: object of type 'NoneType' has no len()\r\n\r\n\r\n**Device Info (please complete the following information):**\r\n - Hardware Specs: [e.g. Apple M2 Pro Chip, 16 GB Memory, etc.] : Apple M3 16 GB\r\n - OS Version: [e.g. Mac OS 14.4.1, Fedora Linux 40] : 15.2 (24C101)\r\n - Python Version: [output of `python --version`] : python 3.12\r\n - InstructLab Version: [output of `ilab system info`] :  ilab, version 0.17.2\r\n\r\n**Additional context**\r\nI am following this video to generate the synthetic data\r\nhttps://www.youtube.com/watch?v=Jb5dh3Uzdbw&t=185s", "labels": ["bug"], "repository": "instructlab/instructlab", "url": "https://github.com/instructlab/instructlab/issues/2834", "created_at": "2024-12-23T11:51:14Z", "updated_at": "2024-12-24T11:54:54Z", "issue_number": 2834, "state": "open", "comments": 1}
{"id": "instructlab/instructlab/2820", "title": "Hard to find out the model location after model convert", "body": "**What is the current experience of ilab you've found issue with?**\r\n<!-- A clear and concise description of what the experience you'd using ilab is -->\r\n\r\n**How do you think the described experience should be?**\r\n<!-- A clear and concise description of how you think the experience you had should be -->\r\n\r\nThere are a lot of output during the `model convert`, no obvious information to tell the model location after conversion.\r\n\r\n**What is the benefit of the change you are proposing?**\r\n<!-- Justify your call to change the CLI here -->\r\n\r\n**If applicable, how would backwards-compatability be handled?**\r\n<!-- Feel free to remove this section if this does not apply -->\r\n\r\n**Additional context**\r\n<!-- Add any other context or screenshots about the design change here. -->\r\n", "labels": ["UX"], "repository": "instructlab/instructlab", "url": "https://github.com/instructlab/instructlab/issues/2820", "created_at": "2024-12-18T04:49:40Z", "updated_at": "2024-12-18T04:50:51Z", "issue_number": 2820, "state": "open", "comments": 1}
{"id": "instructlab/instructlab/2814", "title": "Add Prerequisite Script and Interactive Mode for InstructLab Onboarding for MacOS", "body": "# Overview\r\nThis issue proposes adding a single script that checks pre-requisites and automates the InstructLab setup process for MacOS users. The script ensures foundational tools are installed and guides users through directory creation, installation, and environment setup.\r\n\r\n# Problem Statement\r\nThe current onboarding flow requires users to:\r\n\r\n- Check for foundational tools like Homebrew, Python 3.11, Git, and GitHub CLI manually.\r\n- Execute multiple commands step-by-step, increasing complexity and potential for errors.\r\n- Follow scattered instructions for setup without automation.\r\nThis creates unnecessary friction, especially for non-technical users, and slows down the onboarding process.\r\n\r\n# Proposed Solution\r\nA unified bash script (instructlab-demo.sh) will:\r\n\r\n1. Check and install pre-requisites (Homebrew, Python 3.11, Git, GitHub CLI, and VS Code).\r\n2. Guide users through setting up directories and virtual environments.\r\n3. Automate InstructLab installation and verify the installation step-by-step.\r\n4. Provide clear prompts, status updates, and color-coded feedback for a streamlined experience.\r\n\r\n# Major Changes\r\n- **Pre-requisites Automation**: Automatically checks and installs Homebrew, Python 3.11, Git, and GitHub CLI.\r\n- **Interactive Setup**: Prompts the user for inputs like directory creation and version selection, streamlining user decisions.\r\n- **Error Reduction**: Reduces manual steps, limiting user errors during onboarding.\r\n- **Improved UX**: Clear prompts, color-coded outputs, and verification steps ensure transparency and simplicity.\r\n\r\n# Impact\r\nThis script will significantly streamline the onboarding process by automating pre-requisite checks and setup. It enhances user experience, reduces friction for novice users, and ensures consistency for advanced users.\r\n\r\n# Documentation\r\n- [Documentation Repo](https://github.com/JustinXHale/instructlabdocs)\r\n- [Documentation GitHub Page](https://justinxhale.github.io/instructlabdocs/)\r\n\r\n# Script\r\n\r\n<details> Expand to see code\r\n\r\n```\r\n\r\n#!/bin/bash\r\n\r\n# ============================================\r\n# Combined InstructLab Installation Script\r\n# Simulates the full onboarding flow for InstructLab\r\n# ============================================\r\n\r\n# Function for green 'status_ok' messages\r\nstatus_ok() {\r\n  echo -e \"\\033[0;32m\u2705 $1\\033[0m\"\r\n}\r\n\r\n# Function for red 'status_error' messages\r\nstatus_error() {\r\n  echo -e \"\\033[0;31m\u274c $1\\033[0m\"\r\n}\r\n\r\n# Function to display commands in purple\r\npurple_text() {\r\n  echo -e \"\\033[0;35m$1\\033[0m\"\r\n}\r\n\r\n# Function to display information in light blue\r\nlight_blue_text() {\r\n  echo -e \"\\033[94m$1\\033[0m\"\r\n}\r\n\r\n# Function for gold-colored prompts\r\ngold_text() {\r\n  echo -e \"\\033[1;33m$1\\033[0m\"\r\n}\r\n\r\n# Function to highlight text\r\nhighlight() {\r\n  echo -e \"\\033[44m$1\\033[0m\"\r\n}\r\n\r\n# Function for consistent line spacing\r\nadd_spacing() {\r\n  echo\r\n}\r\n\r\n# ============================================\r\n# Welcome Message\r\n# ============================================\r\necho \"======================================================\"\r\necho \"  Welcome to InstructLab's Interactive Mode\"\r\necho \"  This mode will guide you through the installation.\"\r\necho \"======================================================\"\r\necho\r\necho \"This guided setup process is designed to get you up and running \r\nwith InstructLab as quickly and efficiently as possible. Whether you\u2019re \r\nnew to using a command-line interface (CLI) or an experienced user, \r\nthis interactive mode ensures everything is configured step-by-step \r\nwithout missing a detail.\"\r\necho \"Here\u2019s what we\u2019ll cover in this setup:\"\r\necho \"\r\n- Install pre-requisites\r\n- Account Setup: Set up your Hugging Face and GitHub accounts for model downloads and contributions. \r\n- Installation: Configure directories, install the necessary dependencies, and set up InstructLab. \r\n- Configuration: Initialize InstructLab with default or custom options based on your hardware and preferences. \r\n- Model Preparation: Download and verify your models to start using InstructLab.\"\r\nadd_spacing\r\necho \"Throughout this process: \r\n- You will be prompted with simple [Yes/No] or [Enter] options to make decisions.\r\n- You can stop at any time by pressing Ctrl+C or re-run the wizard if needed. \r\n- For advanced users, manual overrides and customization options are available.\"\r\nadd_spacing\r\necho \"By the end of this setup, you will be ready to: \r\n\u221a Chat with pre-trained models.\r\n\u221a Add knowledge to improve models.\r\n\u221a Train your own custom models.\"\r\nadd_spacing\r\necho \"LET'S GET STARTED\"\r\nadd_spacing\r\n\r\n# ======================================================\r\n# check if the user wants to get started with pre-reqs\r\n# =====================================================\r\n\r\ngold_text \"Do you want to proceed with the setup? [Yes/No]: \"\r\nread -r proceed_input\r\nif [[ ! \"$proceed_input\" =~ ^[Yy] ]]; then\r\n  status_error \"Setup aborted by the user. Exiting.\"\r\n  exit 1\r\nfi\r\n# ============================================\r\n# Pre-Requisites Check\r\n# ============================================\r\n\r\npre_requisites_check() {\r\n  echo \"======================================================\"\r\n  echo \"  Step 1: Checking Pre-Requisites\"\r\n  echo \"======================================================\"\r\n  highlight \"These steps ensure the user will have everything the need to install InstructLab via\r\nMacOS. This pre-req has add the the installation on GitHub CLI and VS Code. Doing this ensures\r\nour users have the right tools from start to finish.\"\r\n  add_spacing\r\n\r\n# Simulate Homebrew Check\r\npurple_text \"brew --version\"\r\nsleep 1\r\necho \"Homebrew not found.\"\r\n\r\necho \"Installing Homebrew...\"\r\nsleep 1\r\nstatus_ok \"Homebrew successfully installed.\"\r\nadd_spacing\r\n\r\n# Simulate Python Check\r\npurple_text \"python3 --version\"\r\nsleep 1\r\necho \"Python not found.\"\r\n\r\necho \"Installing Python 3.11...\"\r\nsleep 1\r\nstatus_ok \"Python 3.11 successfully installed.\"\r\nadd_spacing\r\n\r\n# Simulate Git Check\r\npurple_text \"git --version\"\r\nsleep 1\r\necho \"Git not found.\"\r\n\r\necho \"Installing Git...\"\r\nsleep 1\r\nstatus_ok \"Git successfully installed.\"\r\nadd_spacing\r\n\r\n# Simulate VS Code Check\r\npurple_text \"code --version\"\r\nsleep 1\r\necho \"VS Code not found.\"\r\n\r\necho \"Installing VS Code ...\"\r\nsleep 1\r\nstatus_ok \"VS Code successfully installed.\"\r\nadd_spacing\r\n\r\n# Simulate GitHub CLI Check\r\npurple_text \"gh --version\"\r\nsleep 1\r\necho \"GitHub CLI not found.\"\r\n\r\necho \"Installing GitHub CLI...\"\r\nsleep 1\r\nstatus_ok \"GitHub CLI successfully installed.\"\r\nadd_spacing\r\necho \"======================================================\"\r\n\r\n  # Check Homebrew\r\n  if ! command -v brew &>/dev/null; then\r\n    status_error \"Homebrew not found. Installing...\"\r\n    /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\r\n    status_ok \"Homebrew installed successfully.\"\r\n  else\r\n    status_ok \"Homebrew is already installed.\"\r\n  fi\r\n\r\n  # Check Python 3.11\r\n  if ! python3 --version | grep -q \"3.11\"; then\r\n    status_error \"Python 3.11 not found. Installing...\"\r\n    brew install python@3.11\r\n    status_ok \"Python 3.11 installed successfully.\"\r\n  else\r\n    status_ok \"Python 3.11 is already installed.\"\r\n  fi\r\n\r\n  # Check Git\r\n  if ! command -v git &>/dev/null; then\r\n    status_error \"Git not found. Installing...\"\r\n    brew install git\r\n    status_ok \"Git installed successfully.\"\r\n  else\r\n    status_ok \"Git is already installed.\"\r\n  fi\r\n\r\n  # Check VS Code\r\n  if ! command -v code &>/dev/null; then\r\n    status_error \"VS Code not found. Installing...\"\r\n    brew install --cask visual-studio-code\r\n    status_ok \"VS Code installed successfully.\"\r\n  else\r\n    status_ok \"VS Code is already installed.\"\r\n  fi\r\n\r\n  # Check GitHub CLI\r\n  if ! command -v gh &>/dev/null; then\r\n    status_error \"GitHub CLI not found. Installing...\"\r\n    brew install gh\r\n    status_ok \"GitHub CLI installed successfully.\"\r\n  else\r\n    status_ok \"GitHub CLI is already installed.\"\r\n  fi\r\n\r\n  add_spacing\r\n  status_ok \"All pre-requisites are installed.\"\r\n}\r\n\r\n# ============================================\r\n# Interactive Mode\r\n# ============================================\r\nadd_spacing\r\ninteractive_mode() {\r\n  echo \"======================================================\"\r\n  echo \"  Step 2: Interactive Installation Mode\"\r\n  echo \"======================================================\"\r\n  highlight \"Saving your Hugging Face token locally simplifies model downloads later.\"\r\n  add_spacing\r\n\r\n  # Hugging Face Token Setup\r\necho \"Follow these steps to create your token:\"\r\necho \"1. Visit https://huggingface.co/.\"\r\necho \"2. Create an account if you don\u2019t have one.\"\r\necho \"3. Go to 'Settings > Access Tokens'.\"\r\necho \"4. Create a token with 'READ' permissions and paste it below.\"\r\nadd_spacing  \r\ngold_text \"Enter your Hugging Face Token (Press [ENTER] to skip): \"\r\n  read -r hf_token\r\n  export HF_TOKEN=\"$hf_token\"\r\n  status_ok \"Hugging Face token saved successfully.\"\r\n  add_spacing\r\n\r\n  # GitHub Authentication\r\necho \"Connecting to GitHub Account.\"\r\nhighlight \"Using 'gh auth login' to authenticate and prepare for upstream contributions easily.\"\r\n\r\ngold_text \"Proceed to authenticate GitHub now? [Yes/Skip]: \"\r\nread -r gh_input\r\n\r\nif [[ \"$gh_input\" =~ ^[Yy] ]]; then\r\n  purple_text \"gh auth login (SIMULATION)\"\r\n  add_spacing\r\n\r\n  # Simulated Authentication Steps\r\n  echo \"Follow these steps to simulate GitHub authentication:\"\r\n  echo \"\u2714 Select 'GitHub.com'\"\r\n  echo \"\u2714 Choose HTTPS as the preferred protocol.\"\r\n  echo \"\u2714 Authenticate via a web browser.\"\r\n  echo\r\n  echo -e \"Copy this one-time code: \\033[1;33m1234-5678\\033[0m\"\r\n  read -r -p \"Press Enter to simulate browser authentication...\" fake_input\r\n\r\n  echo \"\u2714 Authentication complete.\"\r\n  echo \"\u2714 Configured git protocol: HTTPS\"\r\n  echo \"\u2714 Logged in as RedHatUXD\"\r\nelse\r\n  status_error \"GitHub connection was skipped. Continuing with remaining InstructLab setup.\"\r\nfi\r\nadd_spacing\r\n\r\nif ! gh auth status &>/dev/null; then\r\n  gold_text \"GitHub authentication required. Proceeding to 'gh auth login'...\"\r\n  purple_text \"gh auth login\"\r\n  sleep 1\r\n  gh auth login --web\r\n  if [[ $? -eq 0 ]]; then\r\n    status_ok \"GitHub account successfully authenticated.\"\r\n  else\r\n    status_error \"GitHub authentication failed. Exiting setup.\"\r\n    exit 1\r\n  fi\r\nelse\r\n  status_ok \"GitHub was successfully authenticated or skipped.\"\r\nfi\r\nadd_spacing\r\n}\r\n\r\n# ============================================\r\n# Configuration Setup (Imported from config.sh)\r\n# ============================================\r\nsleep 1\r\nconfiguration_setup() {\r\n  echo \"===================================================\"\r\n  echo \"InstructLab Configuration: 'ilab config init'\"\r\n  echo \"===================================================\"\r\n  add_spacing\r\n\r\n  echo \"This guide will help you set up your environment for InstructLab.\"\r\n  echo \"You will be prompted to provide necessary paths and choose options.\"\r\n  echo \"Press 'ENTER' to select default values when prompted.\"\r\n  add_spacing\r\n\r\n  # Step 1: Taxonomy Repository Setup\r\n  echo \"-----------------------------------------------\"\r\n  echo \"Setting up Taxonomy Repository\"\r\n  echo \"-----------------------------------------------\"\r\n  highlight \"We will now configure your taxonomy repository using GitHub CLI.\"\r\n\r\n  gold_text \"Path to taxonomy repo [Press ENTER for default '/Users/$USER/Documents/instructlab/taxonomy']: \"\r\n\r\n  default_taxonomy_path=\"/Users/$USER/Documents/instructlab/taxonomy\"\r\n  read -r taxonomy_path\r\n  taxonomy_path=${taxonomy_path:-$default_taxonomy_path}\r\n\r\n  # Simulate Path Check\r\n  if [[ ! -d \"$taxonomy_path\" || -z \"$(ls -A \"$taxonomy_path\" 2>/dev/null)\" ]]; then\r\n    status_error \"The directory '$taxonomy_path' does not exist or is empty.\"\r\n    add_spacing\r\n\r\n    gold_text \"How would you like to set up your taxonomy?\"\r\n    echo \"[1 - Default] Fork the taxonomy repo to your GitHub account and clone your fork.\"\r\n    echo \"[2] Clone the upstream taxonomy repo directly (read-only mode).\"\r\n    echo \"[3] Use an existing taxonomy directory.\"\r\n    add_spacing\r\n\r\n    gold_text \"Enter the number of your choice [1 - Recommended]: \"\r\n    read -r setup_choice\r\n    add_spacing\r\n\r\n    case \"$setup_choice\" in\r\n      1)\r\n        purple_text \"gh repo fork instructlab/taxonomy --clone\"\r\n        echo \"Forking https://github.com/instructlab/taxonomy.git...\"\r\n        echo \"Cloning https://github.com/yourusername/taxonomy.git to $taxonomy_path\"\r\n        status_ok \"Fork and clone simulated successfully.\"\r\n        ;;\r\n      2)\r\n        purple_text \"git clone https://github.com/instructlab/taxonomy.git $taxonomy_path\"\r\n        echo \"Cloning upstream taxonomy repository to $taxonomy_path...\"\r\n        status_ok \"Cloning upstream repository simulated successfully.\"\r\n        ;;\r\n      3)\r\n        gold_text \"Using existing taxonomy directory: $taxonomy_path\"\r\n        status_ok \"Existing taxonomy directory accepted.\"\r\n        ;;\r\n      *)\r\n        status_error \"Invalid choice. Exiting setup.\"\r\n        exit 1\r\n        ;;\r\n    esac\r\n  fi\r\n  add_spacing\r\n\r\n  # Simulate Remote Configuration\r\n  echo \"\u2714 Remote repositories configured:\"\r\n  echo \"- origin: https://github.com/yourusername/taxonomy (your fork)\"\r\n  echo \"- upstream: https://github.com/instructlab/taxonomy (original repository)\"\r\n  add_spacing\r\n  sleep 1\r\n\r\n  purple_text \"git remote -v\"\r\n  echo \"origin    https://github.com/yourusername/taxonomy (fetch)\"\r\n  echo \"origin    https://github.com/yourusername/taxonomy (push)\"\r\n  echo \"upstream  https://github.com/instructlab/taxonomy (fetch)\"\r\n  echo \"upstream  https://github.com/instructlab/taxonomy (push)\"\r\n  add_spacing\r\n  sleep 1\r\n\r\n  # Step 2: Model Path Configuration\r\n  echo \"-----------------------------------------------\"\r\n  echo \"Setting up Model Directory\"\r\n  echo \"-----------------------------------------------\"\r\n  gold_text \"Path to your model [Press ENTER for default '/Users/$USER/Documents/instructlab/models']: \"\r\n  sleep 1\r\n\r\n  default_model_path=\"/Users/$USER/Documents/instructlab/models\"\r\n  read -r model_path\r\n  model_path=${model_path:-$default_model_path}\r\n  add_spacing\r\n  sleep 1\r\n\r\n  purple_text \"mkdir -p $model_path\"\r\n  status_ok \"Model directory simulated as created: $model_path\"\r\n  add_spacing\r\n  sleep 1\r\n\r\n  # Step 3: Generate Configuration & Profile Files\r\n  echo \"-----------------------------------------------\"\r\n  echo \"Generating Configuration Files\"\r\n  echo \"-----------------------------------------------\"\r\n  purple_text \"Simulating generation of config.yaml and system_profiles...\"\r\n  sleep 1\r\n\r\n  echo \"\u2714 Configuration files generated successfully:\"\r\n  echo \"- Config file: $taxonomy_path/config/config.yaml\"\r\n  echo \"- System profiles: $taxonomy_path/config/system_profiles\"\r\n  add_spacing\r\n  sleep 1\r\n\r\n  # Completion Message\r\n  echo \"===================================================\"\r\n  status_ok \"InstructLab Configuration Completed Successfully!\"\r\n  light_blue_text \"Next Step: Run 'ilab model download' to initialize InstructLab.\"\r\n  echo \"===================================================\"\r\n}\r\n\r\n# ============================================\r\n# Model Download\r\n# ============================================\r\nmodel_download() {\r\n  echo \"======================================================\"\r\n  echo \"  Step 4: Downloading Models\"\r\n  echo \"======================================================\"\r\n  add_spacing\r\n\r\n  purple_text \"ilab model download --hf-token $HF_TOKEN\"\r\n  add_spacing\r\n  echo \"Downloading models...\"\r\n  sleep 2\r\n  status_ok \"Model download completed successfully.\"\r\n  add_spacing\r\n\r\n  # List Models\r\n  echo \"Listing available models:\"\r\n  purple_text \"ilab model list\"\r\n  add_spacing\r\n  sleep 1\r\n  echo \"granite-7b-lab-Q4_K_M.gguf    | 4.1 GB | Interaction Model\"\r\n  echo \"mistral-7b-instruct-v0.2.Q4_K | 4.1 GB | Teacher Model\"\r\n  echo \"merlinite-7b-lab-Q4_K_M.gguf  | 2.5 GB | Testing/Fine-Tune\"\r\n  add_spacing\r\n}\r\n\r\n# ============================================\r\n# Main Function\r\n# ============================================\r\nmain() {\r\n  pre_requisites_check\r\n  interactive_mode\r\n  configuration_setup\r\n  model_download\r\necho \"======================================================\"\r\n  status_ok \"InstructLab installation and configuration completed successfully!\"\r\necho \"======================================================\"\r\n  light_blue_text \"Next Step: Run 'ilab model serve' to start using the model.\"\r\n  echo \"Exiting Interactive Mode. Enjoy INSTRUCTLAB!\"\r\n  add_spacing\r\n}\r\n\r\n# Run Main\r\nmain\r\n```\r\n\r\n</details>", "labels": ["enhancement"], "repository": "instructlab/instructlab", "url": "https://github.com/instructlab/instructlab/issues/2814", "created_at": "2024-12-17T16:04:29Z", "updated_at": "2024-12-18T19:07:44Z", "issue_number": 2814, "state": "open", "comments": 2}
{"id": "instructlab/instructlab/2791", "title": "add the print tips for multiline mode in chat", "body": "**What is the current experience of ilab you've found issue with?**\r\n<!-- A clear and concise description of what the experience you'd using ilab is -->\r\n\r\n `Press Alt (or Meta) and Enter or Esc Enter to end multiline input`  in the help not a good tips, should show it after enabled multiline mode\r\n```\r\n>>> /h                                                                                           [S][default]\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 system \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 Help / TL;DR                                                                                              \u2502\r\n\u2502                                                                                                           \u2502\r\n\u2502  \u2022 /q: quit                                                                                               \u2502\r\n\u2502  \u2022 /h: show help                                                                                          \u2502\r\n\u2502  \u2022 /a assistant: amend assistant (i.e., model)                                                            \u2502\r\n\u2502  \u2022 /c context: change context (available contexts: default, cli_helper)                                   \u2502\r\n\u2502  \u2022 /lc: list contexts                                                                                     \u2502\r\n\u2502  \u2022 /m: toggle multiline (for the next session only)            <<<<                                           \u2502\r\n\u2502  \u2022 /M: toggle multiline                                                         <<<<                          \u2502\r\n\u2502  \u2022 /n: new session                                                                                        \u2502\r\n\u2502  \u2022 /N: new session (ignoring loaded)                                                                      \u2502\r\n\u2502  \u2022 /d <int>: display previous response based on input, if passed 1 then previous, if 2 then second last   \u2502\r\n\u2502    response and so on.                                                                                    \u2502\r\n\u2502  \u2022 /p <int>: previous response in plain text based on input, if passed 1 then previous, if 2 then second  \u2502\r\n\u2502    last response and so on.                                                                               \u2502\r\n\u2502  \u2022 /md <int>: previous response in Markdown based on input, if passed 1 then previous, if 2 then second   \u2502\r\n\u2502    last response and so on.                                                                               \u2502\r\n\u2502  \u2022 /s filepath: save current session to filepath                                                          \u2502\r\n\u2502  \u2022 /l filepath: load filepath and start a new session                                                     \u2502\r\n\u2502  \u2022 /L filepath: load filepath (permanently) and start a new session                                       \u2502\r\n\u2502                                                                                                           \u2502\r\n\u2502 Press Alt (or Meta) and Enter or Esc Enter to end multiline input.  <<<  not a good user experience for tips                              \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\n```\r\n\r\n**How do you think the described experience should be?**\r\n<!-- A clear and concise description of how you think the experience you had should be -->\r\n\r\n**What is the benefit of the change you are proposing?**\r\n<!-- Justify your call to change the CLI here -->\r\n\r\n**If applicable, how would backwards-compatability be handled?**\r\n<!-- Feel free to remove this section if this does not apply -->\r\n\r\n**Additional context**\r\n<!-- Add any other context or screenshots about the design change here. -->\r\n", "labels": ["UX"], "repository": "instructlab/instructlab", "url": "https://github.com/instructlab/instructlab/issues/2791", "created_at": "2024-12-13T01:58:18Z", "updated_at": "2024-12-13T02:02:43Z", "issue_number": 2791, "state": "open", "comments": 1}
{"id": "instructlab/instructlab/2787", "title": "Add ability to convert models to OCI format directories", "body": "**Is your feature request related to a problem? Please describe.**\r\nIn order to be able to support uploading models to OCI registries, ilab must first be able to convert the provided model into an OCI layout directory locally\r\n\r\n**Describe the solution you'd like**\r\nilab should include logic that allows it to construct an OCI layout directory out of a supplied model directory. This functionality should be internally invoked within `ilab model upload` if user is targeting an OCI registry as the destination for upload.\r\nThis functionality should separately be exposed as an option via `ilab model convert`\r\n\r\nThe general workflow needed to be followed is:\r\n\r\n- create a new local directory\r\n- find the SHA for each file in the model dir, and rewriting the files into a blobs/ dir in the new directory\r\n- add a manifest json in the blobs dir that references all model files via their SHAs and other required info \r\n- create an index.json that references the manifest sha\r\n- add an oci-layout file\r\n\r\n**Additional context**\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n", "labels": ["enhancement", "jira"], "repository": "instructlab/instructlab", "url": "https://github.com/instructlab/instructlab/issues/2787", "created_at": "2024-12-12T18:51:53Z", "updated_at": "2024-12-12T18:56:09Z", "issue_number": 2787, "state": "open", "comments": 0}
{"id": "instructlab/instructlab/2777", "title": "Make running vLLM optional when running MMLU evaluations", "body": "**What is the current experience of ilab you've found issue with?**\n\nThe instructlab/eval library supports running MMLU without a server by directly evaluating the HuggingFace model. This avoids the requirement of the user needing to rely on vLLM for doing evaluations, which may sometimes be desired if unwanted behavior arises.\n\n\n**How do you think the described experience should be?**\n\nThere could be an optional flag such as `--no-vllm` or maybe `--with-hf` indicating to run without vLLM or just with the huggingface library (instead of vLLM).\n\nThis would prevent the vLLM server from being created.\n\n\n**What is the benefit of the change you are proposing?**\n\nThis way users can run evaluation without vLLM if they just want to see how the model performs when the raw model is loaded.\n\n**If applicable, how would backwards-compatability be handled?**\n\nN/a\n\n\n**Additional context**\n\n\nThe instructlab/eval supports this but the CLI doesn't. Not that the CLI needs to support everything just because a library does, but this seems like a nice UX feature to have.\n\n", "labels": ["UX", "evaluation"], "repository": "instructlab/instructlab", "url": "https://github.com/instructlab/instructlab/issues/2777", "created_at": "2024-12-12T02:48:31Z", "updated_at": "2024-12-13T20:29:02Z", "issue_number": 2777, "state": "open", "comments": 0}
{"id": "instructlab/instructlab/2776", "title": "Increase the number of epochs for the XL t-shirt e2e job to ensure we're effectively testing the model", "body": "**Is your feature request related to a problem? Please describe.**\r\n<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->\r\n\r\nPR #2743 introduced a new XL t-shirt size e2e job, but because the new git workflow introduced in that PR could not be tested easily before merging, we decided to temporarily set `num_epochs`, `phased_phase1_num_epochs`, and `phased_phase2_num_epochs` all equal to `4` so that we could initially ensure the XL e2e job would take <24 hours. By limiting the total runtime, we can quickly triage any discovered bugs or issues. However, the major downside to having a low number of epochs (i.e., <6) for this setup is that we can't effectively assess how well the model has learned, which means the XL e2e job doesn't give us much useful information at this point.\r\n\r\n**Describe the solution you'd like**\r\n<!-- A clear and concise description of what you want to happen. -->\r\nWe'd like to increase the number of epochs from 4 to at least 6 or 7 (with 7 being preferred), but anyone is welcome to suggest other numbers if they think 6 or 7 is insufficient.\r\n\r\n**Additional context**\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\nN/A\r\n", "labels": ["enhancement", "CI/CD"], "repository": "instructlab/instructlab", "url": "https://github.com/instructlab/instructlab/issues/2776", "created_at": "2024-12-11T18:50:47Z", "updated_at": "2024-12-20T17:24:04Z", "issue_number": 2776, "state": "open", "comments": 3}
{"id": "instructlab/instructlab/2772", "title": "AMD ROCm 6.3: Allow for torch==2.5.1 ", "body": "AMD ROCm 6.3 requires torch==2.5.1 support.  However, instructlab only supports torch>=2.3.0,<2.5.0 (from requirements.txt).  Please update to at least allow 2.5.1.\r\n", "labels": ["enhancement"], "repository": "instructlab/instructlab", "url": "https://github.com/instructlab/instructlab/issues/2772", "created_at": "2024-12-10T13:36:56Z", "updated_at": "2024-12-18T15:45:27Z", "issue_number": 2772, "state": "open", "comments": 0}
{"id": "instructlab/instructlab/2747", "title": "InstructLab for German Knowledge and Skills", "body": "Hi, Merlinite (Mistral) and Granite multilingual should work well for fine-tuning student models with German content. How can InstructLab be used for German scenarios? \r\n\r\nVia a proxy I could see the exact English prompts (https://heidloff.net/article/synthetic-data-generation-instructlab/). Can you overwrite these prompts in German language? There are files like common.py and prompt.txt that encapsulate some of this functionality. Is there documentation how to change them for other languages than English?\r\n\r\nThanks\r\nNiklas", "labels": ["enhancement"], "repository": "instructlab/instructlab", "url": "https://github.com/instructlab/instructlab/issues/2747", "created_at": "2024-12-05T12:13:07Z", "updated_at": "2024-12-05T12:13:07Z", "issue_number": 2747, "state": "open", "comments": 0}
{"id": "instructlab/instructlab/2746", "title": "Automate issue status update to \"Investigate\" when a Core Maintainer/Triager comments on a 'New' Issue", "body": "Currently we have to manually update the issue status to Investigation once a CLI Triager/Maintainer responds to a new issue. This action isn't available in the preset project workflows, so I'll need to create and merge an action workflow into main. \r\n\r\nThis issue is to track that work and request for reviews from folks on the CLI team. Whenever this gets approved and merged, I'll mirror the same automation into the other library repos.", "labels": ["chore"], "repository": "instructlab/instructlab", "url": "https://github.com/instructlab/instructlab/issues/2746", "created_at": "2024-12-04T22:25:19Z", "updated_at": "2024-12-16T15:59:50Z", "issue_number": 2746, "state": "open", "comments": 4}
{"id": "instructlab/instructlab/2719", "title": "Provide self test command to check for common installation problems", "body": "**Is your feature request related to a problem? Please describe.**\r\nIn downstream packaging of the current release, we had several cases of missing dependencies, packages, and system libraries. We would like to detect these types of issues as early as possible in our build and release process.\r\n\r\n**Describe the solution you'd like**\r\nIt would help us if InstructLab would have a selftest command (e.g. `ilab system selftest`) that performs basic checks of its installation. In downstream, we could then install InstructLab for each hardware variant and run the self test.\r\n\r\n- import all `instructlab.command` modules and submodules to verify that all imports are working\r\n- perform imports of other packages that are loaded lazily (vLLM, DeepSpeed, Docling extensions?)\r\n- verify Torch build flags like \"Is torch compiled with CUDA, ROCm, Gaudi support\"\r\n- verify Docling's OCR (is tesserocr working?)\r\n- DeepSpeed ops\r\n- ...\r\n\r\nA self test should work on systems without accelerator hardware. The `selftest` probably needs an option to tell it to test for `cuda`, `rocm`, or `hpu` support. There could be another flag that enables additional checks with hardware.\r\n\r\nDocling has multiple OCR backends, too. In downstream we want to verify that tesserocr is available and working. Upstream might want to test for easyocr.", "labels": ["enhancement", "testing", "dependencies", "build/installation"], "repository": "instructlab/instructlab", "url": "https://github.com/instructlab/instructlab/issues/2719", "created_at": "2024-11-28T11:03:51Z", "updated_at": "2024-11-28T16:03:38Z", "issue_number": 2719, "state": "open", "comments": 0}
{"id": "instructlab/instructlab/2718", "title": "Add Homebrew Support for Instructlab CLI Setup", "body": "**Is your feature request related to a problem? Please describe.**\n\nCurrently, setting up the instructlab CLI requires developers to manually manage Python versions using tools like pyenv, which can be challenging, especially for those who are unfamiliar with Python environment management. This adds friction to the installation process and impacts the overall developer experience, particularly on macOS.\n\n**Describe the solution you'd like**\n\nI propose adding support for installing the instructlab CLI via Homebrew, a popular package manager for macOS (and Linux). By creating a Homebrew formula for the CLI, developers can easily install and manage the tool with a single command:\n\n''' brew install instructlab '''\n\nHomebrew will handle the version management which will simplify the setup .\nAlso better developer expierence as homebrew formula will create a virtual env for this setup to avoid impacting system-user machine  \n\n**Additional context**\n\nHigh level implementation steps : \n\nSpecify the Python version as a dependency (depends_on \"python@3.11\").assuming 3.11 would be the stable version for most of upcoming cli releases .\nUse a virtual environment (venv) to isolate the CLI and its dependencies.\nSymlink the CLI binary for global usage.\nAdd a basic test to verify the installation.\n\n'''brew install instructlab\ninstructlab --version\n'''\n\nI am happy to contribute to implementing this feature. I can:\n\nWrite the Homebrew formula and test it locally.\nProvide a pull request with the formula or set up a dedicated tap repository (my-org/homebrew-instructlab).\nDocument the new installation process in the README file.\nLet me know if this proposal aligns with the goals of the project or if there are any specific requirements to consider.", "labels": ["enhancement", "macOS", "build/installation"], "repository": "instructlab/instructlab", "url": "https://github.com/instructlab/instructlab/issues/2718", "created_at": "2024-11-28T10:36:07Z", "updated_at": "2024-12-18T01:33:38Z", "issue_number": 2718, "state": "open", "comments": 9}
{"id": "aurelio-labs/semantic-chunkers/29", "title": "please loosen up package dependencies", "body": "Hi!\r\nit would be great if the package dependencies could be loosened up a bit in the pyproject.toml file because the current state prevents me from installing another package in an environment where I'm also using semantic-chunkers.\r\nConcretely, I want to install https://github.com/VikParuchuri/marker which `depends on regex (>=2024.4.28,<2025.0.0)` and get in conflict w/ semantic-chunker's dependency `depends on regex (>=2023.12.25,<2024.0.0)`. I think often times the dependency is just set w/o much thought but actually very, and too strict. Probably semantic-chunkers will also work w/ newer versions of regex?\r\n\r\nThank you!", "labels": [], "repository": "aurelio-labs/semantic-chunkers", "url": "https://github.com/aurelio-labs/semantic-chunkers/issues/29", "created_at": "2024-12-17T22:23:30Z", "updated_at": "2024-12-17T22:23:30Z", "issue_number": 29, "state": "open", "comments": 0}
{"id": "AnswerDotAI/byaldi/70", "title": "Is the latest version 0.0.5 or 0.0.7?", "body": "Pypi is the source of truth I suspect.  I Just want to make sure I'm looking at the right code in your repo when as I'm testing this out to replace my current rag system.\r\n\r\n```\r\n$ pip freeze | grep Byaldi\r\nByaldi==0.0.7\r\n```\r\nofficial on github?\r\n\r\n![Untitled](https://github.com/user-attachments/assets/17da96cb-49e9-439f-8f4c-6c8639364873)\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/47ea0ecb-3cfc-4cb5-93e1-4535a4b966dc)\r\n\r\n\r\nI'm using:\r\n\r\n   ` rag_model_name: str = \"vidore/colqwen2-v1.0\"`\r\n", "labels": [], "repository": "AnswerDotAI/byaldi", "url": "https://github.com/AnswerDotAI/byaldi/issues/70", "created_at": "2024-12-15T07:53:15Z", "updated_at": "2024-12-15T07:53:15Z", "issue_number": 70, "state": "open", "comments": 0}
{"id": "mixedbread-ai/batched/14", "title": "Errors during process end", "body": "When a container is ending or the process is closing always there is this error:\r\n\r\n```\r\nan error occurred during closing of asynchronous generator <async_generator object AsyncBatchGenerator.optimal_batches at 0x7f25b8642e60>\r\nasyncgen: <async_generator object AsyncBatchGenerator.optimal_batches at 0x7f25b8642e60>\r\nRuntimeError: aclose(): asynchronous generator is already running\r\n```", "labels": [], "repository": "mixedbread-ai/batched", "url": "https://github.com/mixedbread-ai/batched/issues/14", "created_at": "2024-12-04T10:29:38Z", "updated_at": "2024-12-08T10:33:38Z", "issue_number": 14, "state": "open", "comments": 1}
{"id": "robusta-dev/holmesgpt/230", "title": "question about install Holmesgpt in k8s", "body": "can I directly install Holmesgpt in my k8s cluster with out Robusta, How to send prometheus alert to holmesgpt in the simplest way, just for testing purposes", "labels": [], "repository": "robusta-dev/holmesgpt", "url": "https://github.com/robusta-dev/holmesgpt/issues/230", "created_at": "2024-12-13T11:41:59Z", "updated_at": "2024-12-13T11:52:03Z", "issue_number": 230, "state": "open", "comments": 1}
{"id": "koaning/uvtrick/25", "title": "how to run this", "body": "```\r\n\u279c /workspaces/uvtrick (main) $ uv run --with uvtrick --with scikit-learn examples/kmeans.py\r\nInstalled 8 packages in 186ms\r\nTraceback (most recent call last):d\r\n  File \"/tmp/tmpi8nj__rj/pytemp.py\", line 18, in <module>\r\n    args, kwargs = cloudpickle.loads(Path('/tmp/tmpi8nj__rj/pickled_inputs.pickle').read_bytes())\r\nValueError: unsupported pickle protocol: 5\r\nTraceback (most recent call last):\r\n  File \"/workspaces/uvtrick/examples/kmeans.py\", line 31, in <module>\r\n    timed = Env(f\"scikit-learn=={version}\", python=py).run(bench)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/codespace/.cache/uv/archive-v0/wzWov4QbS1pynQbwmt-Rd/lib/python3.12/site-packages/uvtrick/__init__.py\", line 144, in run\r\n    subprocess.run(self.cmd, cwd=temp_dir, check=True)\r\n  File \"/home/codespace/.python/current/lib/python3.12/subprocess.py\", line 571, in run\r\n    raise CalledProcessError(retcode, process.args,\r\nsubprocess.CalledProcessError: Command '['uv', 'run', '--with=cloudpickle', '--quiet', '--with=scikit-learn==0.23', '--python=3.7', '/tmp/tmpi8nj__rj/pytemp.py']' returned non-zero exit status 1.d\r\n```", "labels": [], "repository": "koaning/uvtrick", "url": "https://github.com/koaning/uvtrick/issues/25", "created_at": "2024-12-10T22:05:02Z", "updated_at": "2024-12-11T13:24:31Z", "issue_number": 25, "state": "open", "comments": 1}
{"id": "hinthornw/trustcall/17", "title": "Prerequisites", "body": "As I understand, Trustcall is calling a function by name in the parameter tool_choice.\r\nIn Mistral, tool_choice doesn't support to call a function name but only \"auto\", \"none\", \"required\" and \"any\"\r\nSo an LLM supporting function calling isn't enough as a prerequisites. It needs this function calling by name feature as well.", "labels": [], "repository": "hinthornw/trustcall", "url": "https://github.com/hinthornw/trustcall/issues/17", "created_at": "2024-11-26T10:39:30Z", "updated_at": "2024-12-10T20:45:25Z", "issue_number": 17, "state": "open", "comments": 1}
{"id": "AgentOps-AI/AgentStack/151", "title": "Expand logging capabilities to handle all messages from within the application", "body": "In order to prevent the inclusion of `print` statements within the framework, use `agentstack.log` to handle human-readable output. \r\n\r\nGeneral messages and errors will be sent to `log.info` and `log.error`, respectively, and be printed in the terminal if we are in a CLI environment.\r\n\r\nDebug messages will be sent to `log.debug` and be visible in the project's log file. We can also display debug info in the CLI if the user passes a verbose tag (`-v` or `--verbose`). \r\n\r\nWe could also catch exceptions and print them to the terminal with a friendly message, and include the full traceback in the log. ", "labels": [], "repository": "AgentOps-AI/AgentStack", "url": "https://github.com/AgentOps-AI/AgentStack/issues/151", "created_at": "2024-12-18T17:56:58Z", "updated_at": "2024-12-19T00:18:47Z", "issue_number": 151, "state": "open", "comments": 0}
{"id": "AgentOps-AI/AgentStack/141", "title": "Code from other parts of the application should always throw exceptions and leave the CLI to handle error messaging and control flow. ", "body": "In a few places outside of `agentstack.cli` we're printing error messages and calling `sys.exit`. \r\n\r\nWe should ensure any exit calls exist only within the `cli` package and that all other code in the repo throws exceptions. ", "labels": [], "repository": "AgentOps-AI/AgentStack", "url": "https://github.com/AgentOps-AI/AgentStack/issues/141", "created_at": "2024-12-14T16:49:25Z", "updated_at": "2024-12-15T20:02:10Z", "issue_number": 141, "state": "open", "comments": 1}
{"id": "AgentOps-AI/AgentStack/128", "title": "Store a README with project templates for setup instructions", "body": null, "labels": [], "repository": "AgentOps-AI/AgentStack", "url": "https://github.com/AgentOps-AI/AgentStack/issues/128", "created_at": "2024-12-11T20:38:51Z", "updated_at": "2024-12-11T20:38:51Z", "issue_number": 128, "state": "open", "comments": 0}
{"id": "AgentOps-AI/AgentStack/112", "title": "Friendlier messaging when a tool does not have an API key", "body": "When an API key is not configured in the .env (for example after initializing a project with a template) clean up the stack trace that's printed with a clearer explanation. ", "labels": [], "repository": "AgentOps-AI/AgentStack", "url": "https://github.com/AgentOps-AI/AgentStack/issues/112", "created_at": "2024-12-09T21:19:10Z", "updated_at": "2024-12-09T21:19:11Z", "issue_number": 112, "state": "open", "comments": 0}
{"id": "AgentOps-AI/AgentStack/108", "title": "Write tests for asttools", "body": "`asttools` gets tested via the framework implementations, but having explicit tests for the functions we support would be nice, too.", "labels": ["good first issue", "tests"], "repository": "AgentOps-AI/AgentStack", "url": "https://github.com/AgentOps-AI/AgentStack/issues/108", "created_at": "2024-12-09T18:20:28Z", "updated_at": "2024-12-20T09:07:23Z", "issue_number": 108, "state": "open", "comments": 4}
{"id": "AgentOps-AI/AgentStack/99", "title": "Document the process for adding a new framework", "body": null, "labels": [], "repository": "AgentOps-AI/AgentStack", "url": "https://github.com/AgentOps-AI/AgentStack/issues/99", "created_at": "2024-12-05T03:53:32Z", "updated_at": "2024-12-05T03:53:32Z", "issue_number": 99, "state": "open", "comments": 0}
{"id": "AgentOps-AI/AgentStack/90", "title": "Accept templates from file system", "body": "Currently if you include the `--template` argument in your command, it first checks the internal template folder, then checks if the arg value has `https://` and attempts to fetch it.\r\n\r\nIf not `https://` and not in the templates folder, check if a file exists at the path of the arg value, if so, validate it as a template file (#85). if it vallidates, use it as a template", "labels": ["good first issue"], "repository": "AgentOps-AI/AgentStack", "url": "https://github.com/AgentOps-AI/AgentStack/issues/90", "created_at": "2024-12-04T05:52:57Z", "updated_at": "2024-12-04T05:52:57Z", "issue_number": 90, "state": "open", "comments": 0}
{"id": "AgentOps-AI/AgentStack/88", "title": "Write tests for the auto-updater", "body": "We currently have no testing on our package updating\r\n\r\nRelevant files: `agentstack/packaging.py` and `agentstack/update.py`\r\n", "labels": ["good first issue", "tests"], "repository": "AgentOps-AI/AgentStack", "url": "https://github.com/AgentOps-AI/AgentStack/issues/88", "created_at": "2024-12-03T23:56:58Z", "updated_at": "2024-12-12T16:30:32Z", "issue_number": 88, "state": "open", "comments": 4}
{"id": "AgentOps-AI/AgentStack/83", "title": "Implement Agency Swarms", "body": "[VRSEN Agency Swarms](https://github.com/VRSEN/agency-swarm?tab=readme-ov-file) is another opinionated multi-agent framework. This shares patterns similar enough to Crew that using an agent/task generation method shouldn't be terribly difficult.\r\n\r\n- Tools can be built to inherit from their `BaseTool` class\r\n- Agents can be generated with AST\r\n  - By default, every agent should be able to talk with one another\r\n  - Maybe we optionally set up a hierarchical model for the developer in which a manager agent communicates between agents\r\n- Tasks are added to a markdown file \r\n\r\nI'm also not opposed to keeping the prompt abstraction that Crew has and dynamically inserting the task prompts to the markdown file on `agentstack run`. The agents themselves would have prompts abstracted to yaml as well.", "labels": [], "repository": "AgentOps-AI/AgentStack", "url": "https://github.com/AgentOps-AI/AgentStack/issues/83", "created_at": "2024-12-03T10:02:06Z", "updated_at": "2024-12-03T10:02:06Z", "issue_number": 83, "state": "open", "comments": 0}
{"id": "AgentOps-AI/AgentStack/73", "title": "Update project template dependencies", "body": "Update dependencies for:\r\nagentops\r\ncrewai\r\ncrewai-tools\r\n\r\nIt may be smart to abstract these versions to a file outside of the cookiecutter template. This will become more relevant when we support multiple frameworks. Just a thought for now :)", "labels": [], "repository": "AgentOps-AI/AgentStack", "url": "https://github.com/AgentOps-AI/AgentStack/issues/73", "created_at": "2024-11-30T05:38:50Z", "updated_at": "2024-11-30T05:38:51Z", "issue_number": 73, "state": "open", "comments": 0}
{"id": "AgentOps-AI/AgentStack/69", "title": "Command for switching crew process types", "body": "A number of devs have struggled to switch from sequential mode to hierarchical mode. The Crew docs here are insufficient.\r\n\r\ni'm thinking `agentstack process hierarchical/sequential\" \r\n\r\nWe do need a way to set this for the sake of templating. It makes sense to expose that functionality with the CLI", "labels": [], "repository": "AgentOps-AI/AgentStack", "url": "https://github.com/AgentOps-AI/AgentStack/issues/69", "created_at": "2024-11-28T06:11:14Z", "updated_at": "2024-11-28T06:11:14Z", "issue_number": 69, "state": "open", "comments": 0}
{"id": "google-deepmind/treescope/51", "title": "Feature Request: distinguish the parameters that require grad and that is not for PyTorch models", "body": null, "labels": [], "repository": "google-deepmind/treescope", "url": "https://github.com/google-deepmind/treescope/issues/51", "created_at": "2024-12-06T09:32:11Z", "updated_at": "2024-12-15T07:05:34Z", "issue_number": 51, "state": "open", "comments": 1}
{"id": "erezsh/reladiff/56", "title": "AttributeError: 'Text' object has no attribute 'make_value'", "body": "**Describe the bug**\r\n\r\nHopefully this is enough, unfortunately I don't have anything more relevant/shareable. This is the first time I see this issue even though we use the same code for hundreds of tables. The diffing is done between Snowflake and Postgres tables.\r\n```\r\n    for data in set(data_diff_result):\r\n  File \"/home/user/.local/share/virtualenvs/opt-zvmYt2-H/lib/python3.10/site-packages/reladiff/diff_tables.py\", line 91, in __iter__\r\n    for i in self.diff:\r\n  File \"/home/user/.local/share/virtualenvs/opt-zvmYt2-H/lib/python3.10/site-packages/reladiff/diff_tables.py\", line 179, in _diff_tables_wrapper\r\n    yield from self._diff_tables_root(table1, table2, info_tree)\r\n  File \"/home/user/.local/share/virtualenvs/opt-zvmYt2-H/lib/python3.10/site-packages/reladiff/diff_tables.py\", line 187, in _diff_tables_root\r\n    return self._bisect_and_diff_tables(table1, table2, info_tree)\r\n  File \"/home/user/.local/share/virtualenvs/opt-zvmYt2-H/lib/python3.10/site-packages/reladiff/diff_tables.py\", line 231, in _bisect_and_diff_tables\r\n    min_key1, max_key1 = self._parse_key_range_result(key_types2, next(key_ranges))\r\n  File \"/home/user/.local/share/virtualenvs/opt-zvmYt2-H/lib/python3.10/site-packages/reladiff/diff_tables.py\", line 291, in _parse_key_range_result\r\n    min_key = Vector(key_type.make_value(mn) for key_type, mn in safezip(key_types, min_key_values))\r\n  File \"/home/user/.local/share/virtualenvs/opt-zvmYt2-H/lib/python3.10/site-packages/reladiff/diff_tables.py\", line 291, in <genexpr>\r\n    min_key = Vector(key_type.make_value(mn) for key_type, mn in safezip(key_types, min_key_values))\r\nAttributeError: 'Text' object has no attribute 'make_value'\r\n```\r\n\r\n**Describe the environment**\r\n\r\nUbuntu 22.04, Python 3.10, reladiff 0.5.3\r\n", "labels": ["bug"], "repository": "erezsh/reladiff", "url": "https://github.com/erezsh/reladiff/issues/56", "created_at": "2024-12-20T14:13:19Z", "updated_at": "2024-12-21T17:15:20Z", "issue_number": 56, "state": "open", "comments": 2}
{"id": "TY-Cheng/torchvinecopulib/32", "title": "Discrete (categorical) input data", "body": "Hello,\r\n\r\nI am currently using the torchvinecopula library and have a question regarding the incorporation of categorical (discrete) input features into the model.\r\n\r\nIs there a way to include discrete variables in the input data for the copula models? If so, could you please provide guidance or examples on how to properly encode or handle these categorical features within the framework?\r\n\r\nThank you for your help!\r\n\r\nBest regards,\r\nShahab", "labels": [], "repository": "TY-Cheng/torchvinecopulib", "url": "https://github.com/TY-Cheng/torchvinecopulib/issues/32", "created_at": "2024-12-17T11:41:04Z", "updated_at": "2024-12-19T04:53:36Z", "issue_number": 32, "state": "open", "comments": 1}
{"id": "vllm-project/llm-compressor/993", "title": "A tutorial doc for how to use the sparse prunings", "body": "Is it possible to add to the main `README.MD` a section describing how to perform pruning basics and what pruning methods are supported? From the other issues I realized it has support for **Wanda** for example. But it was not mentioned in the main ` README.md` as supported method. A simple brief tutorial would suffice to just show method steps. I am aware of the `examples/quantization_2of4_sparse_w4a16/README.md` but it is just a short example. I specifically would request a document just listing all the possible recipe options that are available. Also the different functions available such as` apply` or `oneshot` and what is the difference between them. I hope it is possible. Thanks again for your contribution. \r\n", "labels": ["documentation"], "repository": "vllm-project/llm-compressor", "url": "https://github.com/vllm-project/llm-compressor/issues/993", "created_at": "2024-12-18T20:39:04Z", "updated_at": "2024-12-18T20:39:04Z", "issue_number": 993, "state": "open", "comments": 0}
{"id": "vllm-project/llm-compressor/934", "title": "Several wandb init", "body": "**Describe the bug**\r\n\r\nHi thanks for the lib! When trying the script https://github.com/vllm-project/llm-compressor/blob/main/examples/quantization_kv_cache/llama3_fp8_kv_example.py, I see something weird: Several wandb init happens. IMHO it seems to be only a single run, so at most one init looks reasonable.\r\n\r\n```\r\n2024-11-26T05:25:38.849137+0000 | _check_create_state | INFO - State created for compression lifecycle\r\nERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\r\nwandb: Currently logged in as: ch271828n. Use `wandb login --relogin` to force relogin\r\nWaiting for wandb.init()...\r\nwandb version 0.18.7 is available! To upgrade, please run: $ pip install wandb --upgrade\r\nTracking run with wandb version 0.17.5\r\nRun data is saved locally in /host_home/research/code/research_mono/notebooks/math_ai/ad_hoc/wandb/run-20241126_052540-ggjblhrs\r\nSyncing run [glorious-mountain-19](https://wandb.ai/ch271828n/uncategorized/runs/ggjblhrs) to [Weights & Biases](https://wandb.ai/ch271828n/uncategorized) ([docs](https://wandb.me/run))\r\nView project at https://wandb.ai/ch271828n/uncategorized\r\nView run at https://wandb.ai/ch271828n/uncategorized/runs/ggjblhrs\r\n2024-11-26T05:25:52.860459+0000 | pre_initialize_structure | INFO - Compression lifecycle structure pre-initialized for 0 modifiers\r\nFinishing last run (ID:ggjblhrs) before initializing another...\r\nView run glorious-mountain-19 at: https://wandb.ai/ch271828n/uncategorized/runs/ggjblhrs\r\nView project at: https://wandb.ai/ch271828n/uncategorized\r\nSynced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\nFind logs at: ./wandb/run-20241126_052540-ggjblhrs/logs\r\nThe new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\r\nSuccessfully finished last run (ID:ggjblhrs). Initializing new run:\r\nWaiting for wandb.init()...\r\nwandb version 0.18.7 is available! To upgrade, please run: $ pip install wandb --upgrade\r\nTracking run with wandb version 0.17.5\r\nRun data is saved locally in /host_home/research/code/research_mono/notebooks/math_ai/ad_hoc/wandb/run-20241126_052552-um22ijva\r\nSyncing run [vague-universe-20](https://wandb.ai/ch271828n/uncategorized/runs/um22ijva) to [Weights & Biases](https://wandb.ai/ch271828n/uncategorized) ([docs](https://wandb.me/run))\r\nView project at https://wandb.ai/ch271828n/uncategorized\r\nView run at https://wandb.ai/ch271828n/uncategorized/runs/um22ijva\r\n2024-11-26T05:26:12.416123+0000 | pre_initialize_structure | INFO - Compression lifecycle structure pre-initialized for 0 modifiers\r\nFinishing last run (ID:um22ijva) before initializing another...\r\nView run vague-universe-20 at: https://wandb.ai/ch271828n/uncategorized/runs/um22ijva\r\nView project at: https://wandb.ai/ch271828n/uncategorized\r\nSynced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\nFind logs at: ./wandb/run-20241126_052552-um22ijva/logs\r\nThe new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\r\nSuccessfully finished last run (ID:um22ijva). Initializing new run:\r\nWaiting for wandb.init()...\r\nwandb version 0.18.7 is available! To upgrade, please run: $ pip install wandb --upgrade\r\nTracking run with wandb version 0.17.5\r\nRun data is saved locally in /host_home/research/code/research_mono/notebooks/math_ai/ad_hoc/wandb/run-20241126_052612-l8wousmz\r\nSyncing run [fast-water-21](https://wandb.ai/ch271828n/uncategorized/runs/l8wousmz) to [Weights & Biases](https://wandb.ai/ch271828n/uncategorized) ([docs](https://wandb.me/run))\r\nView project at https://wandb.ai/ch271828n/uncategorized\r\nView run at https://wandb.ai/ch271828n/uncategorized/runs/l8wousmz\r\n[2024-11-26 05:26:33,277] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n[/opt/conda/lib/python3.11/site-packages/llmcompressor/transformers/finetune/session_mixin.py:95](https://192.168.0.109:8888/opt/conda/lib/python3.11/site-packages/llmcompressor/transformers/finetune/session_mixin.py#line=94): FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\r\n  super().__init__(**kwargs)\r\n [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\r\n [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.5\r\n [WARNING]  using untested triton version (3.1.0), only 1.0.0 is known to be compatible\r\n[/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47](https://192.168.0.109:8888/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py#line=46): FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n  @autocast_custom_fwd\r\n[/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66](https://192.168.0.109:8888/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py#line=65): FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\r\n  @autocast_custom_bwd\r\n2024-11-26T05:26:34.104132+0000 | one_shot | INFO - *** One Shot ***\r\nFinishing last run (ID:l8wousmz) before initializing another...\r\nView run fast-water-21 at: https://wandb.ai/ch271828n/uncategorized/runs/l8wousmz\r\nView project at: https://wandb.ai/ch271828n/uncategorized\r\nSynced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\nFind logs at: ./wandb/run-20241126_052612-l8wousmz/logs\r\nThe new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\r\nSuccessfully finished last run (ID:l8wousmz). Initializing new run:\r\nWaiting for wandb.init()...\r\nwandb version 0.18.7 is available! To upgrade, please run: $ pip install wandb --upgrade\r\nTracking run with wandb version 0.17.5\r\nRun data is saved locally in /host_home/research/code/research_mono/notebooks/math_ai/ad_hoc/wandb/run-20241126_052634-6zgyfa3j\r\nSyncing run [desert-firefly-22](https://wandb.ai/ch271828n/uncategorized/runs/6zgyfa3j) to [Weights & Biases](https://wandb.ai/ch271828n/uncategorized) ([docs](https://wandb.me/run))\r\nView project at https://wandb.ai/ch271828n/uncategorized\r\nView run at https://wandb.ai/ch271828n/uncategorized/runs/6zgyfa3j\r\n2024-11-26T05:26:53.409986+0000 | _check_compile_recipe | INFO - Recipe compiled and 1 modifiers created\r\n2024-11-26T05:26:53.535971+0000 | _calibrate | INFO - Running QuantizationModifier calibration with 10 samples...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10[/10](https://192.168.0.109:8888/10) [00:05<00:00,  1.86it[/s](https://192.168.0.109:8888/s)]\r\nmanager stage: Modifiers initialized\r\n2024-11-26T05:26:58.922362+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\r\nmanager stage: Modifiers finalized\r\n2024-11-26T05:26:58.923158+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\r\n```\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Environment**\r\nInclude all relevant environment information:\r\n1. OS [e.g. Ubuntu 20.04]:\r\n2. Python version [e.g. 3.7]:\r\n3. LLM Compressor version or commit hash [e.g. 0.1.0, `f7245c8`]:\r\n4. ML framework version(s) [e.g. torch 2.3.1]:\r\n5. Other Python package versions [e.g. vLLM, compressed-tensors, numpy, ONNX]:\r\n6. Other relevant environment information [e.g. hardware, CUDA version]:\r\n\r\n**To Reproduce**\r\nExact steps to reproduce the behavior:\r\n\r\n\r\n**Errors**\r\nIf applicable, add a full print-out of any errors or exceptions that are raised or include screenshots to help explain your problem.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here. Also include any relevant files.\r\n", "labels": ["bug"], "repository": "vllm-project/llm-compressor", "url": "https://github.com/vllm-project/llm-compressor/issues/934", "created_at": "2024-11-26T05:27:28Z", "updated_at": "2024-11-26T05:27:28Z", "issue_number": 934, "state": "open", "comments": 0}
{"id": "WongKinYiu/YOLO/135", "title": "[Feature Request] Add Albumentations for image augmentations.", "body": "## Feature Description\r\nAlbumentations provides 70+ augmentations on images, bounding boxes and key points.\r\n\r\nWould be nice to add it here, as it is only a few lines of code:\r\n\r\nhttps://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\r\n\r\nIt does not support mosaic, but all other transforms from it could be used in the repo.", "labels": ["enhancement"], "repository": "WongKinYiu/YOLO", "url": "https://github.com/WongKinYiu/YOLO/issues/135", "created_at": "2024-12-10T03:21:30Z", "updated_at": "2024-12-10T03:21:30Z", "issue_number": 135, "state": "open", "comments": 0}
{"id": "IBM/terratorch/333", "title": "Add support for running predictions on the provided datamodules", "body": "It would be nice to be able to see predictions over the different splits of the datamodule for both diagnostic and visual inspection reasons. \r\n\r\nI believe this involves adding the prediction mode to the datamodules and also verifying if the trainer.predict() requires any modification.\r\n", "labels": [], "repository": "IBM/terratorch", "url": "https://github.com/IBM/terratorch/issues/333", "created_at": "2024-12-20T15:29:47Z", "updated_at": "2024-12-20T15:30:08Z", "issue_number": 333, "state": "open", "comments": 0}
{"id": "IBM/terratorch/323", "title": "Saving models with ONNX. ", "body": "Is something interesting for TerraTorch to have a way to export models to ONNX format ?", "labels": [], "repository": "IBM/terratorch", "url": "https://github.com/IBM/terratorch/issues/323", "created_at": "2024-12-11T17:55:59Z", "updated_at": "2024-12-17T16:53:24Z", "issue_number": 323, "state": "open", "comments": 2}
{"id": "IBM/terratorch/261", "title": "Add padding while evaluating on the original mask size", "body": "**Is your feature request related to a problem? Please describe.**\r\nSome model have a specific patch size which only allow to run images with multiples of these sizes. E.g., a patch size of 14 and image size of 512 is not possible.\r\n\r\n**Describe the solution you'd like**\r\nA simple approach is to add padding (e.g. nearest or mirror padding) to increase the image size to e.. 518 (which is divisible by 14).\r\nHowever, doing this using `albumentation.PadIfNeeded` would probably also increase the mask (not tested, just an assumption). The evaluation should be performed on the original 512 mask, not the 518 padded mask, as this would upsample the edge pixels. Therefore, TerraTorch needs to handle this in some way. \r\n\r\n**Describe alternatives you've considered (optional)**\r\nAlternatively, the padding is performed only on the images, and the mask is cropped afterwards to the original size. This could happened in the TerraTorch tasks.\r\n\r\n@daniszw @paolo-fraccaro ", "labels": [], "repository": "IBM/terratorch", "url": "https://github.com/IBM/terratorch/issues/261", "created_at": "2024-11-26T13:21:58Z", "updated_at": "2024-12-23T18:06:37Z", "issue_number": 261, "state": "open", "comments": 6}
{"id": "crewAIInc/crewAI-tools/157", "title": "custom tool input error", "body": "Hello, \r\n\r\nI am creating a custom tool. The problem is that sometimes it can run the tool and sometimes it raises a problem with the input. I don't understand why such behaviour. Thanks for helping.\r\n\r\n> I encountered an error while trying to use the tool. This was the error: Arguments validation failed: 1 validation error for MyCustomToolSchema\r\nargument\r\n  Input should be a valid string [type=string_type, input_value={'description': 'My perio...r only?', 'type': 'str'}, input_type=dict]\r\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type.\r\n Tool data retreiver accepts these inputs: Tool Name: data retreiver\r\nTool Arguments: {'argument': {'description': None, 'type': 'str'}}\r\n\r\nthis is my tool :\r\n```\r\n\r\nclass MyCustomTool(BaseTool):\r\n    name: str = \"data retreiver\"\r\n    description: str = (\r\n        \"This tool searches for the most relevant answers given the question.\"\r\n    )\r\n\r\n    def _run(self, argument: str) -> str:\r\n        # Implementation goes here\r\n        url = \"some private url\"\r\n        headers = {\r\n        'x-api-key': \"private\"}\r\n\r\n        response = requests.get(url, headers=headers).json()\r\n        response = response['data']\r\n        response = [r['answer'].replace(\"\\n\",\"\") for r in response]\r\n                \r\n        return response\r\n\r\n```", "labels": [], "repository": "crewAIInc/crewAI-tools", "url": "https://github.com/crewAIInc/crewAI-tools/issues/157", "created_at": "2024-12-20T12:35:18Z", "updated_at": "2024-12-20T12:35:50Z", "issue_number": 157, "state": "open", "comments": 0}
{"id": "crewAIInc/crewAI-tools/152", "title": "getting ModuleNotFoundError: No module named 'matplotlib'", "body": "Hi there,\r\n\r\nKindly need your help. In my crew, I am using allow_code_execution tool to generate my images by one of my agent to write  a python code and run it but somehow in my docker environment not able to install matplotlib which is necessary to visiualize my data and then generate images. Thanks. \r\n\r\nError message: ## Tool Output: Something went wrong while running the code: Traceback (most recent call last): File \"<string>\", line 2, in <module> ModuleNotFoundError: No module named 'matplotlib'\r\n\r\nSome extra infos:\r\nMac, my python version in .venv: 3.11.5", "labels": [], "repository": "crewAIInc/crewAI-tools", "url": "https://github.com/crewAIInc/crewAI-tools/issues/152", "created_at": "2024-12-18T09:13:45Z", "updated_at": "2024-12-18T09:19:34Z", "issue_number": 152, "state": "open", "comments": 0}
{"id": "crewAIInc/crewAI-tools/130", "title": "SerperDev Tool: return more data from serper.dev API", "body": "The serper.dev API returns a lot of data that is currently ignored by the tool. \r\n\r\nWhen using the Search Tool, it would be nice to have the ability to retrieve peopleAlsoAsk and relatedSearch for example. And depending on the search URLs used, there are keys that might be interesting adding to the response too. Here's a non-exhaustive list:\r\n - date (Search, News...)\r\n - source (News)\r\n - imageUrl (News)\r\n - publicationInfo (Scholar)\r\n -  year (Scholar)\r\n etc.\r\n", "labels": ["enhancement"], "repository": "crewAIInc/crewAI-tools", "url": "https://github.com/crewAIInc/crewAI-tools/issues/130", "created_at": "2024-11-28T16:40:17Z", "updated_at": "2024-12-12T13:44:31Z", "issue_number": 130, "state": "open", "comments": 1}
{"id": "pydantic/logfire/674", "title": "API access to the span generated by the @logfire.instrument() decorator", "body": "### Description\n\n(as discussed on Slack)\r\n\r\nI'd like to have access to the span generated by the decorator. eg of how I'm hoping to use it:\r\n\r\n```\r\n@logfire.instrument()\r\ndef lf_example():\r\n    result = do_some_things()\r\n    if result:\r\n        # Set an attribute on the current logfire span from the decorator\r\n        logfire.set_attribute(status='OK')\r\n    else:\r\n        logfire.set_attribute(status='ERROR')\r\n```\r\n\r\nAn idea explored on Slack was to use otel apis:\r\n```\r\nfrom opentelemetry.trace.propagation import get_current_span\r\nget_current_span().set_attribute(...)\r\n```\r\nBut it seems that the otel apis don't work with non-primatives.\r\n\r\nAn idea was to add a `logfire.get_current_span` api that would get the LogfireSpan instance.\r\n", "labels": ["Feature Request"], "repository": "pydantic/logfire", "url": "https://github.com/pydantic/logfire/issues/674", "created_at": "2024-12-13T20:53:18Z", "updated_at": "2024-12-16T11:56:56Z", "issue_number": 674, "state": "open", "comments": 0}
{"id": "pydantic/logfire/656", "title": "Document integrations kwargs", "body": "e.g. HTTPXInstrumentKwargs as in https://github.com/pydantic/logfire/issues/655", "labels": [], "repository": "pydantic/logfire", "url": "https://github.com/pydantic/logfire/issues/656", "created_at": "2024-12-09T15:42:24Z", "updated_at": "2024-12-09T15:42:24Z", "issue_number": 656, "state": "open", "comments": 0}
{"id": "pydantic/logfire/655", "title": "Allow HTTPX instrumentation to capture more request info, and improve API docs", "body": "### Description\n\nWe should add the following kwargs to [`instrument_httpx`](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.Logfire.instrument_httpx):\r\n* `capture_request_headers: bool | None = None`\r\n* `capture_request_body: bool | None = None`\r\n* `capture_response_headers: bool | None = None`\r\n* `capture_response_body: bool | None = None`\r\n* `capture_all: bool = False` - just sets all the above to `True`\r\n\r\nWe can raise an error if you use these together with `async_request_hook` etc.\r\n\r\nWe can add a note saying that capturing the body in production might increase the amount of data collected significantly.\r\n\r\nAlso [`HTTPXInstrumentKwargs`](https://github.com/pydantic/logfire/blob/743fb5d720ffa8b6b99d3f53a56d54bf10c5d8d5/logfire/_internal/integrations/httpx.py#L27-L32) is not included in docs which makes it basically undocumented.\r\n\r\nWe should probably do the same for requests.", "labels": ["Feature Request", "P1"], "repository": "pydantic/logfire", "url": "https://github.com/pydantic/logfire/issues/655", "created_at": "2024-12-09T15:18:33Z", "updated_at": "2024-12-24T10:07:13Z", "issue_number": 655, "state": "open", "comments": 3}
{"id": "ServerlessLLM/ServerlessLLM/176", "title": "[Feature Request] support stream chat", "body": "### Prerequisites\r\n\r\n- [X] I have searched existing issues and reviewed documentation.\r\n\r\n### Problem Description\r\n\r\nIn chat application, the generate api need to support stream mode to improve user experience.\r\n\r\n### Proposed Solution\r\n\r\nFollow the open ai chat api, add `stream: true` to support streaming response by SSE, for example\r\n```shell\r\ncurl http://127.0.0.1:8343/v1/chat/completions \\\r\n-H \"Content-Type: application/json\" \\\r\n-d '{\r\n        \"model\": \"facebook/opt-1.3b\",\r\n        \"messages\": [\r\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n            {\"role\": \"user\", \"content\": \"What is your name?\"}\r\n        ],\r\n        \"stream\": true\r\n    }'\r\n```\r\n\r\n### Alternatives Considered\r\n\r\n_No response_\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### Importance\r\n\r\nImportant\r\n\r\n### Usage Statistics (Optional)\r\n\r\n_No response_", "labels": ["Priority 1", "New feature"], "repository": "ServerlessLLM/ServerlessLLM", "url": "https://github.com/ServerlessLLM/ServerlessLLM/issues/176", "created_at": "2024-12-03T02:56:27Z", "updated_at": "2024-12-23T09:58:10Z", "issue_number": 176, "state": "open", "comments": 4}
{"id": "ServerlessLLM/ServerlessLLM/170", "title": "[Feature Request] [Enhancement] Merge `sllm-cli` and `sllm-serve` into a Unified Interface", "body": "### Prerequisites\n\n- [x] I have searched existing issues and reviewed documentation.\n\n### Problem Description\n\nCurrently, `sllm-cli` and `sllm-serve` exist as separate modules with distinct interfaces (`sllm-cli deploy` and `sllm-serve start`). This separation feels redundant, as users must learn and manage two interfaces. Drawing inspiration from other popular open-source systems like `vllm` and `ollama`, a unified interface could simplify the user experience and reduce complexity.\n\n\n### Proposed Solution\n\nMerge `sllm-cli` and `sllm-serve` into a single unified module, such as `sllm`, to provide a consistent interface. For example, commands like `sllm serve` and `sllm deploy` could replace the current separated commands. \n\nA tool like [Click](https://github.com/pallets/click), already adopted by FastAPI, could facilitate this integration without adding additional dependencies.\n\n\n### Alternatives Considered\n\n_No response_\n\n### Additional Context\n\n_No response_\n\n### Importance\n\nImportant\n\n### Usage Statistics (Optional)\n\n_No response_", "labels": ["Priority 1", "New feature", "Project health"], "repository": "ServerlessLLM/ServerlessLLM", "url": "https://github.com/ServerlessLLM/ServerlessLLM/issues/170", "created_at": "2024-11-29T14:02:53Z", "updated_at": "2024-11-29T14:03:03Z", "issue_number": 170, "state": "open", "comments": 0}
{"id": "foundation-model-stack/fms-fsdp/124", "title": "Dataset format restrictions", "body": "Can your engineering code receive data sets in json format, such as oscar-1G? It seems that only dataset provided with folder and subfolder can be received. The original text data of oscar is stored in JSONL format (one JSON object per line), and each JSON object contains an \"id\" field and a \"text\" field. The \"id\" field stores a sample number, and the \"text\" field stores a piece of text.\r\nI want to compare the llama2 performance between your project and megatron-lm. How can I use the same dataset?", "labels": [], "repository": "foundation-model-stack/fms-fsdp", "url": "https://github.com/foundation-model-stack/fms-fsdp/issues/124", "created_at": "2024-12-19T07:18:30Z", "updated_at": "2024-12-19T07:18:30Z", "issue_number": 124, "state": "open", "comments": 0}
{"id": "block/goose/486", "title": "Missing metadata in pyproject.toml", "body": "The `pyproject.toml` files for both the `goose-ai` and `ai-exchange` packages are missing some common metadata fields. Would you be willing to add the following properties:\r\n\r\n```toml\r\n[project]\r\nlicense = \"Apache 2.0\"\r\nclassifiers = [\r\n    \"Intended Audience :: Developers\",\r\n    \"License :: OSI Approved :: Apache Software License\",  \r\n]\r\n\r\n[project.urls]\r\nChangelog = \"https://github.com/block/goose/blob/main/CHANGELOG.md\"\r\nDocumentation = \"https://block.github.io/goose/\"\r\nHomepage = \"https://github.com/block/goose/\"\r\nSource = \"https://github.com/block/goose/\"\r\n```\r\n\r\nI'm not entirely familiar with conventions around [classifiers](https://pypi.org/classifiers/) but I presume there are a few more that'll be applicable.\r\n\r\nHappy to submit a PR if needed!", "labels": [], "repository": "block/goose", "url": "https://github.com/block/goose/issues/486", "created_at": "2024-12-17T02:42:38Z", "updated_at": "2024-12-17T02:42:38Z", "issue_number": 486, "state": "open", "comments": 0}
{"id": "All-Hands-AI/OpenHands/5781", "title": "Allow us to use MCP servers to extend OpenHand's functionality", "body": "**What problem or use case are you trying to solve?**\r\n\r\nOpenHands' functionality is currently fairly limited, but Anthropic's MCP standard provides a way for LLMs to interact with many additional services and use them as \"tools\". This could allow for much more complex workflows, e.g. to use Puppeteer or Playwright to test the code in the browser, then if it fails use OpenAI o1 (via MCP) to debug/rewrite it, etc.\r\n\r\n**Describe the UX of the solution you'd like**\r\n\r\nI guess the ideal would be to be able to install MCP servers in one click or a [prompt](https://github.com/anaisbetts/mcp-installer), but the main thing is to be able to access them. Perhaps a list of the installed servers could be good to verify they have been recognised, like in Claude Desktop:\r\n\r\n![Screenshot 2024-12-24 at 13 53 01](https://github.com/user-attachments/assets/def0b43f-62a6-4b3c-8ddb-1211ab95a861)\r\n\r\n**Do you have thoughts on the technical implementation?**\r\n\r\nI don't know OpenHands' architecture, but please be sure to add clear documentation with step-by-step instructions so I know any setup that's required to use this functionality.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nUsing Claude Desktop instead of OpenHands, because I can probably replicate a lot of the same functionality by just combining MCP servers. But the UI probably wouldn't be as good and I'm not sure if it would work as effectively.\r\n\r\n**Additional context**\r\n", "labels": ["enhancement"], "repository": "All-Hands-AI/OpenHands", "url": "https://github.com/All-Hands-AI/OpenHands/issues/5781", "created_at": "2024-12-24T12:01:21Z", "updated_at": "2024-12-24T12:04:46Z", "issue_number": 5781, "state": "open", "comments": 1}
{"id": "All-Hands-AI/OpenHands/5762", "title": "Confirmation mode setting should not clear/exit session", "body": "**What problem or use case are you trying to solve?**\r\nIf you need to work slowly through something and want to approve every decision but don't want to lose the session\r\n\r\n**Describe the UX of the solution you'd like**\r\n\r\nWhen enabling/disabling confirmation mode, the session should not exit.\r\n\r\n", "labels": ["enhancement"], "repository": "All-Hands-AI/OpenHands", "url": "https://github.com/All-Hands-AI/OpenHands/issues/5762", "created_at": "2024-12-23T08:39:46Z", "updated_at": "2024-12-23T14:06:39Z", "issue_number": 5762, "state": "open", "comments": 1}
{"id": "All-Hands-AI/OpenHands/5760", "title": "An MCP Server would be cool", "body": "**What problem or use case are you trying to solve?**\n\nScripting OpenHands in headless mode via mcp server could be really cool, allowing easily building agents on top of OpenHands\n\n**Describe the UX of the solution you'd like**\n\nhttps://modelcontextprotocol.io/introduction\n\n**Do you have thoughts on the technical implementation?**\n\nBasically just a simple interface that expects OpenHands running in headless mode and forwards prompts to OpenHands, relays any responses back \n\n**Describe alternatives you've considered**\n\n**Additional context**\n", "labels": ["enhancement"], "repository": "All-Hands-AI/OpenHands", "url": "https://github.com/All-Hands-AI/OpenHands/issues/5760", "created_at": "2024-12-23T05:32:49Z", "updated_at": "2024-12-23T05:32:49Z", "issue_number": 5760, "state": "open", "comments": 0}
{"id": "All-Hands-AI/OpenHands/5748", "title": "Rename \"Ran a Jupyter Command\" to \"Ran a Python Command\" in UI", "body": "**Summary**\r\n\r\nPeople associate \"Jupyter\" strongly with data science, so it'd be better to change the UI to say \"Ran a Python Command\" to indicate that it's running generic commands.", "labels": ["fix-me"], "repository": "All-Hands-AI/OpenHands", "url": "https://github.com/All-Hands-AI/OpenHands/issues/5748", "created_at": "2024-12-22T18:10:03Z", "updated_at": "2024-12-22T18:16:11Z", "issue_number": 5748, "state": "open", "comments": 2}
{"id": "All-Hands-AI/OpenHands/5744", "title": "[Feature]: Create a better leaderboard for OpenHands", "body": "**What problem or use case are you trying to solve?**\r\nCurrently, there is no comprehensive leaderboard that effectively tracks the performance of open-sourced models within OpenHands. A clear leaderboard would facilitate comparison and improvement.\r\n\r\n**Describe the UX of the solution you'd like**\r\nAn online sheet that continuously updates and tracks performance metrics for various models, possibly with the ability to click on links for deeper insights specific to each model.\r\n\r\n**Do you have thoughts on the technical implementation?**\r\nThe leaderboard could utilize existing performance data stored in a central database, automatically populated and refreshed at intervals, ensuring real-time updates.\r\n\r\n**Describe alternatives you've considered**\r\nUsing a static document or manual tracking methods, but these lack the dynamic features and timeliness that a real leaderboard would provide.\r\n\r\n**Additional context**\r\nThis initiative was discussed in a recent thread on Slack with users expressing interest in better tracking mechanisms for performance.\r\n\r\n**Issue Created By**: Graham Neubig on Slack", "labels": ["enhancement"], "repository": "All-Hands-AI/OpenHands", "url": "https://github.com/All-Hands-AI/OpenHands/issues/5744", "created_at": "2024-12-22T01:32:12Z", "updated_at": "2024-12-22T17:43:52Z", "issue_number": 5744, "state": "open", "comments": 0}
{"id": "All-Hands-AI/OpenHands/5727", "title": "Allow the BrowsingAgent to return the CSS selector of a DOM Element (or BID).", "body": "**What problem or use case are you trying to solve?**\r\n\r\nI'm trying to have OH create a web-scrapper, by giving it natural language instructions to browse a web page.\r\nFor this, the `BrowsingAgent` or `browsing tool` must be able to identify a CSS selector associated to a DOM Element (or BID).\r\n\r\n**Describe the UX of the solution you'd like**\r\n\r\nI need the enable the BrowsingAgent to return the CSS selector of a given DOM element (eg: \"What is the CSS selector associated with the login button of the page at www.example.com\").\r\n\r\n**Do you have thoughts on the technical implementation?**\r\n\r\nI guess BIDs are somehow associated with dom elements. This relation could be used to obtain the CSS selector.\r\n", "labels": ["enhancement"], "repository": "All-Hands-AI/OpenHands", "url": "https://github.com/All-Hands-AI/OpenHands/issues/5727", "created_at": "2024-12-21T03:50:06Z", "updated_at": "2024-12-21T12:02:36Z", "issue_number": 5727, "state": "open", "comments": 0}
{"id": "huggingface/lerobot/571", "title": "Refactor CLI Structure Using Command Pattern for Better Organization", "body": "Description:\r\nCurrently, the CLI argument parsing is scattered across different files with each implementing its own parser logic. This makes the codebase harder to maintain, test, and extend.\r\n\r\nWe should refactor the CLI structure to use the Command Pattern, This would:\r\n\r\nBenefits:\r\n- Provide a single consistent way to handle CLI commands\r\n- Make adding new commands easier without modifying existing code\r\n- Improve testability by isolating command logic\r\n- Reduce code duplication\r\n- Make the codebase more maintainable\r\n", "labels": [], "repository": "huggingface/lerobot", "url": "https://github.com/huggingface/lerobot/issues/571", "created_at": "2024-12-10T18:55:45Z", "updated_at": "2024-12-10T18:55:45Z", "issue_number": 571, "state": "open", "comments": 0}
{"id": "modelscope/agentscope/496", "title": "[Feature]: [\u9700\u6c42] \u6709\u6ca1\u6709\u54ea\u4f4d\u5927\u4f6c\u7528\u8fd9\u4e2a\u6846\u67b6\u5b9e\u73b0\u4e86agentic RAG\uff0c\u50cflanggraph\u90a3\u6837\u7684", "body": "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb", "labels": ["enhancement"], "repository": "modelscope/agentscope", "url": "https://github.com/modelscope/agentscope/issues/496", "created_at": "2024-12-02T07:58:05Z", "updated_at": "2024-12-02T07:58:05Z", "issue_number": 496, "state": "open", "comments": 0}
{"id": "DS4SD/docling/648", "title": "Make EasyOCR optional dependency", "body": "### Requested Feature\r\nIt would be beneficial to make OCR models optional during installation, with EasyOCR remaining as the default option. In our case, we use TesseractOCR but are required to install EasyOCR since it's currently mandatory, even though we don't use it.\r\n\r\nHere's a proposed installation approach:\r\n\r\n1. All OCR models:\r\n`pip install docling[all]`\r\n\r\n2. EasyOCR only (default installation):\r\n`pip install docling[easyocr]`\r\n\r\n3. Specific OCR models:\r\n`pip install docling[tesseract]`\r\n\r\n4. Base installation (no OCR models):\r\n`pip install docling`\r\n\r\n### Alternatives\r\n1. Install Docling as is - This installs EasyOCR and its dependencies even when they're not needed.\r\n2. Install Docling without dependencies - This requires significant maintenance effort on our end to ensure version compatibility.", "labels": ["enhancement"], "repository": "DS4SD/docling", "url": "https://github.com/DS4SD/docling/issues/648", "created_at": "2024-12-23T12:49:10Z", "updated_at": "2024-12-23T12:49:10Z", "issue_number": 648, "state": "open", "comments": 0}
{"id": "DS4SD/docling/646", "title": "How can i use docling for csv file", "body": "### Question\r\n<!-- Describe what you would like to achieve and which part you need help with. -->\r\n...\r\nHello, how can i use csv file with docling? is that possible to do so?\r\n<!-- \u26a0\ufe0f ATTENTION: When sharing screenshots, attachments, or other data make sure not to include any sensitive information. -->\r\n", "labels": ["question"], "repository": "DS4SD/docling", "url": "https://github.com/DS4SD/docling/issues/646", "created_at": "2024-12-22T13:21:22Z", "updated_at": "2024-12-22T13:21:22Z", "issue_number": 646, "state": "open", "comments": 0}
{"id": "DS4SD/docling/639", "title": "Feat: scanned PDF orientation detection and fixup before ocr", "body": "# Requested feature\r\nAuto rotate scanned pdf (or on toggle)\r\n\r\n# reason \r\nwhen running ORC on wrongly rotated PDF scanned document \r\n\r\nlike in here :\r\n![image](https://github.com/user-attachments/assets/0a6f3c84-e99a-45f1-984e-f7c315cf83af)\r\nOCR is likely  produces garbled characters (tested on teseract and language czech)\r\n\r\n### Alternatives\r\nunknown", "labels": ["enhancement"], "repository": "DS4SD/docling", "url": "https://github.com/DS4SD/docling/issues/639", "created_at": "2024-12-20T10:20:00Z", "updated_at": "2024-12-20T10:29:25Z", "issue_number": 639, "state": "open", "comments": 0}
{"id": "DS4SD/docling/634", "title": "Pass HTTP request headers to docling when parsing via url", "body": "### Requested feature\r\nMight be out of scope but would it make sense to allow users to pass HTTP request headers to the DocumentParser and cli so we can parse documents on domains behind authentication?\r\n\r\n### Alternatives\r\nAlternatively users can pull the document themselves then parse it, but I figured it would be nice to have OOTB.\r\n\r\n<!-- \u26a0\ufe0f ATTENTION: When sharing screenshots, attachments, or other data make sure not to include any sensitive information. -->\r\n", "labels": ["enhancement"], "repository": "DS4SD/docling", "url": "https://github.com/DS4SD/docling/issues/634", "created_at": "2024-12-19T20:02:55Z", "updated_at": "2024-12-20T17:39:26Z", "issue_number": 634, "state": "open", "comments": 2}
{"id": "DS4SD/docling/632", "title": "Headers and footers for docx", "body": "Hi, \r\n\r\n`MsWordDocumentBackend` does not support headers and footers while other packages do (e.g. [docx2txt](https://github.com/ankushshah89/python-docx2txt)). The docx package allow them to extract quite easily for each document section. Maybe it could be done as follows:\r\n\r\n```python\r\nfor section in docx_obj.sections:\r\n    handle section.header.paragraphs\r\n    run walk_linear on the section objects\r\n    handle section.footer.paragraphs\r\n```\r\n\r\nNote that it can't be perfect because header and footers might change for even / odd pages and it's not really feasible to get the pagination without rendering the document (see [here](https://python-docx.readthedocs.io/en/latest/dev/analysis/features/header.html)). But defaulting to the odd pages (as done in `section.header` and `section.footer` would be great already).\r\n", "labels": ["enhancement"], "repository": "DS4SD/docling", "url": "https://github.com/DS4SD/docling/issues/632", "created_at": "2024-12-19T11:38:35Z", "updated_at": "2024-12-19T17:20:30Z", "issue_number": 632, "state": "open", "comments": 0}
{"id": "DS4SD/docling/625", "title": "Writing picture enrichment annotations to Markdown file", "body": "### Image annotations to MD\r\nFollowing the [Figure Enrichment tutorial](https://ds4sd.github.io/docling/examples/develop_picture_enrichment/) it is easy to add classification metadata to an image through the `element.annotations.append(data)` function.\r\n\r\nHowever this data is not stored during the export to Markdown format. Would there be a way to write it alongside the image path in the final MD document ? Would be great for our RAG application.", "labels": ["question"], "repository": "DS4SD/docling", "url": "https://github.com/DS4SD/docling/issues/625", "created_at": "2024-12-18T16:00:23Z", "updated_at": "2024-12-18T16:00:23Z", "issue_number": 625, "state": "open", "comments": 0}
{"id": "DS4SD/docling/614", "title": "Explicitly reexport all attributes meant as public", "body": "### Requested feature\r\n<!-- Describe the feature you have in mind and the user need it addresses. -->\r\n\r\nMypy configured strictly complains that some attributes, open for import by your users, [aren't explicitly exported](https://mypy.readthedocs.io/en/stable/command_line.html#cmdoption-mypy-no-implicit-reexport). This is a quality issue, since it muddies the boundary between public and non-public symbols/attributes.\r\n\r\n```\r\nsrc/app/data/unstructured/document/__init__.py:5: error: Module\r\n\"docling.datamodel.document\" does not explicitly export attribute\r\n\"DoclingDocument\"  [attr-defined]\r\n    from docling.datamodel.document import DoclingDocument, SectionHeaderI...\r\n    ^\r\nsrc/app/data/unstructured/document/__init__.py:5: error: Module\r\n\"docling.datamodel.document\" does not explicitly export attribute\r\n\"SectionHeaderItem\"  [attr-defined]\r\n    from docling.datamodel.document import DoclingDocument, SectionHeaderI...\r\n    ^\r\nsrc/app/data/unstructured/document/__init__.py:5: error: Module\r\n\"docling.datamodel.document\" does not explicitly export attribute \"TableItem\" \r\n[attr-defined]\r\n    from docling.datamodel.document import DoclingDocument, SectionHeaderI...\r\n    ^\r\nsrc/app/data/unstructured/document/__init__.py:5: error: Module\r\n\"docling.datamodel.document\" does not explicitly export attribute \"TextItem\" \r\n[attr-defined]\r\n    from docling.datamodel.document import DoclingDocument, SectionHeaderI...\r\n    ^\r\nsrc/app/data/unstructured/pipeline/documents/pipeline_documents.py:8: error:\r\nModule \"docling.datamodel.document\" does not explicitly export attribute\r\n\"DoclingDocument\"  [attr-defined]\r\n    from docling.datamodel.document import ConversionResult, DoclingDocume...\r\n```\r\n\r\n### Alternatives\r\n\r\n<!-- Describe any alternatives you have considered. -->\r\nConfiguring Mypy to be more lax, reducing the benefits of this check. Since the solution is inexpensive and has no downsides, I prefer for it to be solved directly.\r\n\r\n<!-- \u26a0\ufe0f ATTENTION: When sharing screenshots, attachments, or other data make sure not to include any sensitive information. -->\r\n", "labels": ["enhancement"], "repository": "DS4SD/docling", "url": "https://github.com/DS4SD/docling/issues/614", "created_at": "2024-12-17T07:55:26Z", "updated_at": "2024-12-17T07:55:26Z", "issue_number": 614, "state": "open", "comments": 0}
{"id": "DS4SD/docling/590", "title": "Doesn't parse the table, treats it as an image", "body": "Thank you guys for your work.\r\nWhile using it I found out that docling can save tables as images by doing the following\r\n```python\r\nfor element, _level in conv_res.document.iterate_items()::\r\n        if isinstance(element, TableItem).\r\n            table_counter += 1\r\n            element_image_filename = (\r\n                output_dir / \u201ctables\u201d / f \u201ctable-{table_counter}.png\u201d\r\n            )\r\n            element_image_filename.parent.mkdir(parents=True, exist_ok=True)\r\n            with element_image_filename.open(\u201cwb\u201d) as fp.\r\n                element.get_image(conv_res.document).save(fp, \u201cPNG\u201d)\r\n```\r\nIn my task, due to the complexity of tables, many of which are not recognized correctly, I wanted it to be treated as an image without parsing, so I set up the\r\n```python\r\n    pipeline_options.do_ocr = False\r\n    pipeline_options.do_table_structure = False\r\n```\r\nBut this causes the table not to be inserted into the resulting markdown, but the table is still recognized. So I think the table recognition can be inserted as an image into the original text position, now is there a way to achieve this task? Or give me a little idea for modification, thanks!", "labels": ["enhancement"], "repository": "DS4SD/docling", "url": "https://github.com/DS4SD/docling/issues/590", "created_at": "2024-12-13T08:41:24Z", "updated_at": "2024-12-13T16:31:49Z", "issue_number": 590, "state": "open", "comments": 3}
{"id": "tox-dev/tox-uv/144", "title": "changedir not working", "body": "## Issue\r\n\r\nI'm using tox specifically so that I can catch errors that pytest would otherwise miss. For example, if I add a package resource to my files and include a unit test that loads that resource, pytest will think everything is fine. Noticing that my package doesn't load because I failed to include the new resource in package data happens only later, when I try to install the package in a new environment using the whl. \r\n\r\nI've found that running tox with this configuration works as desired, raising an error where I try to load a package resource that was not added to package data:\r\n```\r\n[tox]\r\nisolated_build=True\r\n\r\n[testenv]\r\ndescription = Run unit tests\r\ndeps =\r\n    pytest\r\ndependency_groups = extras\r\nchangedir = {envtmpdir}  # suggested by https://blog.ganssle.io/articles/2019/08/test-as-installed.html\r\ncommands = python -m pytest {posargs} {toxinidir}\r\n```\r\n\r\nHowever, deleting\r\n```\r\ndeps =\r\n    pytest\r\n```\r\nand replacing it with\r\n```\r\nrunner = uv-venv-lock-runner\r\nwith_dev = true\r\n```\r\nputs me back where I started; tox runs without noticing the problem. Please let me know if it's obvious what's going on here -- if not I may follow up with a reproducable example.\r\n\r\n## Environment\r\n\r\n- OS: OSX 15.1.1\r\n\r\n", "labels": ["bug"], "repository": "tox-dev/tox-uv", "url": "https://github.com/tox-dev/tox-uv/issues/144", "created_at": "2024-12-21T16:32:27Z", "updated_at": "2024-12-21T16:32:27Z", "issue_number": 144, "state": "open", "comments": 0}
{"id": "whyhow-ai/knowledge-graph-studio/8", "title": "Support for Locally Served Models (Ollama) & Open API Compatible Models", "body": "## What\r\nIs there an idea / WIP for replacing OpenAI Models with local models or cloud providers key like Azure / Bedrock\r\n\r\n## Why\r\nFor someone who wants private data to be experimented with the project & use Graph's for that\r\n\r\n## Implementation guidance\r\nReplacing OpenAI calls with using openai client by having base_url & api_key as env arguments\r\n", "labels": [], "repository": "whyhow-ai/knowledge-graph-studio", "url": "https://github.com/whyhow-ai/knowledge-graph-studio/issues/8", "created_at": "2024-11-26T05:51:24Z", "updated_at": "2024-11-26T05:51:24Z", "issue_number": 8, "state": "open", "comments": 0}
{"id": "ucbepic/docetl/238", "title": "Path arguments must not be null error", "body": "![image](https://github.com/user-attachments/assets/413cba1a-c029-4dda-b5a5-49e210ea5cd6)\r\n\r\nI could successfully built the docker image but the following results appear when I click Run in Playground. Here are contents of my .env and .env.local\r\n\r\n**.env (at root folder)**\r\n```\r\nOPENAI_API_KEY=sk-proj-gB5C2t...BdjwT3Blb...\r\nBACKEND_ALLOW_ORIGINS=\r\nBACKEND_HOST=localhost\r\nBACKEND_PORT=8000\r\nBACKEND_RELOAD=True\r\nFRONTEND_HOST=localhost\r\nFRONTEND_PORT=3000\r\n```\r\n\r\n**.env.local (in website folder)**\r\n```\r\nOPENAI_API_KEY=sk-proj-gB5C2t...BdjwT3Blb...\r\nOPENAI_API_BASE=https://api.openai.com/v1\r\nMODEL_NAME=gpt-4o-mini\r\n\r\nNEXT_PUBLIC_BACKEND_HOST=localhost\r\nNEXT_PUBLIC_BACKEND_PORT=8000\r\n```\r\n\r\nHere is the log in console:\r\n\r\n```\r\nPipeline configuration error: TypeError [ERR_INVALID_ARG_TYPE]: The \"path\" argument must be of type string. Received null\r\n    at Object.join (node:path:1251:7)\r\n    at a (/app/website/.next/server/app/api/writePipelineConfig/route.js:1:4230)\r\n    at g (/app/website/.next/server/app/api/writePipelineConfig/route.js:1:1140)\r\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\r\n    at async /app/website/node_modules/next/dist/compiled/next-server/app-route.runtime.prod.js:6:36934\r\n    at async eC.execute (/app/website/node_modules/next/dist/compiled/next-server/app-route.runtime.prod.js:6:27548)\r\n    at async eC.handle (/app/website/node_modules/next/dist/compiled/next-server/app-route.runtime.prod.js:6:38268)\r\n    at async doRender (/app/website/node_modules/next/dist/server/base-server.js:1359:42)\r\n    at async cacheEntry.responseCache.get.routeKind (/app/website/node_modules/next/dist/server/base-server.js:1581:28)\r\n    at async NextNodeServer.renderToResponseWithComponentsImpl (/app/website/node_modules/next/dist/server/base-server.js:1489:28) {\r\n  code: 'ERR_INVALID_ARG_TYPE'\r\n```", "labels": [], "repository": "ucbepic/docetl", "url": "https://github.com/ucbepic/docetl/issues/238", "created_at": "2024-12-09T22:27:35Z", "updated_at": "2024-12-12T17:30:56Z", "issue_number": 238, "state": "open", "comments": 7}
{"id": "ucbepic/docetl/222", "title": "Clicking \"Stop Pipeline\" should terminate the code_map operation gracefully", "body": "* After starting a code_map operation, clicking \"Stop Pipeline\" during the processing of the operation will not stop the code_map thread.\r\n", "labels": ["bug"], "repository": "ucbepic/docetl", "url": "https://github.com/ucbepic/docetl/issues/222", "created_at": "2024-12-02T00:45:36Z", "updated_at": "2024-12-02T17:42:21Z", "issue_number": 222, "state": "open", "comments": 0}
{"id": "ucbepic/docetl/221", "title": "Use arrow datasets for intermediates", "body": "JSON is a bit bulky. It's easy to view, but for intermediates, especially when using the UI, it does not make sense to store and query the intermediates as JSON.\r\n\r\nWe should have an option in a docetl config to store the intermediates as arrow datasets. We will then have to query them in the UI accordingly.", "labels": ["efficiency", "good first engineering issue"], "repository": "ucbepic/docetl", "url": "https://github.com/ucbepic/docetl/issues/221", "created_at": "2024-12-02T00:08:04Z", "updated_at": "2024-12-02T00:08:22Z", "issue_number": 221, "state": "open", "comments": 0}
{"id": "Akkudoktor-EOS/EOS/265", "title": "Documentation Optimization", "body": "Wiki + openAPI", "labels": ["documentation"], "repository": "Akkudoktor-EOS/EOS", "url": "https://github.com/Akkudoktor-EOS/EOS/issues/265", "created_at": "2024-12-17T18:08:56Z", "updated_at": "2024-12-17T21:20:53Z", "issue_number": 265, "state": "open", "comments": 0}
{"id": "Akkudoktor-EOS/EOS/264", "title": "Documentation PV prediction ", "body": "Wiki + openapi", "labels": ["documentation"], "repository": "Akkudoktor-EOS/EOS", "url": "https://github.com/Akkudoktor-EOS/EOS/issues/264", "created_at": "2024-12-17T18:08:09Z", "updated_at": "2024-12-17T21:20:53Z", "issue_number": 264, "state": "open", "comments": 0}
{"id": "Akkudoktor-EOS/EOS/241", "title": "[ENH]: Config Description + How to use Debug mode", "body": "### Link to discussion and related issues\r\n\r\n_No response_\r\n\r\n### Proposed implementation\r\n\r\n```python\r\nWould be very helpfull for potential developers\r\nOptimization Debug output was completly removed.\r\n```\r\n", "labels": ["documentation", "enhancement"], "repository": "Akkudoktor-EOS/EOS", "url": "https://github.com/Akkudoktor-EOS/EOS/issues/241", "created_at": "2024-12-14T09:03:44Z", "updated_at": "2024-12-17T21:25:56Z", "issue_number": 241, "state": "open", "comments": 0}
{"id": "ib-api-reloaded/ib_async/94", "title": "how to use the `ticker.updateEvent` method", "body": "i have a code like this:\r\n\r\n```\r\n    async def wait_for_midpoint_price(self, ticker: Ticker, wait_time: int) -> bool:\r\n        event = asyncio.Event()\r\n        def onTicker(ticker: Ticker) -> None:\r\n            if not util.isNan(ticker.midpoint()):\r\n                event.set()\r\n        ticker.updateEvent += onTicker\r\n        try:\r\n            await asyncio.wait_for(event.wait(), timeout=wait_time)\r\n            return True\r\n        except asyncio.TimeoutError:\r\n            return False\r\n        finally:\r\n            ticker.updateEvent -= onTicker\r\n```\r\n\r\nhowever, I got some Python type error:\r\n\r\n```\r\nCannot assign to attribute \"updateEvent\" for class \"Ticker\"\r\n  Expression of type \"Event\" cannot be assigned to attribute \"updateEvent\" of class \"Ticker\"\r\n    \"Event\" is not assignable to \"TickerUpdateEvent\"\r\n```\r\n\r\njust wondering is this the right way to use the `updateEvent` handler?", "labels": [], "repository": "ib-api-reloaded/ib_async", "url": "https://github.com/ib-api-reloaded/ib_async/issues/94", "created_at": "2024-12-01T05:29:41Z", "updated_at": "2024-12-02T12:49:30Z", "issue_number": 94, "state": "open", "comments": 1}
{"id": "modelcontextprotocol/python-sdk/111", "title": "Requirement name `mcp` matches project name `mcp`", "body": "When I run the first step `uv add \"mcp[cli]\"`, \r\n\r\nit said: \r\n```error: Requirement name `mcp` matches project name `mcp`, but self-dependencies are not permitted without the `--dev` or `--optional` flags. If your project name (`mcp`) is shadowing that of a third-party dependency, consider renaming the project.```\r\n\r\nI guess something wrong in `pyproject.toml`, but I don't know how to solve it.", "labels": [], "repository": "modelcontextprotocol/python-sdk", "url": "https://github.com/modelcontextprotocol/python-sdk/issues/111", "created_at": "2024-12-24T06:11:32Z", "updated_at": "2024-12-24T09:10:58Z", "issue_number": 111, "state": "open", "comments": 1}
{"id": "modelcontextprotocol/python-sdk/109", "title": "Add Automated Tests for src/mcp/client/sse.py and src/mcp/server/sse.py", "body": "The files src/mcp/client/sse.py and src/mcp/server/sse.py implement the client and server functionalities for SSE transport, but there are no tests in the repository to verify their behavior. \r\n\r\nRelevant Links:\r\nPR: https://github.com/modelcontextprotocol/python-sdk/pull/83\r\nRelated Discussion: https://github.com/jlowin/fastmcp/issues/69\r\nRelated Issue: https://github.com/modelcontextprotocol/python-sdk/issues/101\r\n", "labels": [], "repository": "modelcontextprotocol/python-sdk", "url": "https://github.com/modelcontextprotocol/python-sdk/issues/109", "created_at": "2024-12-21T18:36:12Z", "updated_at": "2024-12-21T18:36:12Z", "issue_number": 109, "state": "open", "comments": 0}
{"id": "modelcontextprotocol/python-sdk/105", "title": "feat: client should handle 307 redirects", "body": "**Is your feature request related to a problem? Please describe.**\r\nCurrently when I stand up an sse client against a server that redirects I get \r\n\r\n[2024-12-17 16:40:04,008][ERROR] Error in post_writer: Redirect response '307 Temporary Redirect' for url 'http://localhost:7860/api/v1/mcp?session_id=430eaa669c7a484a892dbd82c805249d'\r\nRedirect location: 'http://localhost:7860/api/v1/mcp/?session_id=430eaa669c7a484a892dbd82c805249d'\r\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/307\r\n\r\n**Describe the solution you'd like**\r\nI ended up implementing an OPTIONS check before I establish the sse client so that no redirect is necessary but it would be nice for this to be handled automatically.\r\n", "labels": [], "repository": "modelcontextprotocol/python-sdk", "url": "https://github.com/modelcontextprotocol/python-sdk/issues/105", "created_at": "2024-12-17T21:47:26Z", "updated_at": "2024-12-17T21:47:26Z", "issue_number": 105, "state": "open", "comments": 0}
{"id": "modelcontextprotocol/python-sdk/93", "title": "Proposal for Client Examples Repository or Section in MCP Python SDK", "body": "## Is your feature request related to a problem? Please describe.\r\nWhile the MCP ecosystem has a dedicated repository for server examples (modelcontextprotocol/servers), there doesn't seem to be a centralized location for sharing client implementations. \r\n\r\nThis makes it harder for new developers to:\r\n- Find reference implementations\r\n- Learn best practices for client development\r\n- See real-world examples of MCP integration\r\n- Understand different patterns for tool handling\r\n\r\n## Describe the solution you'd like\r\nI propose either:\r\n1. Creating a new `modelcontextprotocol/clients` repository to showcase different client implementations, similar to the servers repository\r\n2. Adding a subdirectory in the `examples/clients` directory in the python-SDK repository for community-contributed client examples.\r\n\r\nI have developed a Python chatbot example that demonstrates:\r\n- Connecting to multiple MCP servers\r\n- Discovering and executing tools\r\n- Supporting any OpenAI API-compatible LLM provider\r\n- Tracking progress and handling capabilities\r\n\r\nThis example could serve as an initial reference implementation.\r\n\r\n## Describe alternatives you've considered\r\n- Including example clients directly in the SDK documentation.\r\n- Creating a separate repository exclusively for community-contributed client resources.\r\n\r\n## Additional context\r\nThe example CLI chatbot follows the SDK's development guidelines:\r\n- Uses proper type hints\r\n- Includes comprehensive docstrings\r\n- Follows PEP 8 style\r\n- Implements proper error handling\r\n\r\nThe code is ready for contribution and can be adjusted to fit any preferred format or additional guidelines.\r\n", "labels": [], "repository": "modelcontextprotocol/python-sdk", "url": "https://github.com/modelcontextprotocol/python-sdk/issues/93", "created_at": "2024-12-05T18:07:46Z", "updated_at": "2024-12-10T18:19:42Z", "issue_number": 93, "state": "open", "comments": 3}
{"id": "modelcontextprotocol/python-sdk/87", "title": "Question: version in `create_initialization_options` is the pkg_ver for \"mcp\"", "body": "This is more in the nature of a request for clarification.\r\n\r\nThe sample code in the SDK here:\r\n\r\nhttps://github.com/modelcontextprotocol/python-sdk/blob/main/examples/servers/simple-prompt/mcp_simple_prompt/server.py\r\n\r\nshows the use of `create_initialization_options` to set the initialization options. However this function is setting the version to 'pkg_ver(\"mcp\")' which seems to indicate that it is the protocol version that is being indicated.\r\n\r\nIn the sqlite example, the version is manually set to 0.1.0, so it won't use the protocol version. Is this a mistake?\r\n\r\nIs the version meant to record the version of whatever the actual server program is? or is it mean to be the protocol version.", "labels": [], "repository": "modelcontextprotocol/python-sdk", "url": "https://github.com/modelcontextprotocol/python-sdk/issues/87", "created_at": "2024-12-03T18:36:41Z", "updated_at": "2024-12-05T05:08:11Z", "issue_number": 87, "state": "open", "comments": 4}
{"id": "modelcontextprotocol/python-sdk/86", "title": "Use Annotated[pydantic.AnyUrl, pydantic.UrlConstraints(host_required=False)] instead of AnyUrl", "body": "Pydantic's AnyUrl requires a host to be specified. For file resources this means that a triple slash such as `file:///foo.txt` is required because there is no host. We should likely be using `Annotated[pydantic.AnyUrl, pydantic.UrlConstraints(host_required=False)]` or something along these lines.", "labels": [], "repository": "modelcontextprotocol/python-sdk", "url": "https://github.com/modelcontextprotocol/python-sdk/issues/86", "created_at": "2024-12-03T17:24:24Z", "updated_at": "2024-12-03T21:03:29Z", "issue_number": 86, "state": "open", "comments": 0}
{"id": "modelcontextprotocol/python-sdk/84", "title": "Possible to launch a server and client in separate processes?", "body": "When looking at the examples, it seems like you always need to launch the server and client in the same script, because they share the read/write variables.\r\n\r\nIs there a way to launch these two pieces in their own scripts? If so, is it documented? This feels like an extremely common use case that might be missing. \r\n\r\nTrying to write an MCP server integration for tools in llama-index and realized I can't figure it out.\r\n\r\nThanks for any help!", "labels": [], "repository": "modelcontextprotocol/python-sdk", "url": "https://github.com/modelcontextprotocol/python-sdk/issues/84", "created_at": "2024-12-02T05:48:07Z", "updated_at": "2024-12-04T01:59:16Z", "issue_number": 84, "state": "open", "comments": 3}
{"id": "modelcontextprotocol/python-sdk/73", "title": "Better logging of when handlers are called and what they return when DEBUG is set", "body": null, "labels": ["enhancement"], "repository": "modelcontextprotocol/python-sdk", "url": "https://github.com/modelcontextprotocol/python-sdk/issues/73", "created_at": "2024-11-26T19:36:00Z", "updated_at": "2024-11-26T19:36:01Z", "issue_number": 73, "state": "open", "comments": 0}
